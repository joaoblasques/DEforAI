# Analytical Queries for Feature Derivation

**Course:** Data Engineering for AI/ML
**Chapter:** 05 - Data Warehousing for Analytics & ML
**Subchapter:** 03 - Analytical Queries for Feature Derivation
**Created:** 2025-10-20
**Updated:** 2025-10-20

---

## 📋 Overview

Feature engineering is the process of transforming raw data into features that better represent the underlying patterns in the data. In data warehouses, **SQL becomes the primary tool for feature derivation**, enabling data engineers and data scientists to compute features at massive scale without moving data.

Modern SQL dialects (PostgreSQL, Snowflake SQL, BigQuery Standard SQL, Redshift SQL) provide powerful analytical capabilities:

- **Window functions**: Compute rolling aggregates, rankings, and lead/lag values
- **CTEs (Common Table Expressions)**: Build complex features in readable, modular steps
- **Array and JSON functions**: Handle semi-structured data and embeddings
- **User-Defined Functions (UDFs)**: Custom transformations in Python/JavaScript
- **Materialized views**: Cache expensive feature computations

**Key Challenge**: Translating ML feature requirements into efficient SQL queries while maintaining:
- **Point-in-time correctness** (no data leakage)
- **Reproducibility** (same query = same features)
- **Performance** (process billions of rows in minutes, not hours)
- **Readability** (maintainable by team)

This subchapter covers SQL patterns for deriving common ML features from dimensional models, including:
- Temporal features (recency, frequency, momentum)
- Aggregated features (counts, sums, averages)
- Categorical encoding (one-hot, label encoding)
- Sequence features (previous N events)
- Statistical features (percentiles, z-scores)
- Text features (length, word count, TF-IDF approximations)

**Key Topics:**
- Window functions (ROW_NUMBER, RANK, LAG, LEAD, FIRST_VALUE, LAST_VALUE)
- Aggregate functions (SUM, AVG, COUNT, STDDEV, PERCENTILE)
- Date/time functions for temporal features
- String functions for text features
- Conditional logic (CASE, COALESCE, NULLIF)
- Array and JSON manipulation
- Pivoting and unpivoting
- Feature scaling and normalization in SQL
- Query optimization for feature engineering

**Prerequisites:**
- Solid understanding of SQL (joins, subqueries, aggregations)
- Knowledge of dimensional modeling (Subchapter 05.02)
- Understanding of ML feature types
- Familiarity with data warehouses (Subchapter 05.01)

---

## 🎯 Learning Objectives

By the end of this subchapter, you will be able to:

1. **Use window functions** to compute temporal features (rolling averages, running totals, ranks)
2. **Derive aggregated features** across multiple time windows (7d, 30d, 90d)
3. **Implement RFM analysis** (Recency, Frequency, Monetary) in SQL
4. **Create categorical features** with one-hot encoding and label encoding
5. **Compute statistical features** (percentiles, z-scores, outlier detection)
6. **Extract text features** (length, word count, pattern matching)
7. **Handle semi-structured data** (arrays, JSON) for embeddings and nested features
8. **Optimize feature queries** for billion-row datasets
9. **Build modular feature pipelines** using CTEs and views
10. **Ensure reproducibility** with parameterized queries and version control

---

## 📚 Core Concepts

### 1. Window Functions for Temporal Features

**Window Function Anatomy:**

```sql
function() OVER (
  PARTITION BY partition_columns  -- Group data (like GROUP BY, but keeps all rows)
  ORDER BY order_columns          -- Define order within partition
  ROWS BETWEEN start AND end      -- Define window frame
)
```

**Common Window Functions:**

| Function | Purpose | Example Use Case |
|----------|---------|------------------|
| **ROW_NUMBER()** | Assign unique rank | Latest transaction per user |
| **RANK()** | Rank with gaps | Top 3 products by revenue |
| **DENSE_RANK()** | Rank without gaps | Product category ranking |
| **LAG()** | Access previous row | Days since last purchase |
| **LEAD()** | Access next row | Predict next action |
| **FIRST_VALUE()** | First value in window | First purchase date |
| **LAST_VALUE()** | Last value in window | Most recent transaction |
| **SUM() OVER** | Running total | Cumulative revenue |
| **AVG() OVER** | Moving average | Rolling 7-day average |
| **COUNT() OVER** | Running count | Number of transactions to date |

**Example 1: Recency (Days Since Last Purchase)**

```sql
SELECT
  customer_id,
  transaction_id,
  transaction_date,

  -- Days since previous transaction
  DATEDIFF(
    day,
    LAG(transaction_date) OVER (PARTITION BY customer_id ORDER BY transaction_date),
    transaction_date
  ) AS days_since_last_purchase,

  -- Days since first transaction (customer tenure in days)
  DATEDIFF(
    day,
    FIRST_VALUE(transaction_date) OVER (PARTITION BY customer_id ORDER BY transaction_date),
    transaction_date
  ) AS days_since_first_purchase

FROM fact_transactions
ORDER BY customer_id, transaction_date;
```

**Example 2: Rolling Averages (7-Day, 30-Day)**

```sql
SELECT
  customer_id,
  transaction_date,
  total_amount,

  -- Rolling 7-day average (including current row)
  AVG(total_amount) OVER (
    PARTITION BY customer_id
    ORDER BY transaction_date
    ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
  ) AS avg_amount_7d,

  -- Rolling 30-day average
  AVG(total_amount) OVER (
    PARTITION BY customer_id
    ORDER BY transaction_date
    ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
  ) AS avg_amount_30d,

  -- Running total (cumulative sum)
  SUM(total_amount) OVER (
    PARTITION BY customer_id
    ORDER BY transaction_date
    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
  ) AS cumulative_spent

FROM fact_transactions
ORDER BY customer_id, transaction_date;
```

**Example 3: Transaction Rank and Percentile**

```sql
SELECT
  customer_id,
  transaction_id,
  total_amount,

  -- Rank by amount within customer (1 = highest)
  RANK() OVER (
    PARTITION BY customer_id
    ORDER BY total_amount DESC
  ) AS amount_rank,

  -- Transaction sequence number
  ROW_NUMBER() OVER (
    PARTITION BY customer_id
    ORDER BY transaction_date
  ) AS transaction_number,

  -- Percentile (0.0 - 1.0)
  PERCENT_RANK() OVER (
    PARTITION BY customer_id
    ORDER BY total_amount
  ) AS amount_percentile

FROM fact_transactions;
```

**Window Frame Types:**

```sql
-- ROWS: Physical row count
ROWS BETWEEN 7 PRECEDING AND CURRENT ROW  -- Last 7 rows

-- RANGE: Logical range based on ORDER BY column
RANGE BETWEEN INTERVAL '7 days' PRECEDING AND CURRENT ROW  -- Last 7 days

-- Unbounded
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING  -- Entire partition

-- Examples:
-- Last 3 transactions
ROWS BETWEEN 2 PRECEDING AND CURRENT ROW

-- Next 5 transactions
ROWS BETWEEN CURRENT ROW AND 5 FOLLOWING

-- Previous transaction only
ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING
```

### 2. RFM Analysis (Recency, Frequency, Monetary)

**RFM Definition:**
- **Recency**: How recently did the customer make a purchase?
- **Frequency**: How often does the customer purchase?
- **Monetary**: How much does the customer spend?

**Implementation:**

```sql
WITH customer_rfm AS (
  SELECT
    customer_id,

    -- Recency: Days since last purchase
    DATEDIFF(day, MAX(transaction_date), CURRENT_DATE) AS recency_days,

    -- Frequency: Total number of purchases
    COUNT(DISTINCT transaction_id) AS frequency_count,

    -- Monetary: Total amount spent
    SUM(total_amount) AS monetary_total

  FROM fact_transactions
  WHERE transaction_date >= DATEADD(year, -1, CURRENT_DATE)  -- Last 12 months
  GROUP BY customer_id
),
rfm_scores AS (
  SELECT
    customer_id,
    recency_days,
    frequency_count,
    monetary_total,

    -- RFM scores (1-5, 5 = best)
    -- Recency: Lower is better (recent = higher score)
    NTILE(5) OVER (ORDER BY recency_days DESC) AS recency_score,

    -- Frequency: Higher is better
    NTILE(5) OVER (ORDER BY frequency_count ASC) AS frequency_score,

    -- Monetary: Higher is better
    NTILE(5) OVER (ORDER BY monetary_total ASC) AS monetary_score

  FROM customer_rfm
)
SELECT
  customer_id,
  recency_days,
  frequency_count,
  monetary_total,
  recency_score,
  frequency_score,
  monetary_score,

  -- Combined RFM score (concatenated)
  CONCAT(recency_score, frequency_score, monetary_score) AS rfm_segment,

  -- Weighted RFM score
  (recency_score * 0.5 + frequency_score * 0.3 + monetary_score * 0.2) AS rfm_weighted_score,

  -- Customer segment
  CASE
    WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'
    WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'
    WHEN recency_score >= 4 AND frequency_score <= 2 THEN 'Promising'
    WHEN recency_score <= 2 AND frequency_score >= 4 THEN 'At Risk'
    WHEN recency_score <= 2 AND frequency_score <= 2 THEN 'Lost'
    ELSE 'Others'
  END AS customer_segment

FROM rfm_scores;
```

### 3. Multi-Window Aggregations

**Pattern: Compare multiple time windows (7d vs 30d vs 90d)**

```sql
SELECT
  customer_id,
  transaction_date,

  -- Last 7 days
  COUNT(*) OVER w7 AS transactions_7d,
  SUM(total_amount) OVER w7 AS revenue_7d,
  AVG(total_amount) OVER w7 AS avg_amount_7d,

  -- Last 30 days
  COUNT(*) OVER w30 AS transactions_30d,
  SUM(total_amount) OVER w30 AS revenue_30d,
  AVG(total_amount) OVER w30 AS avg_amount_30d,

  -- Last 90 days
  COUNT(*) OVER w90 AS transactions_90d,
  SUM(total_amount) OVER w90 AS revenue_90d,
  AVG(total_amount) OVER w90 AS avg_amount_90d,

  -- Momentum features (short-term vs long-term)
  SUM(total_amount) OVER w7 / NULLIF(SUM(total_amount) OVER w30, 0) AS revenue_momentum_7d_30d,
  COUNT(*) OVER w7 / NULLIF(COUNT(*) OVER w30, 0) AS transaction_momentum_7d_30d

FROM fact_transactions
WINDOW
  w7 AS (PARTITION BY customer_id ORDER BY transaction_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),
  w30 AS (PARTITION BY customer_id ORDER BY transaction_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW),
  w90 AS (PARTITION BY customer_id ORDER BY transaction_date ROWS BETWEEN 89 PRECEDING AND CURRENT ROW);
```

### 4. Categorical Feature Encoding

**One-Hot Encoding (Pivoting):**

```sql
-- Before: category column with multiple values
customer_id | category
------------|----------
1           | Electronics
1           | Books
2           | Books

-- After: One column per category (binary)
customer_id | has_electronics | has_books | has_clothing
------------|-----------------|-----------|-------------
1           | 1               | 1         | 0
2           | 0               | 1         | 0
```

**Implementation:**

```sql
SELECT
  customer_id,

  -- One-hot encoding using CASE statements
  MAX(CASE WHEN category = 'Electronics' THEN 1 ELSE 0 END) AS has_electronics,
  MAX(CASE WHEN category = 'Books' THEN 1 ELSE 0 END) AS has_books,
  MAX(CASE WHEN category = 'Clothing' THEN 1 ELSE 0 END) AS has_clothing,
  MAX(CASE WHEN category = 'Home' THEN 1 ELSE 0 END) AS has_home,

  -- Count per category
  SUM(CASE WHEN category = 'Electronics' THEN 1 ELSE 0 END) AS count_electronics,
  SUM(CASE WHEN category = 'Books' THEN 1 ELSE 0 END) AS count_books

FROM fact_transactions f
JOIN dim_product p ON f.product_key = p.product_key
GROUP BY customer_id;
```

**Label Encoding (Ordinal):**

```sql
-- Encode categorical values as integers
SELECT
  customer_id,
  customer_segment,

  -- Manual label encoding
  CASE customer_segment
    WHEN 'Platinum' THEN 4
    WHEN 'Gold' THEN 3
    WHEN 'Silver' THEN 2
    WHEN 'Bronze' THEN 1
    ELSE 0
  END AS segment_encoded,

  -- Frequency-based encoding (count of occurrences)
  COUNT(*) OVER (PARTITION BY customer_segment) AS segment_frequency

FROM dim_customer;
```

**Target Encoding (Mean Encoding):**

```sql
-- Encode category by average target value
WITH category_means AS (
  SELECT
    p.category,
    AVG(CASE WHEN f.is_fraud THEN 1.0 ELSE 0.0 END) AS fraud_rate
  FROM fact_transactions f
  JOIN dim_product p ON f.product_key = p.product_key
  GROUP BY p.category
)
SELECT
  f.transaction_id,
  p.category,
  cm.fraud_rate AS category_fraud_rate,  -- Target encoding
  f.is_fraud
FROM fact_transactions f
JOIN dim_product p ON f.product_key = p.product_key
JOIN category_means cm ON p.category = cm.category;
```

### 5. Statistical Features

**Percentiles and Quartiles:**

```sql
SELECT
  customer_id,
  total_amount,

  -- Percentile of this transaction among customer's transactions
  PERCENT_RANK() OVER (
    PARTITION BY customer_id
    ORDER BY total_amount
  ) AS amount_percentile,

  -- Quartile (1-4)
  NTILE(4) OVER (
    PARTITION BY customer_id
    ORDER BY total_amount
  ) AS amount_quartile,

  -- Global percentiles (across all customers)
  PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY total_amount) OVER () AS p25_global,
  PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY total_amount) OVER () AS p50_global,
  PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY total_amount) OVER () AS p75_global,
  PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_amount) OVER () AS p95_global

FROM fact_transactions;
```

**Z-Score (Standardization):**

```sql
WITH stats AS (
  SELECT
    customer_id,
    AVG(total_amount) AS mean_amount,
    STDDEV(total_amount) AS stddev_amount
  FROM fact_transactions
  GROUP BY customer_id
)
SELECT
  f.customer_id,
  f.transaction_id,
  f.total_amount,
  s.mean_amount,
  s.stddev_amount,

  -- Z-score: (value - mean) / stddev
  (f.total_amount - s.mean_amount) / NULLIF(s.stddev_amount, 0) AS amount_zscore,

  -- Flag outliers (|z| > 3)
  CASE
    WHEN ABS((f.total_amount - s.mean_amount) / NULLIF(s.stddev_amount, 0)) > 3 THEN 1
    ELSE 0
  END AS is_outlier

FROM fact_transactions f
JOIN stats s ON f.customer_id = s.customer_id;
```

**Min-Max Scaling (Normalization):**

```sql
WITH min_max AS (
  SELECT
    customer_id,
    MIN(total_amount) AS min_amount,
    MAX(total_amount) AS max_amount
  FROM fact_transactions
  GROUP BY customer_id
)
SELECT
  f.customer_id,
  f.transaction_id,
  f.total_amount,

  -- Min-max scaling: (value - min) / (max - min)
  (f.total_amount - mm.min_amount) / NULLIF(mm.max_amount - mm.min_amount, 0) AS amount_normalized

FROM fact_transactions f
JOIN min_max mm ON f.customer_id = mm.customer_id;
```

### 6. Text Features

**String Length and Word Count:**

```sql
SELECT
  customer_id,
  review_text,

  -- Length in characters
  LENGTH(review_text) AS text_length,

  -- Word count (approximate, by spaces)
  LENGTH(review_text) - LENGTH(REPLACE(review_text, ' ', '')) + 1 AS word_count,

  -- Average word length
  (LENGTH(review_text) - LENGTH(REPLACE(review_text, ' ', '')) + 1) /
    NULLIF(LENGTH(review_text) - LENGTH(REPLACE(review_text, ' ', '')) + 1, 0) AS avg_word_length,

  -- Character diversity (unique characters / total characters)
  LENGTH(REGEXP_REPLACE(review_text, '(.)(.*?)\\1', '\\1', 'g')) /
    NULLIF(LENGTH(review_text), 0) AS char_diversity

FROM customer_reviews;
```

**Pattern Matching and Regex:**

```sql
SELECT
  customer_id,
  review_text,

  -- Contains specific keywords
  CASE WHEN review_text ILIKE '%excellent%' THEN 1 ELSE 0 END AS has_excellent,
  CASE WHEN review_text ILIKE '%poor%' OR review_text ILIKE '%bad%' THEN 1 ELSE 0 END AS has_negative,

  -- Count exclamation marks (sentiment proxy)
  LENGTH(review_text) - LENGTH(REPLACE(review_text, '!', '')) AS exclamation_count,

  -- Contains email address
  CASE WHEN review_text ~ '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}' THEN 1 ELSE 0 END AS has_email,

  -- Extract numbers
  REGEXP_REPLACE(review_text, '[^0-9]', '', 'g') AS extracted_numbers

FROM customer_reviews;
```

**Sentiment Approximation (Keyword-Based):**

```sql
WITH sentiment_keywords AS (
  SELECT
    customer_id,
    review_text,

    -- Positive keywords
    (LENGTH(review_text) - LENGTH(REGEXP_REPLACE(review_text, 'excellent|great|amazing|love|perfect', '', 'gi'))) /
      NULLIF(LENGTH('excellent'), 0) AS positive_count,

    -- Negative keywords
    (LENGTH(review_text) - LENGTH(REGEXP_REPLACE(review_text, 'poor|bad|terrible|hate|awful', '', 'gi'))) /
      NULLIF(LENGTH('poor'), 0) AS negative_count

  FROM customer_reviews
)
SELECT
  customer_id,
  positive_count,
  negative_count,

  -- Sentiment score
  positive_count - negative_count AS sentiment_score,

  -- Sentiment category
  CASE
    WHEN positive_count > negative_count THEN 'Positive'
    WHEN negative_count > positive_count THEN 'Negative'
    ELSE 'Neutral'
  END AS sentiment_category

FROM sentiment_keywords;
```

### 7. Array and JSON Features

**Array Operations:**

```sql
-- Assuming product_categories is an ARRAY column
SELECT
  customer_id,
  product_categories,

  -- Array length
  ARRAY_LENGTH(product_categories, 1) AS category_count,

  -- Check if array contains element
  CASE WHEN 'Electronics' = ANY(product_categories) THEN 1 ELSE 0 END AS has_electronics,

  -- Array to string (for text processing)
  ARRAY_TO_STRING(product_categories, ', ') AS categories_text

FROM customer_preferences;
```

**JSON Extraction (Snowflake/BigQuery):**

```sql
-- Snowflake VARIANT column
SELECT
  customer_id,
  metadata::VARIANT AS metadata,

  -- Extract scalar values
  metadata:age::INT AS age,
  metadata:country::STRING AS country,

  -- Extract nested values
  metadata:preferences:color::STRING AS favorite_color,

  -- Extract array element
  metadata:interests[0]::STRING AS first_interest

FROM customers;
```

**BigQuery JSON Functions:**

```sql
SELECT
  customer_id,
  metadata,

  -- Extract values from JSON string
  JSON_EXTRACT_SCALAR(metadata, '$.age') AS age,
  JSON_EXTRACT_SCALAR(metadata, '$.country') AS country,
  JSON_EXTRACT_ARRAY(metadata, '$.interests') AS interests,

  -- Array length
  ARRAY_LENGTH(JSON_EXTRACT_ARRAY(metadata, '$.interests')) AS interest_count

FROM customers;
```

### 8. Sequence Features (Previous N Events)

**Last N Transactions as Features:**

```sql
WITH ranked_transactions AS (
  SELECT
    customer_id,
    transaction_date,
    product_category,
    total_amount,
    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY transaction_date DESC) AS rn
  FROM fact_transactions
)
SELECT
  customer_id,

  -- Last 3 transaction amounts
  MAX(CASE WHEN rn = 1 THEN total_amount END) AS last_1_amount,
  MAX(CASE WHEN rn = 2 THEN total_amount END) AS last_2_amount,
  MAX(CASE WHEN rn = 3 THEN total_amount END) AS last_3_amount,

  -- Last 3 product categories
  MAX(CASE WHEN rn = 1 THEN product_category END) AS last_1_category,
  MAX(CASE WHEN rn = 2 THEN product_category END) AS last_2_category,
  MAX(CASE WHEN rn = 3 THEN product_category END) AS last_3_category,

  -- Days between last 2 transactions
  MAX(CASE WHEN rn = 1 THEN transaction_date END) - MAX(CASE WHEN rn = 2 THEN transaction_date END) AS days_between_last_2

FROM ranked_transactions
WHERE rn <= 3
GROUP BY customer_id;
```

**Event Sequence as Array (for Sequence Models):**

```sql
SELECT
  customer_id,

  -- Array of last 10 product IDs (in order)
  ARRAY_AGG(product_id ORDER BY transaction_date DESC LIMIT 10) AS last_10_products,

  -- Array of last 10 transaction amounts
  ARRAY_AGG(total_amount ORDER BY transaction_date DESC LIMIT 10) AS last_10_amounts,

  -- Concatenated sequence (for text-based models)
  STRING_AGG(product_id::TEXT, ',' ORDER BY transaction_date DESC LIMIT 10) AS product_sequence

FROM fact_transactions
GROUP BY customer_id;
```

---

## 💡 Practical Examples

### Example 1: Comprehensive Customer Features for Churn Prediction

```sql
WITH customer_transactions AS (
  -- Base transaction data with point-in-time customer attributes
  SELECT
    f.customer_id,
    f.transaction_id,
    f.transaction_date,
    f.total_amount,
    f.quantity,
    p.category,
    c.customer_segment,
    c.signup_date
  FROM fact_transactions f
  JOIN dim_customer c
    ON f.customer_key = c.customer_key
    AND f.transaction_date >= c.effective_date
    AND f.transaction_date < c.expiration_date
  JOIN dim_product p ON f.product_key = p.product_key
  WHERE f.transaction_date >= DATEADD(year, -1, CURRENT_DATE)
),
customer_features AS (
  SELECT
    customer_id,
    MAX(signup_date) AS signup_date,
    MAX(customer_segment) AS customer_segment,

    -- === RECENCY FEATURES ===
    DATEDIFF(day, MAX(transaction_date), CURRENT_DATE) AS days_since_last_purchase,
    DATEDIFF(day, MIN(transaction_date), CURRENT_DATE) AS days_since_first_purchase,

    -- === FREQUENCY FEATURES ===
    COUNT(DISTINCT transaction_id) AS total_transactions,
    COUNT(DISTINCT DATE_TRUNC('month', transaction_date)) AS active_months,
    COUNT(DISTINCT transaction_id) /
      NULLIF(DATEDIFF(month, MIN(transaction_date), MAX(transaction_date)), 0) AS avg_transactions_per_month,

    -- === MONETARY FEATURES ===
    SUM(total_amount) AS total_spent,
    AVG(total_amount) AS avg_transaction_amount,
    STDDEV(total_amount) AS stddev_transaction_amount,
    MIN(total_amount) AS min_transaction_amount,
    MAX(total_amount) AS max_transaction_amount,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_amount) AS median_transaction_amount,

    -- === PRODUCT FEATURES ===
    COUNT(DISTINCT category) AS distinct_categories,
    MODE() WITHIN GROUP (ORDER BY category) AS favorite_category,
    SUM(CASE WHEN category = 'Electronics' THEN 1 ELSE 0 END) AS electronics_count,
    SUM(CASE WHEN category = 'Books' THEN 1 ELSE 0 END) AS books_count,

    -- === TEMPORAL FEATURES ===
    AVG(DATEDIFF(day,
      LAG(transaction_date) OVER (PARTITION BY customer_id ORDER BY transaction_date),
      transaction_date
    )) AS avg_days_between_purchases,

    -- === RECENT ACTIVITY (Last 30 days) ===
    COUNT(DISTINCT CASE WHEN transaction_date >= DATEADD(day, -30, CURRENT_DATE)
      THEN transaction_id END) AS transactions_last_30d,
    SUM(CASE WHEN transaction_date >= DATEADD(day, -30, CURRENT_DATE)
      THEN total_amount ELSE 0 END) AS spent_last_30d,

    -- === TREND FEATURES ===
    -- Compare recent (30d) vs historical (90d)
    COUNT(DISTINCT CASE WHEN transaction_date >= DATEADD(day, -30, CURRENT_DATE)
      THEN transaction_id END) /
    NULLIF(COUNT(DISTINCT CASE WHEN transaction_date >= DATEADD(day, -90, CURRENT_DATE)
      THEN transaction_id END), 0) AS transaction_momentum_30d_90d

  FROM customer_transactions
  GROUP BY customer_id
)
SELECT
  cf.*,

  -- === DERIVED FEATURES ===
  -- Customer lifetime value per day
  cf.total_spent / NULLIF(cf.days_since_first_purchase, 0) AS daily_ltv,

  -- Transaction frequency score (higher = more frequent)
  CASE
    WHEN cf.avg_days_between_purchases < 7 THEN 5
    WHEN cf.avg_days_between_purchases < 14 THEN 4
    WHEN cf.avg_days_between_purchases < 30 THEN 3
    WHEN cf.avg_days_between_purchases < 60 THEN 2
    ELSE 1
  END AS frequency_score,

  -- Engagement score (recency + frequency + monetary)
  (CASE WHEN cf.days_since_last_purchase < 30 THEN 5
        WHEN cf.days_since_last_purchase < 60 THEN 3
        ELSE 1 END) +
  (CASE WHEN cf.total_transactions > 50 THEN 5
        WHEN cf.total_transactions > 20 THEN 3
        ELSE 1 END) +
  (CASE WHEN cf.total_spent > 1000 THEN 5
        WHEN cf.total_spent > 500 THEN 3
        ELSE 1 END) AS engagement_score

FROM customer_features cf;
```

### Example 2: Product Recommendation Features

```sql
WITH user_product_interactions AS (
  -- User-product interaction matrix
  SELECT
    f.customer_id,
    p.product_id,
    p.category,
    COUNT(*) AS purchase_count,
    SUM(f.total_amount) AS total_spent,
    MAX(f.transaction_date) AS last_purchase_date
  FROM fact_transactions f
  JOIN dim_product p ON f.product_key = p.product_key
  WHERE f.transaction_date >= DATEADD(year, -1, CURRENT_DATE)
  GROUP BY f.customer_id, p.product_id, p.category
),
product_popularity AS (
  -- Global product popularity
  SELECT
    product_id,
    COUNT(DISTINCT customer_id) AS unique_buyers,
    SUM(purchase_count) AS total_purchases,
    AVG(total_spent) AS avg_spent_per_customer
  FROM user_product_interactions
  GROUP BY product_id
),
user_category_affinity AS (
  -- User affinity for each category
  SELECT
    customer_id,
    category,
    SUM(purchase_count) AS category_purchases,
    SUM(total_spent) AS category_spent,
    COUNT(DISTINCT product_id) AS distinct_products_in_category
  FROM user_product_interactions
  GROUP BY customer_id, category
)
SELECT
  upi.customer_id,
  upi.product_id,
  upi.category,

  -- === USER-PRODUCT FEATURES ===
  upi.purchase_count,
  upi.total_spent,
  DATEDIFF(day, upi.last_purchase_date, CURRENT_DATE) AS days_since_last_purchase,

  -- === PRODUCT POPULARITY FEATURES ===
  pp.unique_buyers,
  pp.total_purchases,
  pp.avg_spent_per_customer,

  -- Relative popularity (user vs global)
  upi.total_spent / NULLIF(pp.avg_spent_per_customer, 0) AS spending_ratio_vs_avg,

  -- === CATEGORY AFFINITY FEATURES ===
  uca.category_purchases,
  uca.category_spent,
  uca.distinct_products_in_category,

  -- User's affinity for this category (spending share)
  uca.category_spent / NULLIF(SUM(uca.category_spent) OVER (PARTITION BY upi.customer_id), 0) AS category_spend_share,

  -- === COLLABORATIVE FILTERING HINTS ===
  -- How many users bought this product?
  pp.unique_buyers AS cobuyer_count,

  -- Product diversity in category (more = exploratory user)
  uca.distinct_products_in_category / NULLIF(uca.category_purchases, 0) AS category_exploration_ratio

FROM user_product_interactions upi
JOIN product_popularity pp ON upi.product_id = pp.product_id
JOIN user_category_affinity uca ON upi.customer_id = uca.customer_id AND upi.category = uca.category;
```

### Example 3: Time-Series Features for Fraud Detection

```sql
WITH transaction_sequences AS (
  SELECT
    customer_id,
    transaction_id,
    transaction_timestamp,
    total_amount,
    payment_method,
    device_type,

    -- Time-based features
    EXTRACT(HOUR FROM transaction_timestamp) AS hour_of_day,
    EXTRACT(DOW FROM transaction_timestamp) AS day_of_week,

    -- Sequence features
    ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY transaction_timestamp) AS transaction_seq,

    LAG(transaction_timestamp) OVER (PARTITION BY customer_id ORDER BY transaction_timestamp) AS prev_transaction_time,
    LAG(total_amount) OVER (PARTITION BY customer_id ORDER BY transaction_timestamp) AS prev_amount,
    LAG(payment_method) OVER (PARTITION BY customer_id ORDER BY transaction_timestamp) AS prev_payment_method

  FROM fact_transactions
  WHERE transaction_timestamp >= DATEADD(year, -1, CURRENT_DATE)
),
transaction_features AS (
  SELECT
    ts.*,

    -- === VELOCITY FEATURES ===
    -- Time since last transaction (seconds)
    EXTRACT(EPOCH FROM (ts.transaction_timestamp - ts.prev_transaction_time)) AS seconds_since_last_tx,

    -- Transaction velocity (transactions per hour)
    COUNT(*) OVER (
      PARTITION BY ts.customer_id
      ORDER BY ts.transaction_timestamp
      RANGE BETWEEN INTERVAL '1 hour' PRECEDING AND CURRENT ROW
    ) AS transactions_last_hour,

    COUNT(*) OVER (
      PARTITION BY ts.customer_id
      ORDER BY ts.transaction_timestamp
      RANGE BETWEEN INTERVAL '24 hours' PRECEDING AND CURRENT ROW
    ) AS transactions_last_24h,

    -- === AMOUNT ANOMALY FEATURES ===
    -- Amount change vs previous
    (ts.total_amount - ts.prev_amount) / NULLIF(ts.prev_amount, 0) AS amount_change_pct,

    -- Z-score of amount (rolling 30-day window)
    (ts.total_amount -
      AVG(ts.total_amount) OVER (
        PARTITION BY ts.customer_id
        ORDER BY ts.transaction_timestamp
        ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING
      )
    ) / NULLIF(
      STDDEV(ts.total_amount) OVER (
        PARTITION BY ts.customer_id
        ORDER BY ts.transaction_timestamp
        ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING
      ), 0
    ) AS amount_zscore_30d,

    -- === BEHAVIORAL CHANGE FEATURES ===
    -- Payment method change
    CASE WHEN ts.payment_method != ts.prev_payment_method THEN 1 ELSE 0 END AS payment_method_changed,

    -- Unusual hour flag
    CASE WHEN EXTRACT(HOUR FROM ts.transaction_timestamp) BETWEEN 1 AND 5 THEN 1 ELSE 0 END AS is_unusual_hour,

    -- === SEQUENCE FEATURES ===
    -- Average transaction amount (last 10 transactions)
    AVG(ts.total_amount) OVER (
      PARTITION BY ts.customer_id
      ORDER BY ts.transaction_timestamp
      ROWS BETWEEN 9 PRECEDING AND CURRENT ROW
    ) AS avg_amount_last_10_tx

  FROM transaction_sequences ts
)
SELECT
  tf.*,

  -- === DERIVED RISK FEATURES ===
  -- High velocity flag
  CASE WHEN tf.transactions_last_hour >= 5 THEN 1 ELSE 0 END AS high_velocity_flag,

  -- Amount spike flag
  CASE WHEN ABS(tf.amount_zscore_30d) > 3 THEN 1 ELSE 0 END AS amount_spike_flag,

  -- Rapid succession flag (< 1 minute since last)
  CASE WHEN tf.seconds_since_last_tx < 60 THEN 1 ELSE 0 END AS rapid_succession_flag,

  -- Composite risk score
  (CASE WHEN tf.transactions_last_hour >= 5 THEN 3 ELSE 0 END) +
  (CASE WHEN ABS(tf.amount_zscore_30d) > 3 THEN 3 ELSE 0 END) +
  (CASE WHEN tf.payment_method_changed = 1 THEN 2 ELSE 0 END) +
  (CASE WHEN tf.is_unusual_hour = 1 THEN 1 ELSE 0 END) AS risk_score

FROM transaction_features tf;
```

### Example 4: Modular Feature Pipeline with CTEs

```sql
-- Modular, reusable feature engineering pipeline
WITH
-- Step 1: Base transactions
base_transactions AS (
  SELECT
    f.customer_id,
    f.transaction_id,
    f.transaction_date,
    f.total_amount,
    p.category,
    c.customer_segment,
    c.signup_date
  FROM fact_transactions f
  JOIN dim_customer c ON f.customer_key = c.customer_key
  JOIN dim_product p ON f.product_key = p.product_key
  WHERE f.transaction_date >= '2024-01-01'
),

-- Step 2: Recency features
recency_features AS (
  SELECT
    customer_id,
    DATEDIFF(day, MAX(transaction_date), CURRENT_DATE) AS days_since_last_purchase,
    DATEDIFF(day, MIN(transaction_date), MAX(transaction_date)) AS customer_lifetime_days
  FROM base_transactions
  GROUP BY customer_id
),

-- Step 3: Frequency features
frequency_features AS (
  SELECT
    customer_id,
    COUNT(DISTINCT transaction_id) AS total_transactions,
    COUNT(DISTINCT DATE_TRUNC('month', transaction_date)) AS active_months,
    AVG(DATEDIFF(day,
      LAG(transaction_date) OVER (PARTITION BY customer_id ORDER BY transaction_date),
      transaction_date
    )) AS avg_days_between_purchases
  FROM base_transactions
  GROUP BY customer_id
),

-- Step 4: Monetary features
monetary_features AS (
  SELECT
    customer_id,
    SUM(total_amount) AS total_spent,
    AVG(total_amount) AS avg_transaction_amount,
    STDDEV(total_amount) AS stddev_transaction_amount
  FROM base_transactions
  GROUP BY customer_id
),

-- Step 5: Category features
category_features AS (
  SELECT
    customer_id,
    COUNT(DISTINCT category) AS distinct_categories,
    MODE() WITHIN GROUP (ORDER BY category) AS favorite_category,
    MAX(CASE WHEN category = 'Electronics' THEN 1 ELSE 0 END) AS purchased_electronics
  FROM base_transactions
  GROUP BY customer_id
),

-- Step 6: Trend features
trend_features AS (
  SELECT
    customer_id,
    COUNT(CASE WHEN transaction_date >= DATEADD(day, -30, CURRENT_DATE) THEN 1 END) AS transactions_last_30d,
    COUNT(CASE WHEN transaction_date >= DATEADD(day, -90, CURRENT_DATE) THEN 1 END) AS transactions_last_90d
  FROM base_transactions
  GROUP BY customer_id
)

-- Final: Combine all feature sets
SELECT
  bt.customer_id,
  MAX(bt.customer_segment) AS customer_segment,

  -- Recency
  rf.days_since_last_purchase,
  rf.customer_lifetime_days,

  -- Frequency
  ff.total_transactions,
  ff.active_months,
  ff.avg_days_between_purchases,

  -- Monetary
  mf.total_spent,
  mf.avg_transaction_amount,
  mf.stddev_transaction_amount,

  -- Category
  cf.distinct_categories,
  cf.favorite_category,
  cf.purchased_electronics,

  -- Trend
  tf.transactions_last_30d,
  tf.transactions_last_90d,
  tf.transactions_last_30d / NULLIF(tf.transactions_last_90d, 0) AS transaction_momentum

FROM base_transactions bt
LEFT JOIN recency_features rf ON bt.customer_id = rf.customer_id
LEFT JOIN frequency_features ff ON bt.customer_id = ff.customer_id
LEFT JOIN monetary_features mf ON bt.customer_id = mf.customer_id
LEFT JOIN category_features cf ON bt.customer_id = cf.customer_id
LEFT JOIN trend_features tf ON bt.customer_id = tf.customer_id
GROUP BY
  bt.customer_id,
  rf.days_since_last_purchase, rf.customer_lifetime_days,
  ff.total_transactions, ff.active_months, ff.avg_days_between_purchases,
  mf.total_spent, mf.avg_transaction_amount, mf.stddev_transaction_amount,
  cf.distinct_categories, cf.favorite_category, cf.purchased_electronics,
  tf.transactions_last_30d, tf.transactions_last_90d;
```

---

## 🔧 Code Examples

### Code Example 1: Feature Engineering Framework (Python + SQL)

```python
import pandas as pd
from typing import List, Dict, Any
from dataclasses import dataclass

@dataclass
class FeatureDefinition:
    """Define a feature with SQL template"""
    name: str
    sql_expression: str
    description: str
    feature_type: str  # 'numerical', 'categorical', 'temporal'

class FeatureEngineering:
    """SQL-based feature engineering framework"""

    def __init__(self, conn):
        self.conn = conn
        self.features: List[FeatureDefinition] = []

    def register_feature(self, name: str, sql_expr: str, description: str, feature_type: str):
        """Register a feature definition"""
        feature = FeatureDefinition(name, sql_expr, description, feature_type)
        self.features.append(feature)
        print(f"✅ Registered feature: {name}")

    def register_feature_set(self, feature_set: Dict[str, Dict[str, Any]]):
        """Register multiple features at once"""
        for name, config in feature_set.items():
            self.register_feature(
                name=name,
                sql_expr=config['sql'],
                description=config.get('description', ''),
                feature_type=config.get('type', 'numerical')
            )

    def generate_feature_query(self, base_table: str, entity_key: str) -> str:
        """Generate SQL query with all registered features"""

        feature_selects = [f"{f.sql_expression} AS {f.name}" for f in self.features]

        query = f"""
        SELECT
          {entity_key},
          {',\\n          '.join(feature_selects)}
        FROM {base_table}
        GROUP BY {entity_key}
        """

        return query

    def extract_features(self, base_table: str, entity_key: str) -> pd.DataFrame:
        """Extract features and return DataFrame"""
        query = self.generate_feature_query(base_table, entity_key)

        print(f"🔄 Extracting {len(self.features)} features...")
        df = pd.read_sql(query, self.conn)
        print(f"✅ Extracted features for {len(df)} entities")

        return df

    def get_feature_metadata(self) -> pd.DataFrame:
        """Get metadata about registered features"""
        metadata = []
        for f in self.features:
            metadata.append({
                'feature_name': f.name,
                'feature_type': f.feature_type,
                'description': f.description,
                'sql_expression': f.sql_expression
            })
        return pd.DataFrame(metadata)

# Usage
import psycopg2

conn = psycopg2.connect(
    host='warehouse.example.com',
    database='ml_features',
    user='data_scientist',
    password='password'
)

# Initialize framework
fe = FeatureEngineering(conn)

# Define feature set
customer_features = {
    'days_since_last_purchase': {
        'sql': 'DATEDIFF(day, MAX(transaction_date), CURRENT_DATE)',
        'description': 'Days since most recent transaction',
        'type': 'numerical'
    },
    'total_transactions': {
        'sql': 'COUNT(DISTINCT transaction_id)',
        'description': 'Total number of transactions',
        'type': 'numerical'
    },
    'total_spent': {
        'sql': 'SUM(total_amount)',
        'description': 'Total lifetime spending',
        'type': 'numerical'
    },
    'avg_order_value': {
        'sql': 'AVG(total_amount)',
        'description': 'Average transaction amount',
        'type': 'numerical'
    },
    'favorite_category': {
        'sql': "MODE() WITHIN GROUP (ORDER BY category)",
        'description': 'Most frequently purchased category',
        'type': 'categorical'
    },
    'is_high_value': {
        'sql': 'CASE WHEN SUM(total_amount) > 1000 THEN 1 ELSE 0 END',
        'description': 'Flag for high-value customers (>$1000 LTV)',
        'type': 'categorical'
    }
}

# Register features
fe.register_feature_set(customer_features)

# Generate and execute query
features_df = fe.extract_features(
    base_table='fact_transactions f JOIN dim_product p ON f.product_key = p.product_key',
    entity_key='f.customer_id'
)

# View feature metadata
metadata_df = fe.get_feature_metadata()
print(metadata_df)

# Export features
features_df.to_csv('customer_features.csv', index=False)
print(f"✅ Exported {len(features_df)} customer features to CSV")
```

### Code Example 2: Parameterized Feature Query Generator

```python
from string import Template

class ParameterizedFeatures:
    """Generate parameterized SQL queries for feature engineering"""

    @staticmethod
    def rolling_aggregations(
        table: str,
        entity_col: str,
        value_col: str,
        timestamp_col: str,
        windows: List[int]
    ) -> str:
        """Generate rolling window features for multiple time windows"""

        feature_clauses = []

        for window_days in windows:
            feature_clauses.append(f"""
    -- {window_days}-day features
    COUNT(*) OVER (
      PARTITION BY {entity_col}
      ORDER BY {timestamp_col}
      RANGE BETWEEN INTERVAL '{window_days} days' PRECEDING AND CURRENT ROW
    ) AS count_{window_days}d,

    SUM({value_col}) OVER (
      PARTITION BY {entity_col}
      ORDER BY {timestamp_col}
      RANGE BETWEEN INTERVAL '{window_days} days' PRECEDING AND CURRENT ROW
    ) AS sum_{window_days}d,

    AVG({value_col}) OVER (
      PARTITION BY {entity_col}
      ORDER BY {timestamp_col}
      RANGE BETWEEN INTERVAL '{window_days} days' PRECEDING AND CURRENT ROW
    ) AS avg_{window_days}d
            """.strip())

        query = f"""
SELECT
  {entity_col},
  {timestamp_col},
  {value_col},
  {',\\n  '.join(feature_clauses)}
FROM {table}
ORDER BY {entity_col}, {timestamp_col}
        """

        return query

    @staticmethod
    def categorical_encoding(
        table: str,
        entity_col: str,
        category_col: str,
        categories: List[str],
        encoding_type: str = 'one_hot'
    ) -> str:
        """Generate categorical encoding features"""

        if encoding_type == 'one_hot':
            feature_clauses = [
                f"MAX(CASE WHEN {category_col} = '{cat}' THEN 1 ELSE 0 END) AS has_{cat.lower().replace(' ', '_')}"
                for cat in categories
            ]
        elif encoding_type == 'count':
            feature_clauses = [
                f"SUM(CASE WHEN {category_col} = '{cat}' THEN 1 ELSE 0 END) AS count_{cat.lower().replace(' ', '_')}"
                for cat in categories
            ]
        else:
            raise ValueError(f"Unknown encoding_type: {encoding_type}")

        query = f"""
SELECT
  {entity_col},
  {',\\n  '.join(feature_clauses)}
FROM {table}
GROUP BY {entity_col}
        """

        return query

# Usage
pf = ParameterizedFeatures()

# Generate rolling window features
rolling_query = pf.rolling_aggregations(
    table='fact_transactions',
    entity_col='customer_id',
    value_col='total_amount',
    timestamp_col='transaction_date',
    windows=[7, 30, 90]
)

print("Rolling Aggregation Query:")
print(rolling_query)

# Generate categorical encoding
category_query = pf.categorical_encoding(
    table='fact_transactions f JOIN dim_product p ON f.product_key = p.product_key',
    entity_col='f.customer_id',
    category_col='p.category',
    categories=['Electronics', 'Books', 'Clothing', 'Home'],
    encoding_type='one_hot'
)

print("\nCategorical Encoding Query:")
print(category_query)
```

---

## ✅ Best Practices

### 1. Window Functions

**DO:**
- ✅ Use PARTITION BY to compute features per entity (customer, product)
- ✅ Use ROWS BETWEEN for physical row counts, RANGE BETWEEN for time-based windows
- ✅ Name window clauses with WINDOW for readability
- ✅ Avoid window functions on unsorted data (add ORDER BY)

**DON'T:**
- ❌ Use window functions without ORDER BY (undefined behavior)
- ❌ Forget NULLIF when dividing (prevents divide-by-zero errors)
- ❌ Apply window functions to already-aggregated data
- ❌ Nest window functions (not allowed in SQL)

### 2. CTEs and Modularity

**DO:**
- ✅ Break complex queries into multiple CTEs (one per feature set)
- ✅ Name CTEs descriptively (recency_features, frequency_features)
- ✅ Use CTEs for readability, even if not performance-critical
- ✅ Document each CTE with comments

**DON'T:**
- ❌ Create overly complex single queries (hard to debug)
- ❌ Reuse CTE names (confusing)
- ❌ Forget that CTEs are evaluated once (good for performance)
- ❌ Use subqueries when CTEs are clearer

### 3. Point-in-Time Correctness

**DO:**
- ✅ Always join dimensions with temporal predicates (effective_date, expiration_date)
- ✅ Use transaction event_timestamp, not processing_timestamp
- ✅ Compute features using only past data (no future leakage)
- ✅ Test for data leakage (check feature values make sense temporally)

**DON'T:**
- ❌ Use CURRENT_DATE in training feature queries (breaks reproducibility)
- ❌ Join on is_current = TRUE without checking dates
- ❌ Include future events in rolling windows
- ❌ Forget to filter by date range

### 4. Performance Optimization

**DO:**
- ✅ Use materialized views for expensive features
- ✅ Partition tables by date for time-based queries
- ✅ Index columns used in JOINs and WHERE clauses
- ✅ Use approximate functions when exact not needed (APPROX_COUNT_DISTINCT)
- ✅ Limit data scanned (filter by date early in query)

**DON'T:**
- ❌ Compute all features in single query (split by update frequency)
- ❌ Recompute static features daily (cache and reuse)
- ❌ Use SELECT * (specify only needed columns)
- ❌ Forget to EXPLAIN queries before running on production

---

## 📝 Key Takeaways

1. **Window functions are essential**: ROW_NUMBER, RANK, LAG, LEAD, and aggregate OVER enable temporal features without self-joins.

2. **RFM analysis in SQL**: Recency, Frequency, Monetary can be computed with DATEDIFF, COUNT, SUM and NTILE for customer segmentation.

3. **Multi-window features**: Compare 7d vs 30d vs 90d metrics to capture trends and momentum (short-term / long-term ratios).

4. **CTEs improve readability**: Break complex feature engineering into modular steps (recency_features, frequency_features, monetary_features).

5. **Categorical encoding in SQL**: One-hot encoding with CASE statements, label encoding with ordinal mapping, target encoding with AVG.

6. **Statistical features**: Z-scores, percentiles, min-max scaling can all be computed in SQL using STDDEV, PERCENTILE_CONT, MIN, MAX.

7. **Text features**: String length, word count, regex patterns, and keyword sentiment approximations enable NLP in SQL.

8. **Sequence features**: Last N events can be pivoted as features using ROW_NUMBER and CASE, or aggregated as ARRAY_AGG.

9. **Point-in-time correctness**: Always use event timestamps and temporal joins to prevent data leakage. Test features make temporal sense.

10. **Modular pipelines**: Register features with metadata (name, SQL expression, type) for version control and reproducibility.

---

## ✏️ Notes Section

**Personal Insights:**

---

**Questions:**

---

**Action Items:**

---

**Related Projects:**

---

**Code Snippets:**

---

**Further Exploration:**

---

**📌 Tags:** `#feature-engineering` `#sql` `#window-functions` `#analytics` `#ml-features` `#data-warehousing` `#cte` `#query-optimization`

---

**Navigation:**
- ← Previous: [05.02 - Dimensional Modeling for ML](02.%20Dimensional%20Modeling%20for%20ML.md)
- → Next: [05.04 - Integration with ML Platforms](04.%20Integration%20with%20ML%20Platforms.md)
- ↑ Up: [Chapter 05 - Data Warehousing for Analytics & ML](README.md)
- ⌂ Home: [DEforAI Course Index](../../README.md)

---

*Last updated: 2025-10-20*
