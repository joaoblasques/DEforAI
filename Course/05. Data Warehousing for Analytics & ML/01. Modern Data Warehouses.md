# Modern Data Warehouses

**Course:** Data Engineering for AI/ML
**Chapter:** 05 - Data Warehousing for Analytics & ML
**Subchapter:** 01 - Modern Data Warehouses
**Created:** 2025-10-20
**Updated:** 2025-10-20

---

## 📋 Overview

Modern cloud data warehouses have revolutionized how organizations handle analytics and ML workloads. Unlike traditional on-premises warehouses (Oracle Exadata, Teradata), cloud data warehouses offer:

- **Elastic compute and storage**: Scale up/down in seconds, pay only for what you use
- **Separation of compute and storage**: Multiple workloads can access same data without contention
- **Automatic optimization**: Query optimization, caching, and clustering without manual tuning
- **Native ML integration**: Built-in ML functions, direct connections to ML platforms
- **Petabyte-scale performance**: Handle ML training datasets (10-100 TB) with sub-second query latency

For ML workloads specifically, modern data warehouses excel at:
- **Feature engineering at scale**: SQL-based transformations on billions of rows
- **Analytical feature computation**: Window functions, aggregations, time-series operations
- **Training data preparation**: Joining multiple data sources, sampling, filtering
- **Feature serving for batch inference**: Real-time analytical queries for feature lookups
- **Experimentation and exploration**: Ad-hoc queries on massive datasets without data movement

The three dominant players—**Snowflake**, **Google BigQuery**, and **AWS Redshift**—each have distinct architectures and trade-offs. Understanding these differences is critical for selecting the right warehouse for ML workloads.

**Key Topics:**
- Cloud data warehouse architectures (shared-disk vs shared-nothing vs multi-cluster)
- Snowflake architecture and features
- BigQuery architecture and features
- Redshift architecture and features
- Cost models and pricing strategies
- Performance characteristics for ML workloads
- ML-specific capabilities (UDFs, external functions, vector operations)
- Integration with ML platforms

**Prerequisites:**
- Understanding of SQL and relational databases
- Knowledge of data lakes and storage systems (Chapter 04)
- Familiarity with ML data pipelines (Chapter 02)
- Basic understanding of cloud computing

---

## 🎯 Learning Objectives

By the end of this subchapter, you will be able to:

1. **Compare architectures** of Snowflake, BigQuery, and Redshift for ML use cases
2. **Select the right warehouse** based on workload characteristics (batch vs interactive, query patterns)
3. **Understand cost models** and estimate monthly costs for typical ML workloads
4. **Leverage ML-specific features** (UDFs, external functions, array operations, ML.* functions)
5. **Design warehouse schemas** optimized for feature engineering and training data preparation
6. **Optimize query performance** using clustering, partitioning, and materialized views
7. **Integrate warehouses** with ML platforms (SageMaker, Vertex AI, Databricks)
8. **Implement data governance** (access control, auditing, data masking)
9. **Monitor warehouse performance** and costs (query profiling, cost attribution)
10. **Apply best practices** for ML analytics workloads

---

## 📚 Core Concepts

### 1. Cloud Data Warehouse Architecture Patterns

**Traditional (On-Premises) Warehouse:**

```
┌─────────────────────────────────────┐
│   Monolithic Architecture           │
│                                     │
│   ┌─────────────────────────────┐  │
│   │  Tightly Coupled             │  │
│   │  Compute + Storage           │  │
│   │                              │  │
│   │  - Fixed capacity            │  │
│   │  - Manual tuning             │  │
│   │  - Expensive scaling         │  │
│   └─────────────────────────────┘  │
└─────────────────────────────────────┘

Examples: Teradata, Oracle Exadata, Netezza
```

**Modern Cloud Warehouse:**

```
┌─────────────────────────────────────────────┐
│   Decoupled Architecture                    │
│                                             │
│   ┌─────────────┐       ┌──────────────┐  │
│   │  Compute    │       │  Storage     │  │
│   │  (Elastic)  │◄─────►│  (Object)    │  │
│   │             │       │              │  │
│   │  Scale      │       │  Unlimited   │  │
│   │  Up/Down    │       │  Cheap       │  │
│   └─────────────┘       └──────────────┘  │
│                                             │
│   ┌──────────────────────────────────────┐ │
│   │  Metadata Layer                       │ │
│   │  (Schema, Stats, Lineage)            │ │
│   └──────────────────────────────────────┘ │
└─────────────────────────────────────────────┘

Examples: Snowflake, BigQuery, Redshift
```

**Key Advantages:**
- **Independent scaling**: Scale compute without moving data
- **Cost efficiency**: Pay for compute only when running queries
- **Concurrency**: Multiple compute clusters access same data
- **Durability**: Object storage (S3, GCS) provides 99.999999999% durability

### 2. Snowflake Architecture

**Multi-Cluster Shared Data Architecture:**

```
┌───────────────────────────────────────────────────────┐
│                  SNOWFLAKE                             │
│                                                        │
│  ┌──────────────────────────────────────────────┐    │
│  │  Services Layer (Cloud Services)              │    │
│  │  - Authentication, metadata, query optimizer  │    │
│  │  - Transaction management, result caching     │    │
│  └────────────────┬─────────────────────────────┘    │
│                   │                                    │
│  ┌────────────────▼─────────────────────────────┐    │
│  │  Compute Layer (Virtual Warehouses)           │    │
│  │                                                │    │
│  │  ┌────────┐  ┌────────┐  ┌────────┐          │    │
│  │  │ WH-ML  │  │ WH-ETL │  │ WH-BI  │ ...      │    │
│  │  │(L-size)│  │(XL-size)│  │(M-size)│          │    │
│  │  └────┬───┘  └────┬───┘  └────┬───┘          │    │
│  └───────┼───────────┼───────────┼───────────────┘    │
│          │           │           │                     │
│  ┌───────▼───────────▼───────────▼───────────────┐    │
│  │  Storage Layer (Micro-Partitions)              │    │
│  │                                                 │    │
│  │  S3/Azure Blob/GCS                             │    │
│  │  - Columnar storage                            │    │
│  │  - Automatic clustering                        │    │
│  │  - Time travel, cloning                        │    │
│  └─────────────────────────────────────────────────┘   │
└────────────────────────────────────────────────────────┘
```

**Key Features:**

1. **Virtual Warehouses**: Independent compute clusters
   - T-shirt sizing: X-Small → 6X-Large
   - Auto-suspend/resume (save costs)
   - Multi-cluster warehouses (auto-scale for concurrency)

2. **Micro-Partitions**: 50-500 MB compressed files
   - Automatic partitioning (no manual tuning)
   - Columnar storage with metadata (min/max values, bloom filters)
   - Pruning based on query predicates

3. **Time Travel**: Query historical data (1-90 days)
   - Rollback to previous versions
   - Audit and compliance
   - Undrop tables

4. **Zero-Copy Cloning**: Instant table/schema/database copies
   - No data duplication
   - Useful for ML experimentation

5. **Result Caching**: 24-hour query result cache
   - Free tier (no compute charges for cached results)

**ML-Specific Capabilities:**

- **External Functions**: Call external APIs (SageMaker, Vertex AI) from SQL
- **UDFs (Python, Java, JavaScript)**: Custom feature transformations
- **Snowpark**: DataFrame API for Python/Scala (like Spark)
- **ARRAY/OBJECT support**: Semi-structured data for embeddings
- **Geospatial functions**: Location-based features

**Cost Model:**

```
Monthly Cost = Compute Cost + Storage Cost + Cloud Services Cost

Compute: $2-4 per credit (varies by cloud/region)
- X-Small: 1 credit/hour
- Small: 2 credits/hour
- Medium: 4 credits/hour
- Large: 8 credits/hour
- X-Large: 16 credits/hour

Storage: $23-40 per TB/month (compressed)

Cloud Services: 10% of compute (typically included)
```

**Example Cost:**
```python
# ML training data preparation
# Dataset: 10 TB
# Query frequency: 100 queries/day (1 min each)
# Warehouse: Large (8 credits/hour)

daily_compute_hours = (100 queries * 1 min) / 60 = 1.67 hours
daily_compute_cost = 1.67 * 8 credits * $3/credit = $40
monthly_compute_cost = $40 * 30 = $1,200

storage_cost = 10 TB * $23/TB = $230

total_monthly = $1,200 + $230 = $1,430
```

### 3. Google BigQuery Architecture

**Serverless Architecture:**

```
┌─────────────────────────────────────────────────────┐
│                  BIGQUERY                            │
│                                                      │
│  ┌──────────────────────────────────────────────┐  │
│  │  Dremel Engine (Query Execution)              │  │
│  │  - Columnar scan, multi-level tree             │  │
│  │  - Automatic parallelization                   │  │
│  │  - Serverless (no cluster management)          │  │
│  └────────────────┬─────────────────────────────┘  │
│                   │                                  │
│  ┌────────────────▼─────────────────────────────┐  │
│  │  Borg (Resource Management)                   │  │
│  │  - Dynamic slot allocation                    │  │
│  │  - Automatic scaling                          │  │
│  └────────────────┬─────────────────────────────┘  │
│                   │                                  │
│  ┌────────────────▼─────────────────────────────┐  │
│  │  Colossus (Distributed Storage)               │  │
│  │  - Google's distributed file system            │  │
│  │  - Automatic replication, encryption           │  │
│  └─────────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────────┘
```

**Key Features:**

1. **Serverless**: No cluster management
   - Automatic resource allocation
   - Scales to 1000s of nodes automatically
   - No warm-up time

2. **Columnar Storage (Capacitor)**:
   - Optimized for analytical queries
   - Automatic encoding and compression
   - Nested/repeated columns for JSON/arrays

3. **Partitioning & Clustering**:
   - Table partitioning by date/timestamp (automatic cost savings)
   - Clustering by columns (data locality)

4. **Streaming Inserts**: Real-time data ingestion
   - Low-latency (< 1 second)
   - Useful for feature stores

5. **BI Engine**: In-memory caching
   - Sub-second query latency
   - Useful for dashboards

**ML-Specific Capabilities:**

- **BigQuery ML (BQML)**: Train models using SQL
  ```sql
  CREATE MODEL my_model
  OPTIONS(model_type='logistic_reg')
  AS SELECT features, label FROM training_data
  ```

- **ML.PREDICT**: Inference in SQL
  ```sql
  SELECT * FROM ML.PREDICT(MODEL my_model, TABLE new_data)
  ```

- **ML.FEATURE_IMPORTANCE**: Model explainability

- **Remote Models**: Call Vertex AI models from SQL
  ```sql
  CREATE MODEL vertex_llm
  REMOTE WITH CONNECTION `project.connection`
  OPTIONS(remote_service_type='CLOUD_AI_LARGE_LANGUAGE_MODEL_V1')
  ```

- **Vector Search (NEW)**: Similarity search for embeddings
  ```sql
  SELECT * FROM embeddings
  ORDER BY COSINE_DISTANCE(embedding, query_embedding)
  LIMIT 10
  ```

**Cost Model:**

```
On-Demand:
- Analysis (queries): $6.25 per TB scanned
- Storage: $20 per TB/month (active), $10 per TB/month (long-term)
- Streaming inserts: $0.05 per 200 MB

Flat-Rate (Slots):
- $2,000/month per 100 slots
- Predictable costs for heavy users
```

**Example Cost:**
```python
# ML feature engineering
# Dataset: 100 TB
# Queries: 1,000/month scanning 1 TB each

query_cost = 1000 queries * 1 TB * $6.25 = $6,250
storage_cost = 100 TB * $20 = $2,000

total_monthly = $6,250 + $2,000 = $8,250

# Alternative: Flat-rate
# If scanning > 320 TB/month, flat-rate is cheaper
# 1000 TB scanned/month → Use flat-rate ($2,000-4,000/month)
```

### 4. AWS Redshift Architecture

**Massively Parallel Processing (MPP):**

```
┌─────────────────────────────────────────────────────┐
│                  REDSHIFT                            │
│                                                      │
│  ┌──────────────────────────────────────────────┐  │
│  │  Leader Node                                  │  │
│  │  - Query planning, result aggregation         │  │
│  │  - Client connections                         │  │
│  └────────────────┬─────────────────────────────┘  │
│                   │                                  │
│  ┌────────────────▼─────────────────────────────┐  │
│  │  Compute Nodes (EC2 instances)                │  │
│  │                                                │  │
│  │  ┌────────┐  ┌────────┐  ┌────────┐          │  │
│  │  │ Node 1 │  │ Node 2 │  │ Node N │ ...      │  │
│  │  │        │  │        │  │        │          │  │
│  │  │ Slices │  │ Slices │  │ Slices │          │  │
│  │  └────┬───┘  └────┬───┘  └────┬───┘          │  │
│  └───────┼───────────┼───────────┼───────────────┘  │
│          │           │           │                   │
│  ┌───────▼───────────▼───────────▼───────────────┐  │
│  │  Local Storage (NVMe SSD) + S3 (Managed)      │  │
│  │  - Columnar storage (zone maps)                │  │
│  │  - Distribution keys (EVEN, KEY, ALL)          │  │
│  │  - Sort keys (compound, interleaved)           │  │
│  └─────────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────────┘
```

**Key Features:**

1. **Node Types**:
   - **RA3**: Managed storage (S3), scale compute independently
   - **DC2**: Dense compute (NVMe SSD), fast but fixed storage
   - **DS2**: Dense storage (HDD), deprecated

2. **Redshift Spectrum**: Query S3 data directly
   - No loading required
   - Pay per byte scanned ($5/TB)
   - Combine with Redshift tables (federated queries)

3. **Concurrency Scaling**: Automatic burst capacity
   - Handle spike in concurrent queries
   - Free for 1 hour/day

4. **Materialized Views**: Precomputed aggregations
   - Automatic refresh
   - Incremental updates

5. **Federated Queries**: Join with external databases
   - PostgreSQL, MySQL, Aurora
   - Useful for operational data

**ML-Specific Capabilities:**

- **Redshift ML**: Create models using SQL (via SageMaker Autopilot)
  ```sql
  CREATE MODEL customer_churn_model
  FROM (SELECT * FROM customer_features)
  TARGET churn
  FUNCTION predict_churn
  IAM_ROLE 'arn:aws:iam::123:role/RedshiftML'
  SETTINGS (S3_BUCKET 'ml-bucket')
  ```

- **UDFs (Python, Lambda)**: Custom transformations

- **SageMaker Integration**: Export data to S3, trigger training

- **SUPER data type**: Semi-structured data (JSON, PartiQL queries)

**Cost Model:**

```
On-Demand:
- RA3.4xlarge: $3.26/hour (12 vCPU, 96 GB RAM, managed storage)
- RA3.16xlarge: $13.04/hour (48 vCPU, 384 GB RAM)
- Managed storage: $0.024/GB/month

Reserved (1-year):
- 40-50% discount

Spectrum:
- $5 per TB scanned
```

**Example Cost:**
```python
# ML training pipeline
# Cluster: 4 × RA3.4xlarge nodes (running 24/7)
# Storage: 50 TB

compute_cost = 4 nodes * $3.26/hour * 24 hours * 30 days = $9,403
storage_cost = 50 TB * 1000 GB * $0.024 = $1,200

total_monthly = $9,403 + $1,200 = $10,603

# With 1-year reserved instances (50% discount):
reserved_compute = $9,403 * 0.5 = $4,702
total_reserved = $4,702 + $1,200 = $5,902 (44% savings)
```

### 5. Comparison Matrix

| Feature | Snowflake | BigQuery | Redshift |
|---------|-----------|----------|----------|
| **Architecture** | Multi-cluster shared data | Serverless (Dremel) | MPP (cluster-based) |
| **Scaling** | Manual (resize warehouse) | Automatic | Manual (resize cluster) |
| **Pricing** | Per-second compute | Per-byte scanned | Per-hour compute |
| **Best For** | Diverse workloads | Ad-hoc analytics | Predictable workloads |
| **ML Integration** | Snowpark, External UDFs | BQML, Vertex AI | Redshift ML, SageMaker |
| **Query Latency** | 1-10s (cold), <1s (cached) | 1-5s (serverless) | <1s (warm cluster) |
| **Concurrency** | Excellent (multi-cluster) | Excellent (serverless) | Good (concurrency scaling) |
| **Data Sharing** | Native (Snowflake-to-Snowflake) | Authorized views | Manual (export to S3) |
| **Time Travel** | 1-90 days | 7 days | Snapshots (manual) |
| **Geospatial** | Yes (GEOGRAPHY) | Yes (GEOGRAPHY) | Yes (GEOMETRY) |
| **Semi-Structured** | VARIANT, OBJECT, ARRAY | JSON, ARRAY, STRUCT | SUPER |
| **Streaming** | Snowpipe (micro-batches) | Streaming inserts (real-time) | Kinesis integration |
| **Cost (10 TB, 1000 queries)** | ~$1,500-3,000 | ~$6,000-8,000 | ~$5,000-10,000 |

### 6. Choosing the Right Warehouse for ML

**Decision Framework:**

```
Query Pattern:
├─ Ad-hoc exploration (varying queries) → BigQuery (serverless)
├─ Predictable batch jobs (fixed schedule) → Redshift (reserved)
└─ Mixed (interactive + batch) → Snowflake (multi-cluster)

Data Volume:
├─ < 10 TB → Any (cost differences minimal)
├─ 10-100 TB → BigQuery or Snowflake (better scaling)
└─ > 100 TB → BigQuery (best for massive scans)

Concurrency:
├─ < 10 concurrent users → Any
├─ 10-100 users → Snowflake or BigQuery
└─ > 100 users → BigQuery (infinite scaling)

Cost Sensitivity:
├─ Variable workload (spiky) → Snowflake or BigQuery (pay per use)
├─ Steady workload (24/7) → Redshift (reserved instances)
└─ Unpredictable → BigQuery (no fixed costs)

Ecosystem:
├─ AWS-native → Redshift (tight SageMaker integration)
├─ GCP-native → BigQuery (Vertex AI, Dataflow)
└─ Multi-cloud or Azure → Snowflake (cloud-agnostic)

ML Maturity:
├─ Basic SQL-based ML → BigQuery ML (easiest)
├─ Python-based pipelines → Snowpark (Snowflake)
└─ Advanced ML platforms → Any (use external orchestration)
```

**Common ML Use Case Recommendations:**

1. **Feature Engineering for Training**:
   - **Best**: Snowflake (Snowpark for complex transformations)
   - **Alternative**: BigQuery (BQML for simple models)

2. **Real-Time Feature Serving**:
   - **Best**: BigQuery (streaming inserts, BI Engine caching)
   - **Alternative**: Redshift + Redis cache

3. **Batch Inference**:
   - **Best**: BigQuery (massive parallelism)
   - **Alternative**: Snowflake (external functions to SageMaker)

4. **Exploratory Analysis**:
   - **Best**: BigQuery (serverless, no cluster management)
   - **Alternative**: Snowflake (result caching)

5. **Data Sharing Across Teams**:
   - **Best**: Snowflake (native data sharing)
   - **Alternative**: BigQuery (authorized views)

---

## 💡 Practical Examples

### Example 1: Snowflake - Feature Engineering with Snowpark

**Scenario**: Compute user features from 1 billion event logs for churn prediction.

**Snowpark (Python) Implementation:**

```python
from snowflake.snowpark import Session
from snowflake.snowpark.functions import col, sum, avg, count, datediff, current_date

# Create Snowflake session
session = Session.builder.configs({
    "account": "abc12345.us-east-1",
    "user": "ml_engineer",
    "password": "...",
    "role": "ML_ROLE",
    "warehouse": "ML_WH",
    "database": "ML_DB",
    "schema": "FEATURES"
}).create()

# Read raw events
events = session.table("USER_EVENTS")

# Compute features using DataFrame API (lazy evaluation)
user_features = events.group_by("user_id").agg([
    count("event_id").alias("total_events"),
    count_if(col("event_type") == "purchase").alias("purchase_count"),
    avg("session_duration_sec").alias("avg_session_duration"),
    datediff("day", max("event_timestamp"), current_date()).alias("days_since_last_activity"),
    sum(when(col("event_type") == "purchase", col("amount")).otherwise(0)).alias("total_spent")
])

# Join with user demographics
users = session.table("USERS")
training_data = user_features.join(
    users,
    user_features["user_id"] == users["user_id"]
).select(
    user_features["user_id"],
    user_features["total_events"],
    user_features["purchase_count"],
    user_features["avg_session_duration"],
    user_features["days_since_last_activity"],
    user_features["total_spent"],
    users["age"],
    users["country"],
    users["signup_date"],
    users["churn_label"]
)

# Write to feature table
training_data.write.mode("overwrite").save_as_table("CHURN_FEATURES")

print("✅ Feature engineering complete")
print(f"Total users: {training_data.count()}")
```

**Create Virtual Warehouse:**

```sql
-- Create dedicated warehouse for ML workloads
CREATE WAREHOUSE ML_WH
WITH
  WAREHOUSE_SIZE = 'LARGE'  -- 8 credits/hour
  AUTO_SUSPEND = 300        -- Suspend after 5 min idle
  AUTO_RESUME = TRUE
  INITIALLY_SUSPENDED = TRUE
  COMMENT = 'Warehouse for ML feature engineering';

-- Grant access
GRANT USAGE ON WAREHOUSE ML_WH TO ROLE ML_ROLE;

-- Use warehouse
USE WAREHOUSE ML_WH;
```

**Time Travel for Experimentation:**

```sql
-- Create zero-copy clone for experimentation
CREATE TABLE CHURN_FEATURES_EXPERIMENT
CLONE CHURN_FEATURES;

-- Experiment with clone (no impact on production)
ALTER TABLE CHURN_FEATURES_EXPERIMENT
ADD COLUMN new_feature FLOAT;

-- If experiment fails, rollback using time travel
CREATE OR REPLACE TABLE CHURN_FEATURES_EXPERIMENT
CLONE CHURN_FEATURES
AT(TIMESTAMP => DATEADD(hour, -2, CURRENT_TIMESTAMP()));
```

### Example 2: BigQuery - SQL-Based ML Model Training

**Scenario**: Train logistic regression model for fraud detection using BigQuery ML.

**Data Preparation:**

```sql
-- Create feature table
CREATE OR REPLACE TABLE `ml_project.fraud_features` AS
SELECT
  transaction_id,
  user_id,
  merchant_id,
  amount,
  -- Time-based features
  EXTRACT(HOUR FROM transaction_timestamp) AS hour_of_day,
  EXTRACT(DAYOFWEEK FROM transaction_timestamp) AS day_of_week,
  -- Aggregated features (window functions)
  AVG(amount) OVER (
    PARTITION BY user_id
    ORDER BY transaction_timestamp
    ROWS BETWEEN 100 PRECEDING AND 1 PRECEDING
  ) AS avg_amount_last_100_tx,
  COUNT(*) OVER (
    PARTITION BY user_id
    ORDER BY transaction_timestamp
    RANGE BETWEEN INTERVAL 24 HOUR PRECEDING AND CURRENT ROW
  ) AS tx_count_last_24h,
  -- Label
  is_fraud
FROM `ml_project.transactions`
WHERE transaction_timestamp >= DATE_SUB(CURRENT_DATE(), INTERVAL 365 DAY);
```

**Train Model:**

```sql
-- Train logistic regression model
CREATE OR REPLACE MODEL `ml_project.fraud_detection_model`
OPTIONS(
  model_type='LOGISTIC_REG',
  input_label_cols=['is_fraud'],
  auto_class_weights=TRUE,  -- Handle class imbalance
  enable_global_explain=TRUE,
  max_iterations=50,
  l1_reg=0.1,
  l2_reg=0.1
) AS
SELECT
  amount,
  hour_of_day,
  day_of_week,
  avg_amount_last_100_tx,
  tx_count_last_24h,
  is_fraud
FROM `ml_project.fraud_features`
WHERE transaction_id IS NOT NULL;
```

**Evaluate Model:**

```sql
-- Evaluate model performance
SELECT
  *
FROM ML.EVALUATE(
  MODEL `ml_project.fraud_detection_model`,
  (
    SELECT
      amount,
      hour_of_day,
      day_of_week,
      avg_amount_last_100_tx,
      tx_count_last_24h,
      is_fraud
    FROM `ml_project.fraud_features`
    WHERE MOD(ABS(FARM_FINGERPRINT(transaction_id)), 10) = 0  -- 10% holdout
  )
);

-- Output:
-- precision: 0.87
-- recall: 0.82
-- accuracy: 0.95
-- f1_score: 0.84
-- roc_auc: 0.93
```

**Make Predictions:**

```sql
-- Batch inference on new transactions
CREATE OR REPLACE TABLE `ml_project.fraud_predictions` AS
SELECT
  transaction_id,
  predicted_is_fraud,
  predicted_is_fraud_probs[OFFSET(1)].prob AS fraud_probability
FROM ML.PREDICT(
  MODEL `ml_project.fraud_detection_model`,
  (
    SELECT
      transaction_id,
      amount,
      hour_of_day,
      day_of_week,
      avg_amount_last_100_tx,
      tx_count_last_24h
    FROM `ml_project.fraud_features`
    WHERE transaction_timestamp >= CURRENT_DATE()
  )
)
WHERE predicted_is_fraud = 1 AND fraud_probability > 0.8;
```

**Feature Importance:**

```sql
-- Analyze feature importance
SELECT
  *
FROM ML.FEATURE_IMPORTANCE(MODEL `ml_project.fraud_detection_model`)
ORDER BY importance_weight DESC;

-- Output:
-- feature                    | importance_weight
-- ---------------------------|------------------
-- avg_amount_last_100_tx     | 0.35
-- tx_count_last_24h          | 0.28
-- amount                     | 0.21
-- hour_of_day                | 0.10
-- day_of_week                | 0.06
```

### Example 3: Redshift - Integration with SageMaker

**Scenario**: Export training data from Redshift to S3, trigger SageMaker training.

**Data Preparation in Redshift:**

```sql
-- Create feature table
CREATE TABLE ml_features.customer_churn AS
SELECT
  c.customer_id,
  c.age,
  c.tenure_months,
  c.monthly_spend,
  COUNT(DISTINCT o.order_id) AS order_count,
  SUM(o.total_amount) AS total_revenue,
  AVG(o.total_amount) AS avg_order_value,
  DATEDIFF(day, MAX(o.order_date), CURRENT_DATE) AS days_since_last_order,
  c.churn_flag
FROM customers c
LEFT JOIN orders o ON c.customer_id = o.customer_id
WHERE o.order_date >= DATEADD(year, -1, CURRENT_DATE())
GROUP BY c.customer_id, c.age, c.tenure_months, c.monthly_spend, c.churn_flag;

-- Export to S3 (for SageMaker training)
UNLOAD ('SELECT * FROM ml_features.customer_churn')
TO 's3://ml-training-data/customer_churn/train_'
IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3Access'
FORMAT AS PARQUET
PARTITION BY (churn_flag)
MAXFILESIZE 1 GB;
```

**Redshift ML (Alternative - No Export Needed):**

```sql
-- Train model directly in Redshift (uses SageMaker Autopilot behind the scenes)
CREATE MODEL ml_features.churn_prediction_model
FROM (
  SELECT
    age,
    tenure_months,
    monthly_spend,
    order_count,
    total_revenue,
    avg_order_value,
    days_since_last_order,
    churn_flag
  FROM ml_features.customer_churn
)
TARGET churn_flag
FUNCTION predict_churn
IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftMLRole'
SETTINGS (
  S3_BUCKET 'ml-autopilot-bucket',
  MAX_RUNTIME 5400  -- 90 minutes
);

-- Check model status
SHOW MODEL ml_features.churn_prediction_model;
```

**Make Predictions:**

```sql
-- Batch inference
SELECT
  customer_id,
  predict_churn(
    age,
    tenure_months,
    monthly_spend,
    order_count,
    total_revenue,
    avg_order_value,
    days_since_last_order
  ) AS predicted_churn
FROM ml_features.customer_churn
WHERE churn_flag IS NULL;  -- Customers without known label
```

### Example 4: Cost Optimization - Query Optimization

**Scenario**: Optimize expensive feature engineering query across all three warehouses.

**Unoptimized Query (Scans Entire Table):**

```sql
-- BAD: Full table scan
SELECT
  user_id,
  COUNT(*) AS event_count,
  AVG(session_duration) AS avg_duration
FROM events
WHERE event_date >= '2025-01-01'
GROUP BY user_id;

-- Cost (100 TB table):
-- Snowflake: ~200 credits ($600)
-- BigQuery: 100 TB × $6.25 = $6,250
-- Redshift: 4-hour query on 10-node cluster = $130
```

**Optimized Query (Partition Pruning):**

**Snowflake:**
```sql
-- Create clustering key
ALTER TABLE events CLUSTER BY (event_date, user_id);

-- Query now benefits from automatic pruning
SELECT
  user_id,
  COUNT(*) AS event_count,
  AVG(session_duration) AS avg_duration
FROM events
WHERE event_date >= '2025-01-01'
GROUP BY user_id;

-- Snowflake automatically prunes micro-partitions
-- Cost: ~10 credits ($30) - 95% reduction
```

**BigQuery:**
```sql
-- Create partitioned table
CREATE OR REPLACE TABLE events_partitioned
PARTITION BY DATE(event_timestamp)
CLUSTER BY user_id
AS SELECT * FROM events;

-- Query with partition filter
SELECT
  user_id,
  COUNT(*) AS event_count,
  AVG(session_duration) AS avg_duration
FROM events_partitioned
WHERE DATE(event_timestamp) >= '2025-01-01'
GROUP BY user_id;

-- BigQuery scans only relevant partitions
-- Cost: 1 TB × $6.25 = $6.25 - 99% reduction
```

**Redshift:**
```sql
-- Create table with sort key
CREATE TABLE events_partitioned
SORTKEY (event_date, user_id)
AS SELECT * FROM events;

-- Analyze table (update statistics)
ANALYZE events_partitioned;

-- Query with sort key predicate
SELECT
  user_id,
  COUNT(*) AS event_count,
  AVG(session_duration) AS avg_duration
FROM events_partitioned
WHERE event_date >= '2025-01-01'
GROUP BY user_id;

-- Redshift uses zone maps to skip blocks
-- Query time: 4 hours → 10 minutes
-- Cost: $130 → $3.25 - 97% reduction
```

### Example 5: Data Sharing

**Snowflake (Native Data Sharing):**

```sql
-- Provider account creates share
CREATE SHARE ml_training_data_share;

-- Grant access to objects
GRANT USAGE ON DATABASE ML_DB TO SHARE ml_training_data_share;
GRANT USAGE ON SCHEMA ML_DB.FEATURES TO SHARE ml_training_data_share;
GRANT SELECT ON TABLE ML_DB.FEATURES.USER_FEATURES TO SHARE ml_training_data_share;

-- Add consumer account
ALTER SHARE ml_training_data_share
ADD ACCOUNTS = xyz12345;

-- Consumer account imports share
CREATE DATABASE ML_TRAINING_DATA
FROM SHARE abc12345.ml_training_data_share;

-- Query shared data (no data copy!)
SELECT * FROM ML_TRAINING_DATA.FEATURES.USER_FEATURES LIMIT 10;
```

**BigQuery (Authorized Views):**

```sql
-- Create view that restricts access
CREATE VIEW `ml_project.shared_features` AS
SELECT
  user_id,
  feature_1,
  feature_2,
  feature_3
FROM `ml_project.raw_features`
WHERE country = SESSION_USER();  -- Row-level security

-- Grant access to specific users
GRANT `roles/bigquery.dataViewer`
ON TABLE `ml_project.shared_features`
TO "user:data-scientist@company.com";
```

**Redshift (Manual Export):**

```sql
-- Export data to S3 for sharing
UNLOAD ('SELECT * FROM ml_features.user_features')
TO 's3://shared-data/ml_features/'
IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftS3Access'
FORMAT AS PARQUET
ENCRYPTED;

-- Other account imports from S3
COPY ml_features.imported_user_features
FROM 's3://shared-data/ml_features/'
IAM_ROLE 'arn:aws:iam::987654321098:role/RedshiftS3Access'
FORMAT AS PARQUET;
```

---

## 🔧 Code Examples

### Code Example 1: Warehouse Performance Benchmarking

```python
import time
import snowflake.connector
from google.cloud import bigquery
import psycopg2  # Redshift
import pandas as pd

class WarehouseBenchmark:
    """Benchmark query performance across warehouses"""

    def __init__(self):
        # Snowflake connection
        self.snowflake_conn = snowflake.connector.connect(
            user='ml_engineer',
            password='...',
            account='abc12345.us-east-1',
            warehouse='ML_WH',
            database='ML_DB',
            schema='FEATURES'
        )

        # BigQuery client
        self.bq_client = bigquery.Client(project='ml-project')

        # Redshift connection
        self.redshift_conn = psycopg2.connect(
            host='ml-cluster.abc.us-east-1.redshift.amazonaws.com',
            port=5439,
            database='mldb',
            user='ml_engineer',
            password='...'
        )

    def benchmark_query(self, query, warehouse='snowflake', iterations=3):
        """Run query and measure performance"""
        times = []

        for i in range(iterations):
            start = time.time()

            if warehouse == 'snowflake':
                cursor = self.snowflake_conn.cursor()
                cursor.execute(query)
                result = cursor.fetchall()
                cursor.close()

            elif warehouse == 'bigquery':
                query_job = self.bq_client.query(query)
                result = query_job.result()

            elif warehouse == 'redshift':
                cursor = self.redshift_conn.cursor()
                cursor.execute(query)
                result = cursor.fetchall()
                cursor.close()

            elapsed = time.time() - start
            times.append(elapsed)

            print(f"  Run {i+1}: {elapsed:.2f}s")

        avg_time = sum(times) / len(times)
        print(f"Average time: {avg_time:.2f}s")

        return {
            'warehouse': warehouse,
            'avg_time': avg_time,
            'min_time': min(times),
            'max_time': max(times)
        }

    def benchmark_feature_engineering(self):
        """Benchmark typical feature engineering query"""
        query = """
        SELECT
          user_id,
          COUNT(*) AS event_count,
          AVG(session_duration) AS avg_duration,
          SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) AS purchase_count
        FROM events
        WHERE event_date >= DATEADD(day, -30, CURRENT_DATE)
        GROUP BY user_id
        """

        results = []

        print("Benchmarking Snowflake...")
        results.append(self.benchmark_query(query, 'snowflake'))

        print("\nBenchmarking BigQuery...")
        bq_query = query.replace('DATEADD(day, -30, CURRENT_DATE)', 'DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)')
        results.append(self.benchmark_query(bq_query, 'bigquery'))

        print("\nBenchmarking Redshift...")
        results.append(self.benchmark_query(query, 'redshift'))

        # Display results
        df = pd.DataFrame(results)
        print("\n📊 Benchmark Results:")
        print(df.to_string(index=False))

        return df

# Usage
benchmark = WarehouseBenchmark()
results = benchmark.benchmark_feature_engineering()
```

### Code Example 2: Cost Estimator

```python
class WarehouseCostEstimator:
    """Estimate monthly costs for different warehouses"""

    def __init__(self, data_size_tb, queries_per_month, avg_query_scan_tb):
        self.data_size_tb = data_size_tb
        self.queries_per_month = queries_per_month
        self.avg_query_scan_tb = avg_query_scan_tb

    def snowflake_cost(self, warehouse_size='LARGE', avg_query_minutes=2):
        """Estimate Snowflake costs"""
        # Warehouse credit consumption
        credits_per_hour = {
            'X-SMALL': 1,
            'SMALL': 2,
            'MEDIUM': 4,
            'LARGE': 8,
            'X-LARGE': 16,
            '2X-LARGE': 32,
            '3X-LARGE': 64,
            '4X-LARGE': 128
        }

        credit_rate = 3.00  # $3 per credit

        # Compute cost
        credits_per_query = (avg_query_minutes / 60) * credits_per_hour[warehouse_size]
        monthly_compute_credits = credits_per_query * self.queries_per_month
        compute_cost = monthly_compute_credits * credit_rate

        # Storage cost
        storage_cost = self.data_size_tb * 23  # $23/TB/month

        # Cloud services (typically 10% of compute, but usually included)
        cloud_services_cost = 0  # Free tier

        total_cost = compute_cost + storage_cost + cloud_services_cost

        return {
            'warehouse': 'Snowflake',
            'compute_cost': compute_cost,
            'storage_cost': storage_cost,
            'total_cost': total_cost,
            'details': f'{monthly_compute_credits:.0f} credits @ ${credit_rate}/credit'
        }

    def bigquery_cost(self, pricing_model='on-demand'):
        """Estimate BigQuery costs"""
        if pricing_model == 'on-demand':
            # Per-query cost
            scan_cost_per_tb = 6.25
            query_cost = self.queries_per_month * self.avg_query_scan_tb * scan_cost_per_tb

            # Storage cost
            storage_cost = self.data_size_tb * 20  # $20/TB/month (active)

            total_cost = query_cost + storage_cost

            return {
                'warehouse': 'BigQuery (On-Demand)',
                'compute_cost': query_cost,
                'storage_cost': storage_cost,
                'total_cost': total_cost,
                'details': f'{self.queries_per_month * self.avg_query_scan_tb:.0f} TB scanned @ $6.25/TB'
            }

        else:  # flat-rate
            slots_needed = 100  # Baseline
            flat_rate_cost = (slots_needed / 100) * 2000  # $2000 per 100 slots

            storage_cost = self.data_size_tb * 20

            total_cost = flat_rate_cost + storage_cost

            return {
                'warehouse': 'BigQuery (Flat-Rate)',
                'compute_cost': flat_rate_cost,
                'storage_cost': storage_cost,
                'total_cost': total_cost,
                'details': f'{slots_needed} slots @ $2000 per 100 slots'
            }

    def redshift_cost(self, node_type='RA3.4XLARGE', num_nodes=4, reserved=False):
        """Estimate Redshift costs"""
        # Node pricing (on-demand, per hour)
        node_prices = {
            'RA3.4XLARGE': 3.26,
            'RA3.16XLARGE': 13.04
        }

        hourly_rate = node_prices[node_type] * num_nodes

        # Discount for reserved instances
        if reserved:
            hourly_rate *= 0.5  # 50% discount

        # Compute cost (assuming 24/7)
        compute_cost = hourly_rate * 24 * 30

        # Storage cost (managed storage for RA3)
        storage_cost = self.data_size_tb * 1000 * 0.024  # $0.024/GB/month

        total_cost = compute_cost + storage_cost

        return {
            'warehouse': f'Redshift ({node_type}, {"Reserved" if reserved else "On-Demand"})',
            'compute_cost': compute_cost,
            'storage_cost': storage_cost,
            'total_cost': total_cost,
            'details': f'{num_nodes} × {node_type} @ ${hourly_rate:.2f}/hour'
        }

    def compare_all(self):
        """Compare costs across all warehouses"""
        results = []

        # Snowflake
        results.append(self.snowflake_cost('LARGE'))
        results.append(self.snowflake_cost('X-LARGE'))

        # BigQuery
        results.append(self.bigquery_cost('on-demand'))
        results.append(self.bigquery_cost('flat-rate'))

        # Redshift
        results.append(self.redshift_cost('RA3.4XLARGE', 4, reserved=False))
        results.append(self.redshift_cost('RA3.4XLARGE', 4, reserved=True))

        df = pd.DataFrame(results)

        print("💰 Cost Comparison:")
        print(df.to_string(index=False))

        # Find cheapest option
        cheapest = df.loc[df['total_cost'].idxmin()]
        print(f"\n✅ Cheapest option: {cheapest['warehouse']} (${cheapest['total_cost']:,.2f}/month)")

        return df

# Usage
estimator = WarehouseCostEstimator(
    data_size_tb=100,
    queries_per_month=1000,
    avg_query_scan_tb=1
)

cost_comparison = estimator.compare_all()
```

---

## ✅ Best Practices

### 1. Warehouse Selection

**DO:**
- ✅ Choose Snowflake for mixed workloads (interactive + batch)
- ✅ Choose BigQuery for serverless, ad-hoc analytics
- ✅ Choose Redshift for AWS-native, predictable workloads
- ✅ Consider cost model (per-second vs per-byte vs per-hour)
- ✅ Test with real workload before committing

**DON'T:**
- ❌ Choose based on hype alone
- ❌ Ignore ecosystem integration (AWS, GCP, Azure)
- ❌ Forget about data egress costs
- ❌ Assume one size fits all

### 2. Performance Optimization

**DO:**
- ✅ Partition/cluster tables by frequently filtered columns
- ✅ Use materialized views for repeated aggregations
- ✅ Enable result caching
- ✅ Monitor query performance with EXPLAIN
- ✅ Use appropriate warehouse/cluster size

**DON'T:**
- ❌ Run full table scans on 100+ TB tables
- ❌ Over-provision compute (wastes money)
- ❌ Under-provision compute (slow queries)
- ❌ Ignore query optimization (partition pruning)

### 3. Cost Management

**DO:**
- ✅ Set up auto-suspend for Snowflake warehouses
- ✅ Use reserved instances for Redshift (steady workloads)
- ✅ Monitor costs with dashboards and alerts
- ✅ Tag resources for cost attribution
- ✅ Compress data (saves storage + query costs)

**DON'T:**
- ❌ Leave clusters running 24/7 unnecessarily
- ❌ Ignore BigQuery query costs (can spiral quickly)
- ❌ Forget about data storage costs
- ❌ Skip cost optimization (50-90% savings possible)

### 4. ML Integration

**DO:**
- ✅ Use native ML features (BQML, Redshift ML, Snowpark)
- ✅ Export to S3/GCS for external ML platforms
- ✅ Leverage UDFs for custom transformations
- ✅ Integrate with SageMaker/Vertex AI/Databricks
- ✅ Use external functions for model inference

**DON'T:**
- ❌ Move massive datasets unnecessarily
- ❌ Ignore warehouse ML capabilities
- ❌ Train in warehouse when external platform is better
- ❌ Forget about data movement costs

---

## 📝 Key Takeaways

1. **Modern cloud warehouses decouple compute and storage**: Scale independently, pay per use, no cluster management (except Redshift).

2. **Architecture differences matter**: Snowflake (multi-cluster), BigQuery (serverless), Redshift (MPP cluster). Choose based on workload.

3. **Cost models vary significantly**: Snowflake (per-second), BigQuery (per-byte scanned), Redshift (per-hour). Optimize accordingly.

4. **All three support ML**: BQML (easiest SQL-based), Snowpark (Python pipelines), Redshift ML (SageMaker integration).

5. **Query optimization saves 95%+ costs**: Partition/cluster tables, use materialized views, enable caching.

6. **Snowflake excels at data sharing**: Native sharing across accounts (no data copy). BigQuery/Redshift require manual exports.

7. **BigQuery is cheapest for ad-hoc queries**: Serverless, no idle costs. But per-byte pricing can be expensive for heavy scans.

8. **Redshift is cheapest for 24/7 workloads**: Reserved instances offer 50% discount. Best for predictable batch jobs.

9. **Snowflake balances flexibility and cost**: Multi-cluster, auto-suspend, result caching. Best for diverse teams.

10. **Always benchmark before committing**: Test real workloads, measure performance, estimate costs. Don't rely on vendor claims.

---

## ✏️ Notes Section

**Personal Insights:**

---

**Questions:**

---

**Action Items:**

---

**Related Projects:**

---

**Code Snippets:**

---

**Further Exploration:**

---

**📌 Tags:** `#data-warehousing` `#snowflake` `#bigquery` `#redshift` `#cloud-computing` `#sql` `#ml-platforms` `#cost-optimization`

---

**Navigation:**
- → Next: [05.02 - Dimensional Modeling for ML](02.%20Dimensional%20Modeling%20for%20ML.md)
- ↑ Up: [Chapter 05 - Data Warehousing for Analytics & ML](README.md)
- ⌂ Home: [DEforAI Course Index](../../README.md)

---

*Last updated: 2025-10-20*
