# Dimensional Modeling for ML

**Course:** Data Engineering for AI/ML
**Chapter:** 05 - Data Warehousing for Analytics & ML
**Subchapter:** 02 - Dimensional Modeling for ML
**Created:** 2025-10-20
**Updated:** 2025-10-20

---

## 📋 Overview

Dimensional modeling, pioneered by Ralph Kimball, has been the gold standard for analytical data warehouses for decades. The classic **star schema** (fact table surrounded by dimension tables) optimizes for business intelligence queries: aggregations, drill-downs, and slice-and-dice analysis.

However, ML workloads have different requirements than traditional BI:

**Traditional BI Focus:**
- Aggregated metrics (SUM, AVG, COUNT)
- Historical trends and comparisons
- Drill-down analysis (region → country → city)
- Human-readable reports and dashboards

**ML Focus:**
- **Row-level features** (not just aggregates)
- **Temporal consistency** (point-in-time correctness)
- **Feature richness** (100s-1000s of features per entity)
- **Training/serving skew prevention** (same logic in both)
- **Reproducibility** (features must be reconstructable)

This subchapter shows how to **adapt dimensional modeling for ML**, including:
- Designing star schemas that support both BI and ML
- Using Slowly Changing Dimensions (SCD) for temporal features
- Creating feature-friendly fact tables
- Modeling many-to-many relationships for ML
- Handling time-series data and event streams
- Denormalization strategies for feature serving

The goal: A unified dimensional model that serves both business analytics AND ML feature engineering, avoiding duplicate pipelines and data inconsistencies.

**Key Topics:**
- Star schema fundamentals (fact + dimension tables)
- Slowly Changing Dimensions (Type 1, 2, 3) for ML
- Fact table design for training data
- Conformed dimensions across models
- Bridge tables for many-to-many relationships
- Time-series and event modeling
- Feature derivation patterns
- Denormalized wide tables for ML
- Point-in-time correctness

**Prerequisites:**
- Understanding of relational databases and SQL
- Knowledge of data warehouses (Subchapter 05.01)
- Familiarity with ML training pipelines (Chapter 02)
- Basic understanding of normalization and denormalization

---

## 🎯 Learning Objectives

By the end of this subchapter, you will be able to:

1. **Design star schemas** that support both BI reporting and ML feature engineering
2. **Implement SCDs** (Type 1, 2, 3) to track feature history and enable temporal features
3. **Create fact tables** optimized for ML training data preparation
4. **Model many-to-many relationships** using bridge tables (e.g., users ↔ products)
5. **Ensure point-in-time correctness** to prevent data leakage in training
6. **Derive analytical features** from dimensional models (aggregations, window functions)
7. **Denormalize for performance** while maintaining reproducibility
8. **Handle late-arriving data** without breaking feature consistency
9. **Version dimensional models** for ML experiment tracking
10. **Optimize dimensional models** for large-scale ML workloads (partitioning, indexing)

---

## 📚 Core Concepts

### 1. Traditional Star Schema

**Structure:**

```
┌─────────────────────────────────────────────────────┐
│                  STAR SCHEMA                         │
│                                                      │
│  ┌──────────────┐                                   │
│  │ DIM_DATE     │                                   │
│  │              │                                   │
│  │ date_key (PK)│                                   │
│  │ date         │                                   │
│  │ year         │◄──────┐                          │
│  │ quarter      │       │                          │
│  │ month        │       │                          │
│  │ day_of_week  │       │                          │
│  └──────────────┘       │                          │
│                          │                          │
│  ┌──────────────┐       │     ┌───────────────┐   │
│  │ DIM_CUSTOMER │       │     │  FACT_ORDERS  │   │
│  │              │       │     │               │   │
│  │ customer_key │◄──────┼─────┤ date_key (FK) │   │
│  │ customer_id  │       │     │ customer_key  │   │
│  │ name         │       └─────┤ product_key   │   │
│  │ age          │             │ store_key     │   │
│  │ country      │       ┌─────┤               │   │
│  │ segment      │       │     │ quantity      │   │
│  └──────────────┘       │     │ amount        │   │
│                          │     │ discount      │   │
│  ┌──────────────┐       │     └───────────────┘   │
│  │ DIM_PRODUCT  │       │                          │
│  │              │       │                          │
│  │ product_key  │◄──────┘                          │
│  │ product_id   │                                  │
│  │ name         │                                  │
│  │ category     │                                  │
│  │ brand        │                                  │
│  └──────────────┘                                  │
└──────────────────────────────────────────────────────┘
```

**Characteristics:**
- **Fact table** (center): Measures/metrics (quantity, amount, discount)
- **Dimension tables** (surrounding): Descriptive attributes (customer name, product category)
- **Foreign keys** in fact table reference dimension primary keys
- **Denormalized dimensions**: All attributes in single table (no subdimensions)

**Typical BI Query:**
```sql
SELECT
  d.year,
  d.quarter,
  c.country,
  SUM(f.amount) AS total_revenue
FROM fact_orders f
JOIN dim_date d ON f.date_key = d.date_key
JOIN dim_customer c ON f.customer_key = c.customer_key
WHERE d.year = 2025
GROUP BY d.year, d.quarter, c.country;
```

**Why It Works for BI:**
- Fast aggregations (star join optimization)
- Easy to understand (business logic in dimensions)
- Flexible slicing/dicing (drill-down, roll-up)

**Challenges for ML:**
- Aggregations lose row-level detail
- No temporal history (current state only)
- Missing many-to-many relationships
- Point-in-time correctness not guaranteed

### 2. Slowly Changing Dimensions (SCD) for ML

**Problem**: Customer attributes change over time. How do we track history?

**Example**: Customer "John" moves from "New York" to "California"

**Type 1: Overwrite (No History)**

```sql
-- DIM_CUSTOMER (Type 1)
customer_key | customer_id | name | state
-------------|-------------|------|-------
1001         | C123        | John | California  -- Overwrites "New York"
```

**Pros**: Simple, always current
**Cons**: No history, breaks reproducibility for ML

**Use Case**: Corrections (typos), non-essential attributes

---

**Type 2: Add New Row (Full History)**

```sql
-- DIM_CUSTOMER (Type 2)
customer_key | customer_id | name | state      | effective_date | expiration_date | is_current
-------------|-------------|------|------------|----------------|-----------------|------------
1001         | C123        | John | New York   | 2024-01-01     | 2025-05-15      | FALSE
1002         | C123        | John | California | 2025-05-16     | 9999-12-31      | TRUE
```

**Pros**: Complete history, point-in-time correctness
**Cons**: Larger dimension, more complex queries

**Use Case**: ML features (customer state at training time)

**Point-in-Time Query:**
```sql
-- Get customer state as of 2025-03-01 (before move)
SELECT *
FROM dim_customer
WHERE customer_id = 'C123'
  AND effective_date <= '2025-03-01'
  AND expiration_date > '2025-03-01';

-- Result: state = 'New York'
```

---

**Type 3: Add New Column (Limited History)**

```sql
-- DIM_CUSTOMER (Type 3)
customer_key | customer_id | name | current_state | previous_state
-------------|-------------|------|---------------|----------------
1001         | C123        | John | California    | New York
```

**Pros**: Easy to compare current vs previous
**Cons**: Only tracks one change

**Use Case**: ML features (before/after comparisons)

---

**SCD Type 2 for ML - Extended Pattern:**

```sql
CREATE TABLE dim_customer (
  customer_key BIGINT PRIMARY KEY,        -- Surrogate key
  customer_id VARCHAR(50),                 -- Natural key

  -- Attributes
  name VARCHAR(200),
  email VARCHAR(200),
  age INT,
  country VARCHAR(100),
  state VARCHAR(100),
  city VARCHAR(100),
  customer_segment VARCHAR(50),

  -- SCD Type 2 metadata
  effective_date DATE,                     -- Start date
  expiration_date DATE,                    -- End date (9999-12-31 for current)
  is_current BOOLEAN,                      -- Flag for current record

  -- Audit columns
  inserted_at TIMESTAMP,
  updated_at TIMESTAMP,
  source_system VARCHAR(100)
);

-- Index for point-in-time queries
CREATE INDEX idx_customer_pit ON dim_customer(customer_id, effective_date, expiration_date);
```

**Updating SCD Type 2:**

```sql
-- When customer moves from NY to CA on 2025-05-16

-- Step 1: Expire old record
UPDATE dim_customer
SET
  expiration_date = '2025-05-15',
  is_current = FALSE,
  updated_at = CURRENT_TIMESTAMP
WHERE customer_id = 'C123'
  AND is_current = TRUE;

-- Step 2: Insert new record
INSERT INTO dim_customer (
  customer_key, customer_id, name, state,
  effective_date, expiration_date, is_current,
  inserted_at
)
VALUES (
  1002, 'C123', 'John', 'California',
  '2025-05-16', '9999-12-31', TRUE,
  CURRENT_TIMESTAMP
);
```

### 3. Fact Table Design for ML

**Traditional Fact Table (Aggregated):**

```sql
CREATE TABLE fact_daily_sales (
  date_key INT,
  store_key INT,
  product_key INT,

  -- Aggregated measures
  total_quantity INT,
  total_amount DECIMAL(15,2),
  transaction_count INT,
  avg_discount DECIMAL(5,2),

  PRIMARY KEY (date_key, store_key, product_key)
);
```

**Problem for ML**: Lost row-level detail (individual transactions)

**ML-Friendly Fact Table (Transactional/Atomic):**

```sql
CREATE TABLE fact_transactions (
  transaction_id BIGINT PRIMARY KEY,       -- Grain: Individual transaction

  -- Foreign keys to dimensions
  date_key INT REFERENCES dim_date(date_key),
  customer_key INT REFERENCES dim_customer(customer_key),
  product_key INT REFERENCES dim_product(product_key),
  store_key INT REFERENCES dim_store(store_key),

  -- Transaction attributes
  timestamp TIMESTAMP,
  quantity INT,
  unit_price DECIMAL(10,2),
  discount_pct DECIMAL(5,2),
  total_amount DECIMAL(10,2),
  payment_method VARCHAR(50),

  -- Degenerate dimensions (transaction-specific, no separate dimension)
  order_id VARCHAR(100),
  session_id VARCHAR(100),

  -- ML-specific metadata
  is_fraud BOOLEAN,                        -- Label for fraud detection
  is_returned BOOLEAN,                     -- Label for return prediction
  customer_lifetime_value DECIMAL(15,2)   -- Pre-computed feature
);

-- Partition by date for query performance
-- PARTITION BY RANGE (date_key);
```

**Benefits for ML:**
- Row-level features (quantity, discount for each transaction)
- Join with dimensions to get customer/product features
- Temporal ordering (timestamp) for sequence models
- Labels (is_fraud, is_returned) for supervised learning

**Feature Derivation Example:**

```sql
-- Create training dataset
SELECT
  -- Customer features (from dimension)
  c.customer_id,
  c.age,
  c.state,
  c.customer_segment,

  -- Product features (from dimension)
  p.category,
  p.brand,
  p.price_tier,

  -- Transaction features (from fact)
  f.quantity,
  f.unit_price,
  f.discount_pct,
  f.total_amount,

  -- Temporal features
  d.day_of_week,
  d.is_holiday,
  EXTRACT(HOUR FROM f.timestamp) AS hour_of_day,

  -- Aggregated features (window functions)
  COUNT(*) OVER (
    PARTITION BY f.customer_key
    ORDER BY f.timestamp
    ROWS BETWEEN 30 PRECEDING AND 1 PRECEDING
  ) AS transactions_last_30,

  AVG(f.total_amount) OVER (
    PARTITION BY f.customer_key
    ORDER BY f.timestamp
    ROWS BETWEEN 100 PRECEDING AND 1 PRECEDING
  ) AS avg_amount_last_100,

  -- Label
  f.is_fraud

FROM fact_transactions f
JOIN dim_customer c ON f.customer_key = c.customer_key
  AND f.timestamp BETWEEN c.effective_date AND c.expiration_date  -- Point-in-time join
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_date d ON f.date_key = d.date_key
WHERE f.timestamp >= '2024-01-01'
  AND f.timestamp < '2025-01-01';
```

### 4. Bridge Tables for Many-to-Many Relationships

**Problem**: Users can have multiple interests, products can belong to multiple categories

**Traditional (One-to-Many)**:
```
USER → ORDERS (user_id foreign key)
```

**Many-to-Many (Bridge Table)**:

```
┌─────────────────────────────────────────────────┐
│           MANY-TO-MANY WITH BRIDGE               │
│                                                  │
│  ┌──────────────┐        ┌──────────────────┐  │
│  │ DIM_USER     │        │ BRIDGE_USER_     │  │
│  │              │        │ INTEREST         │  │
│  │ user_key (PK)│◄───────┤ user_key (FK)    │  │
│  │ user_id      │        │ interest_key (FK)│  │
│  │ name         │        │ weight           │  │
│  └──────────────┘        │ created_date     │  │
│                          └────────┬─────────┘  │
│                                   │             │
│  ┌──────────────┐                 │             │
│  │ DIM_INTEREST │◄────────────────┘             │
│  │              │                                │
│  │ interest_key │                                │
│  │ interest_name│                                │
│  │ category     │                                │
│  └──────────────┘                                │
└──────────────────────────────────────────────────┘
```

**Example:**

```sql
-- User dimension
CREATE TABLE dim_user (
  user_key BIGINT PRIMARY KEY,
  user_id VARCHAR(100),
  name VARCHAR(200),
  signup_date DATE
);

-- Interest dimension
CREATE TABLE dim_interest (
  interest_key BIGINT PRIMARY KEY,
  interest_name VARCHAR(100),
  category VARCHAR(50)
);

-- Bridge table (many-to-many)
CREATE TABLE bridge_user_interest (
  user_key BIGINT REFERENCES dim_user(user_key),
  interest_key BIGINT REFERENCES dim_interest(interest_key),
  weight DECIMAL(5,2),            -- Interest strength (0.0 - 1.0)
  created_date DATE,
  PRIMARY KEY (user_key, interest_key)
);
```

**Feature Derivation from Bridge:**

```sql
-- Create user features with interest counts
SELECT
  u.user_id,
  u.name,
  COUNT(DISTINCT b.interest_key) AS interest_count,
  COUNT(DISTINCT CASE WHEN i.category = 'Sports' THEN b.interest_key END) AS sports_interests,
  COUNT(DISTINCT CASE WHEN i.category = 'Tech' THEN b.interest_key END) AS tech_interests,
  AVG(b.weight) AS avg_interest_weight
FROM dim_user u
LEFT JOIN bridge_user_interest b ON u.user_key = b.user_key
LEFT JOIN dim_interest i ON b.interest_key = i.interest_key
GROUP BY u.user_id, u.name;
```

**ML Use Case - Collaborative Filtering:**

```sql
-- Create user-item matrix for recommendation
SELECT
  u.user_id,
  i.interest_name,
  b.weight AS affinity
FROM bridge_user_interest b
JOIN dim_user u ON b.user_key = u.user_key
JOIN dim_interest i ON b.interest_key = i.interest_key
ORDER BY u.user_id, b.weight DESC;
```

### 5. Denormalized Wide Tables for ML

**Problem**: Joining 10+ tables for each training example is slow

**Solution**: Create denormalized "feature store" tables

**Denormalization Strategy:**

```sql
-- Denormalized training table (pre-joined)
CREATE TABLE ml_features_customer_transactions AS
SELECT
  -- IDs
  f.transaction_id,
  f.customer_key,
  f.product_key,

  -- Customer features (denormalized from dim_customer)
  c.customer_id,
  c.age,
  c.state,
  c.country,
  c.customer_segment,

  -- Product features (denormalized from dim_product)
  p.product_id,
  p.product_name,
  p.category,
  p.brand,
  p.price_tier,

  -- Store features (denormalized from dim_store)
  s.store_id,
  s.store_region,
  s.store_size,

  -- Date features (denormalized from dim_date)
  d.date,
  d.year,
  d.quarter,
  d.month,
  d.day_of_week,
  d.is_holiday,

  -- Transaction features
  f.timestamp,
  f.quantity,
  f.unit_price,
  f.discount_pct,
  f.total_amount,
  f.payment_method,

  -- Aggregated features (pre-computed)
  cs.total_transactions AS customer_total_tx,
  cs.total_spent AS customer_total_spent,
  cs.avg_order_value AS customer_avg_order,
  cs.days_since_signup AS customer_tenure_days,

  -- Labels
  f.is_fraud,
  f.is_returned

FROM fact_transactions f
JOIN dim_customer c ON f.customer_key = c.customer_key
  AND f.timestamp BETWEEN c.effective_date AND c.expiration_date
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_store s ON f.store_key = s.store_key
JOIN dim_date d ON f.date_key = d.date_key
LEFT JOIN customer_stats cs ON f.customer_key = cs.customer_key  -- Pre-aggregated stats
WHERE f.timestamp >= '2024-01-01';

-- Partition for fast access
-- PARTITION BY RANGE (f.timestamp);
```

**Trade-offs:**

| Aspect | Normalized (Star Schema) | Denormalized (Wide Table) |
|--------|--------------------------|---------------------------|
| **Storage** | Efficient (no duplication) | Larger (duplicates dimension data) |
| **Query Speed** | Slower (many joins) | Faster (single table scan) |
| **Updates** | Easy (update dimension once) | Hard (update all denormalized rows) |
| **ML Training** | Slower (join overhead) | Faster (no joins needed) |
| **Feature Engineering** | Flexible (join on demand) | Fixed (pre-selected features) |
| **Use Case** | Exploratory analysis | Production training pipelines |

**Best Practice**: Keep both
- **Star schema** for exploration and ad-hoc analysis
- **Denormalized tables** for production training (refreshed daily/weekly)

### 6. Point-in-Time Correctness

**Problem**: Training with future information (data leakage)

**Example of Data Leakage:**

```sql
-- BAD: Uses current customer attributes (introduces leakage)
SELECT
  f.transaction_id,
  f.timestamp AS transaction_time,
  c.customer_segment,  -- This is the CURRENT segment, not segment at transaction time!
  f.is_fraud
FROM fact_transactions f
JOIN dim_customer c ON f.customer_key = c.customer_key
WHERE c.is_current = TRUE;  -- Always uses current record
```

**Problem**: If customer was upgraded from "Bronze" to "Gold" on 2025-06-01, using current segment for transactions in May would leak future information.

**Solution: Point-in-Time Join (SCD Type 2)**

```sql
-- GOOD: Uses customer attributes as of transaction time
SELECT
  f.transaction_id,
  f.timestamp AS transaction_time,
  c.customer_segment,  -- Segment as of transaction time
  f.is_fraud
FROM fact_transactions f
JOIN dim_customer c ON f.customer_key = c.customer_key
  AND f.timestamp >= c.effective_date
  AND f.timestamp < c.expiration_date;  -- Point-in-time join
```

**Verification Query:**

```sql
-- Check if point-in-time join is correct
SELECT
  f.transaction_id,
  f.timestamp,
  c.customer_segment,
  c.effective_date,
  c.expiration_date
FROM fact_transactions f
JOIN dim_customer c ON f.customer_key = c.customer_key
  AND f.timestamp >= c.effective_date
  AND f.timestamp < c.expiration_date
WHERE f.customer_key = 1001  -- Customer who changed segments
ORDER BY f.timestamp;

-- Expected: Transactions before segment change show old segment
```

**Temporal Feature Calculation (Point-in-Time Aware):**

```sql
-- Calculate "transactions in last 30 days" as of training time
WITH training_transactions AS (
  SELECT
    f.transaction_id,
    f.customer_key,
    f.timestamp AS training_time,
    f.is_fraud
  FROM fact_transactions f
  WHERE f.timestamp BETWEEN '2025-01-01' AND '2025-12-31'
)
SELECT
  t.transaction_id,
  t.training_time,
  -- Count transactions in 30 days BEFORE training_time
  COUNT(h.transaction_id) AS transactions_last_30_days,
  t.is_fraud
FROM training_transactions t
LEFT JOIN fact_transactions h
  ON t.customer_key = h.customer_key
  AND h.timestamp >= DATEADD(day, -30, t.training_time)
  AND h.timestamp < t.training_time  -- Only past transactions
GROUP BY t.transaction_id, t.training_time, t.is_fraud;
```

### 7. Dimensional Model for Time-Series ML

**Time-Series Use Case**: Predict customer churn based on activity over time

**Event Fact Table:**

```sql
CREATE TABLE fact_customer_events (
  event_id BIGINT PRIMARY KEY,
  customer_key INT REFERENCES dim_customer(customer_key),
  event_type VARCHAR(50),              -- login, purchase, support_ticket, etc.
  event_timestamp TIMESTAMP,
  event_value DECIMAL(10,2),           -- Optional numeric value
  event_metadata JSON,                 -- Flexible attributes

  -- Time dimension foreign key
  date_key INT REFERENCES dim_date(date_key),

  -- Derived columns for partitioning
  event_date DATE,
  event_hour INT
)
PARTITION BY RANGE (event_date);
```

**Time-Series Feature Engineering:**

```sql
-- Create time-series features for churn prediction
WITH customer_activity AS (
  SELECT
    customer_key,
    event_date,
    -- Daily activity features
    COUNT(*) AS daily_event_count,
    COUNT(DISTINCT event_type) AS daily_event_types,
    SUM(CASE WHEN event_type = 'login' THEN 1 ELSE 0 END) AS daily_logins,
    SUM(CASE WHEN event_type = 'purchase' THEN 1 ELSE 0 END) AS daily_purchases,
    SUM(event_value) AS daily_value
  FROM fact_customer_events
  WHERE event_date >= DATEADD(day, -90, CURRENT_DATE)
  GROUP BY customer_key, event_date
)
SELECT
  ca.customer_key,
  ca.event_date,

  -- Rolling window features (7 days)
  AVG(ca.daily_event_count) OVER w7 AS avg_events_7d,
  SUM(ca.daily_logins) OVER w7 AS total_logins_7d,
  SUM(ca.daily_purchases) OVER w7 AS total_purchases_7d,
  SUM(ca.daily_value) OVER w7 AS total_value_7d,

  -- Rolling window features (30 days)
  AVG(ca.daily_event_count) OVER w30 AS avg_events_30d,
  SUM(ca.daily_logins) OVER w30 AS total_logins_30d,

  -- Trend features (compare 7d vs 30d)
  AVG(ca.daily_event_count) OVER w7 / NULLIF(AVG(ca.daily_event_count) OVER w30, 0) AS activity_trend_ratio,

  -- Recency features
  DATEDIFF(day, MAX(ca.event_date) OVER (PARTITION BY ca.customer_key), CURRENT_DATE) AS days_since_last_activity

FROM customer_activity ca
WINDOW
  w7 AS (PARTITION BY ca.customer_key ORDER BY ca.event_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),
  w30 AS (PARTITION BY ca.customer_key ORDER BY ca.event_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW);
```

---

## 💡 Practical Examples

### Example 1: E-Commerce Star Schema for ML

**Business Requirements:**
- Predict customer churn
- Recommend products
- Detect fraudulent transactions

**Dimensional Model:**

```sql
-- Date Dimension
CREATE TABLE dim_date (
  date_key INT PRIMARY KEY,
  date DATE,
  year INT,
  quarter INT,
  month INT,
  day_of_week INT,
  is_weekend BOOLEAN,
  is_holiday BOOLEAN,
  fiscal_year INT,
  fiscal_quarter INT
);

-- Customer Dimension (SCD Type 2)
CREATE TABLE dim_customer (
  customer_key BIGINT PRIMARY KEY,
  customer_id VARCHAR(100),
  email VARCHAR(200),
  name VARCHAR(200),
  age INT,
  gender VARCHAR(20),
  country VARCHAR(100),
  state VARCHAR(100),
  city VARCHAR(100),
  zip_code VARCHAR(20),
  customer_segment VARCHAR(50),      -- Bronze, Silver, Gold, Platinum
  account_status VARCHAR(50),        -- Active, Suspended, Closed
  signup_date DATE,

  -- SCD Type 2 columns
  effective_date DATE,
  expiration_date DATE,
  is_current BOOLEAN
);

-- Product Dimension (SCD Type 1 - product attributes rarely change)
CREATE TABLE dim_product (
  product_key BIGINT PRIMARY KEY,
  product_id VARCHAR(100),
  product_name VARCHAR(200),
  category VARCHAR(100),
  subcategory VARCHAR(100),
  brand VARCHAR(100),
  price_tier VARCHAR(50),           -- Budget, Mid-range, Premium
  cost DECIMAL(10,2),
  list_price DECIMAL(10,2),
  is_active BOOLEAN
);

-- Transaction Fact Table
CREATE TABLE fact_transactions (
  transaction_id BIGINT PRIMARY KEY,

  -- Foreign keys
  date_key INT REFERENCES dim_date(date_key),
  customer_key BIGINT REFERENCES dim_customer(customer_key),
  product_key BIGINT REFERENCES dim_product(product_key),

  -- Transaction attributes
  transaction_timestamp TIMESTAMP,
  quantity INT,
  unit_price DECIMAL(10,2),
  discount_pct DECIMAL(5,2),
  total_amount DECIMAL(10,2),
  tax_amount DECIMAL(10,2),
  shipping_cost DECIMAL(10,2),
  payment_method VARCHAR(50),
  device_type VARCHAR(50),
  session_id VARCHAR(200),

  -- Labels for ML
  is_fraud BOOLEAN,
  is_returned BOOLEAN,
  churn_within_90_days BOOLEAN     -- Forward-looking label
)
PARTITION BY RANGE (date_key);

-- Customer Aggregates Table (Pre-computed for performance)
CREATE TABLE agg_customer_stats (
  customer_key BIGINT PRIMARY KEY,

  -- Lifetime stats
  first_purchase_date DATE,
  last_purchase_date DATE,
  total_transactions INT,
  total_spent DECIMAL(15,2),
  avg_order_value DECIMAL(10,2),
  total_products_purchased INT,

  -- Recent activity (30 days)
  transactions_last_30d INT,
  spent_last_30d DECIMAL(15,2),

  -- Recent activity (90 days)
  transactions_last_90d INT,
  spent_last_90d DECIMAL(15,2),

  -- Product preferences
  favorite_category VARCHAR(100),
  favorite_brand VARCHAR(100),

  -- Behavioral features
  avg_days_between_purchases DECIMAL(10,2),
  return_rate DECIMAL(5,4),

  -- Updated timestamp
  updated_at TIMESTAMP
);
```

**Materialized View for Training Data:**

```sql
CREATE MATERIALIZED VIEW mv_churn_training_data AS
SELECT
  -- IDs
  f.transaction_id,
  c.customer_id,

  -- Customer features (point-in-time)
  c.age,
  c.gender,
  c.state,
  c.customer_segment,
  DATEDIFF(day, c.signup_date, f.transaction_timestamp) AS tenure_days,

  -- Product features
  p.category,
  p.subcategory,
  p.brand,
  p.price_tier,

  -- Transaction features
  f.quantity,
  f.total_amount,
  f.discount_pct,
  f.payment_method,
  f.device_type,

  -- Temporal features
  d.day_of_week,
  d.is_weekend,
  d.is_holiday,
  EXTRACT(HOUR FROM f.transaction_timestamp) AS hour_of_day,

  -- Aggregated customer features
  a.total_transactions AS customer_lifetime_transactions,
  a.total_spent AS customer_lifetime_value,
  a.avg_order_value AS customer_avg_order_value,
  a.transactions_last_30d,
  a.spent_last_30d,
  a.transactions_last_90d,
  a.avg_days_between_purchases,
  a.return_rate,

  -- Label
  f.churn_within_90_days

FROM fact_transactions f
JOIN dim_customer c
  ON f.customer_key = c.customer_key
  AND f.transaction_timestamp >= c.effective_date
  AND f.transaction_timestamp < c.expiration_date  -- Point-in-time join
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_date d ON f.date_key = d.date_key
LEFT JOIN agg_customer_stats a ON f.customer_key = a.customer_key
WHERE f.transaction_timestamp >= DATEADD(year, -2, CURRENT_DATE);

-- Refresh daily
-- REFRESH MATERIALIZED VIEW mv_churn_training_data;
```

### Example 2: Recommendation System Bridge Table

**Many-to-Many**: Users interact with multiple products, products are interacted by multiple users

```sql
-- Bridge table for user-product interactions
CREATE TABLE bridge_user_product_interaction (
  user_key BIGINT REFERENCES dim_user(user_key),
  product_key BIGINT REFERENCES dim_product(product_key),

  -- Interaction metrics
  view_count INT DEFAULT 0,
  add_to_cart_count INT DEFAULT 0,
  purchase_count INT DEFAULT 0,
  rating DECIMAL(3,2),

  -- Temporal info
  first_interaction_date DATE,
  last_interaction_date DATE,

  -- Derived affinity score
  affinity_score DECIMAL(5,2),      -- 0.0 - 1.0

  PRIMARY KEY (user_key, product_key)
);

-- Populate from transactions
INSERT INTO bridge_user_product_interaction
SELECT
  f.customer_key AS user_key,
  f.product_key,
  0 AS view_count,
  0 AS add_to_cart_count,
  COUNT(*) AS purchase_count,
  NULL AS rating,
  MIN(f.transaction_timestamp)::DATE AS first_interaction_date,
  MAX(f.transaction_timestamp)::DATE AS last_interaction_date,
  -- Simple affinity: log-scaled purchase count
  LEAST(LOG(1 + COUNT(*)) / 5.0, 1.0) AS affinity_score
FROM fact_transactions f
GROUP BY f.customer_key, f.product_key;
```

**Create User-Item Matrix for Collaborative Filtering:**

```sql
-- User-item matrix (sparse)
SELECT
  u.user_id,
  p.product_id,
  b.affinity_score
FROM bridge_user_product_interaction b
JOIN dim_user u ON b.user_key = u.user_key
JOIN dim_product p ON b.product_key = p.product_key
WHERE b.affinity_score > 0.1  -- Filter low-affinity interactions
ORDER BY u.user_id, b.affinity_score DESC;
```

**Export to Python for Matrix Factorization:**

```python
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD

# Load user-item matrix
df = pd.read_sql("""
  SELECT user_id, product_id, affinity_score
  FROM bridge_user_product_interaction b
  JOIN dim_user u ON b.user_key = u.user_key
  JOIN dim_product p ON b.product_key = p.product_key
""", con=conn)

# Create sparse matrix
user_item_matrix = df.pivot(
    index='user_id',
    columns='product_id',
    values='affinity_score'
).fillna(0)

sparse_matrix = csr_matrix(user_item_matrix.values)

# Matrix factorization (SVD)
svd = TruncatedSVD(n_components=50, random_state=42)
user_factors = svd.fit_transform(sparse_matrix)

print(f"User factors shape: {user_factors.shape}")  # (n_users, 50)
```

### Example 3: Late-Arriving Data Handling

**Problem**: Transaction on 2025-10-15 arrives on 2025-10-20 (5 days late)

**Solution: Event Time vs Processing Time**

```sql
-- Fact table with event time and processing time
CREATE TABLE fact_transactions (
  transaction_id BIGINT PRIMARY KEY,

  -- Temporal columns
  event_timestamp TIMESTAMP,          -- When transaction actually occurred
  processing_timestamp TIMESTAMP,     -- When transaction was recorded in warehouse

  -- Partitioning by event_timestamp for ML (reproducibility)
  event_date DATE GENERATED ALWAYS AS (event_timestamp::DATE) STORED,

  -- Other columns...
  customer_key BIGINT,
  total_amount DECIMAL(10,2)
)
PARTITION BY RANGE (event_date);

-- Materialized view with late-arriving data cutoff
CREATE MATERIALIZED VIEW mv_training_data_snapshot AS
SELECT *
FROM fact_transactions
WHERE event_timestamp >= '2024-01-01'
  AND event_timestamp < '2025-01-01'
  AND processing_timestamp <= '2025-01-05';  -- Cutoff: Jan 5 (5 days after year end)

-- This ensures reproducibility: same snapshot regardless of when late data arrives
```

**Backfilling Late Data:**

```sql
-- Mark fact table for reprocessing
ALTER TABLE fact_transactions
ADD COLUMN needs_reprocessing BOOLEAN DEFAULT FALSE;

-- When late data arrives, mark affected partitions
UPDATE fact_transactions
SET needs_reprocessing = TRUE
WHERE event_date = '2025-10-15'
  AND processing_timestamp > '2025-10-20';

-- Reprocess affected training data
CREATE OR REPLACE MATERIALIZED VIEW mv_training_data_snapshot AS
SELECT *
FROM fact_transactions
WHERE event_timestamp >= '2024-01-01'
  AND event_timestamp < '2025-01-01'
  AND (processing_timestamp <= '2025-01-05' OR needs_reprocessing = FALSE);
```

### Example 4: Versioned Dimensional Model for Experiments

**Problem**: Different ML experiments need different feature sets

**Solution: Version dimensions and feature tables**

```sql
-- Version metadata table
CREATE TABLE dim_model_version (
  model_version_key INT PRIMARY KEY,
  model_version VARCHAR(50),
  description TEXT,
  feature_set VARCHAR(200),
  created_date DATE,
  created_by VARCHAR(100)
);

-- Versioned feature table
CREATE TABLE ml_features_versioned (
  feature_id BIGINT PRIMARY KEY,
  model_version_key INT REFERENCES dim_model_version(model_version_key),

  -- Entity
  customer_id VARCHAR(100),
  transaction_id BIGINT,

  -- Features (versioned)
  feature_vector JSON,              -- All features as JSON

  -- Metadata
  created_at TIMESTAMP
)
PARTITION BY LIST (model_version_key);

-- Example: Create features for model v1.0
INSERT INTO dim_model_version VALUES
(1, 'v1.0', 'Baseline churn model', 'age,tenure,total_spent', '2025-01-01', 'data_scientist');

INSERT INTO ml_features_versioned
SELECT
  ROW_NUMBER() OVER () AS feature_id,
  1 AS model_version_key,
  c.customer_id,
  f.transaction_id,
  JSON_BUILD_OBJECT(
    'age', c.age,
    'tenure_days', DATEDIFF(day, c.signup_date, f.transaction_timestamp),
    'total_spent', a.total_spent
  ) AS feature_vector,
  CURRENT_TIMESTAMP AS created_at
FROM fact_transactions f
JOIN dim_customer c ON f.customer_key = c.customer_key
JOIN agg_customer_stats a ON f.customer_key = a.customer_key
WHERE f.transaction_timestamp >= '2024-01-01';

-- Example: Create features for model v2.0 (additional features)
INSERT INTO dim_model_version VALUES
(2, 'v2.0', 'Enhanced churn model', 'age,tenure,total_spent,avg_order_value,return_rate', '2025-02-01', 'data_scientist');

-- Query specific version
SELECT *
FROM ml_features_versioned
WHERE model_version_key = 1;  -- v1.0 features
```

---

## 🔧 Code Examples

### Code Example 1: SCD Type 2 Update Logic

```python
from datetime import datetime, timedelta
import pandas as pd

class SCDType2Manager:
    """Manage SCD Type 2 updates for dimensions"""

    def __init__(self, conn):
        self.conn = conn

    def update_dimension(self, table, natural_key, updates, effective_date=None):
        """
        Update dimension with SCD Type 2 logic

        Args:
            table: Dimension table name
            natural_key: Business key (e.g., customer_id)
            updates: Dict of column:value to update
            effective_date: Date when change becomes effective
        """
        if effective_date is None:
            effective_date = datetime.now().date()

        expiration_date = effective_date - timedelta(days=1)

        # Step 1: Expire current record
        expire_sql = f"""
        UPDATE {table}
        SET
          expiration_date = %s,
          is_current = FALSE,
          updated_at = CURRENT_TIMESTAMP
        WHERE {natural_key} = %s
          AND is_current = TRUE
        """

        with self.conn.cursor() as cursor:
            cursor.execute(expire_sql, (expiration_date, updates[natural_key]))
            print(f"✅ Expired {cursor.rowcount} record(s)")

        # Step 2: Insert new record
        columns = list(updates.keys())
        columns.extend(['effective_date', 'expiration_date', 'is_current', 'inserted_at'])

        values = list(updates.values())
        values.extend([effective_date, datetime(9999, 12, 31).date(), True, datetime.now()])

        placeholders = ', '.join(['%s'] * len(values))
        columns_str = ', '.join(columns)

        insert_sql = f"""
        INSERT INTO {table} ({columns_str})
        VALUES ({placeholders})
        """

        with self.conn.cursor() as cursor:
            cursor.execute(insert_sql, values)
            print(f"✅ Inserted new record")

        self.conn.commit()

    def point_in_time_query(self, table, natural_key, key_value, as_of_date):
        """
        Query dimension as of specific date

        Args:
            table: Dimension table name
            natural_key: Business key column
            key_value: Key value to look up
            as_of_date: Date to query as of

        Returns:
            DataFrame with record as of date
        """
        sql = f"""
        SELECT *
        FROM {table}
        WHERE {natural_key} = %s
          AND effective_date <= %s
          AND expiration_date > %s
        """

        return pd.read_sql(sql, self.conn, params=(key_value, as_of_date, as_of_date))

# Usage
import psycopg2

conn = psycopg2.connect(
    host='localhost',
    database='warehouse',
    user='user',
    password='password'
)

scd_manager = SCDType2Manager(conn)

# Update customer segment
scd_manager.update_dimension(
    table='dim_customer',
    natural_key='customer_id',
    updates={
        'customer_id': 'C123',
        'name': 'John Doe',
        'customer_segment': 'Gold'  # Changed from Silver
    },
    effective_date=datetime(2025, 10, 20).date()
)

# Query historical state
result = scd_manager.point_in_time_query(
    table='dim_customer',
    natural_key='customer_id',
    key_value='C123',
    as_of_date=datetime(2025, 10, 15).date()  # Before upgrade
)

print(result)
```

### Code Example 2: Feature Engineering from Star Schema

```python
class FeatureEngineering:
    """Extract ML features from dimensional model"""

    def __init__(self, conn):
        self.conn = conn

    def create_customer_features(self, training_start, training_end):
        """
        Create customer features for churn prediction

        Args:
            training_start: Start date for training period
            training_end: End date for training period

        Returns:
            DataFrame with customer features
        """

        query = """
        WITH customer_transactions AS (
          SELECT
            c.customer_id,
            c.age,
            c.gender,
            c.state,
            c.customer_segment,
            f.transaction_timestamp,
            f.total_amount,
            f.quantity,
            f.is_returned
          FROM fact_transactions f
          JOIN dim_customer c
            ON f.customer_key = c.customer_key
            AND f.transaction_timestamp >= c.effective_date
            AND f.transaction_timestamp < c.expiration_date
          WHERE f.transaction_timestamp >= %s
            AND f.transaction_timestamp < %s
        ),
        customer_features AS (
          SELECT
            customer_id,

            -- Demographic features
            MAX(age) AS age,
            MAX(gender) AS gender,
            MAX(state) AS state,
            MAX(customer_segment) AS customer_segment,

            -- Transaction features
            COUNT(*) AS total_transactions,
            SUM(total_amount) AS total_spent,
            AVG(total_amount) AS avg_order_value,
            STDDEV(total_amount) AS std_order_value,
            MAX(total_amount) AS max_order_value,

            -- Temporal features
            DATE_PART('day', MAX(transaction_timestamp) - MIN(transaction_timestamp)) AS days_active,
            DATE_PART('day', CURRENT_DATE - MAX(transaction_timestamp)) AS days_since_last_tx,

            -- Behavioral features
            SUM(CASE WHEN is_returned THEN 1 ELSE 0 END)::FLOAT / COUNT(*) AS return_rate,
            COUNT(DISTINCT DATE_TRUNC('day', transaction_timestamp)) AS active_days

          FROM customer_transactions
          GROUP BY customer_id
        )
        SELECT *
        FROM customer_features
        """

        df = pd.read_sql(query, self.conn, params=(training_start, training_end))

        print(f"✅ Generated features for {len(df)} customers")
        return df

    def create_time_series_features(self, customer_id, window_days=30):
        """
        Create time-series features for a customer

        Args:
            customer_id: Customer ID
            window_days: Rolling window size in days

        Returns:
            DataFrame with time-series features
        """

        query = """
        WITH daily_activity AS (
          SELECT
            c.customer_id,
            DATE_TRUNC('day', f.transaction_timestamp) AS activity_date,
            COUNT(*) AS daily_tx_count,
            SUM(f.total_amount) AS daily_spent
          FROM fact_transactions f
          JOIN dim_customer c
            ON f.customer_key = c.customer_key
            AND f.transaction_timestamp >= c.effective_date
            AND f.transaction_timestamp < c.expiration_date
          WHERE c.customer_id = %s
            AND f.transaction_timestamp >= CURRENT_DATE - INTERVAL '%s days'
          GROUP BY c.customer_id, DATE_TRUNC('day', f.transaction_timestamp)
        )
        SELECT
          customer_id,
          activity_date,
          daily_tx_count,
          daily_spent,

          -- Rolling averages
          AVG(daily_tx_count) OVER w7 AS avg_tx_count_7d,
          AVG(daily_spent) OVER w7 AS avg_spent_7d,
          AVG(daily_tx_count) OVER w30 AS avg_tx_count_30d,
          AVG(daily_spent) OVER w30 AS avg_spent_30d

        FROM daily_activity
        WINDOW
          w7 AS (ORDER BY activity_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW),
          w30 AS (ORDER BY activity_date ROWS BETWEEN 29 PRECEDING AND CURRENT ROW)
        ORDER BY activity_date
        """

        df = pd.read_sql(query, self.conn, params=(customer_id, window_days))

        return df

# Usage
fe = FeatureEngineering(conn)

# Create customer features
features = fe.create_customer_features(
    training_start='2024-01-01',
    training_end='2025-01-01'
)

print(features.head())
print(f"Feature columns: {features.columns.tolist()}")
```

---

## ✅ Best Practices

### 1. SCD Type Selection

**DO:**
- ✅ Use SCD Type 2 for ML-critical attributes (customer segment, location)
- ✅ Use SCD Type 1 for corrections (typos, data quality fixes)
- ✅ Use SCD Type 3 for limited before/after comparisons
- ✅ Document SCD strategy per dimension
- ✅ Include effective_date, expiration_date, is_current columns

**DON'T:**
- ❌ Use Type 1 for attributes that impact model predictions
- ❌ Mix SCD types within same dimension without clear reason
- ❌ Forget to index (natural_key, effective_date, expiration_date)
- ❌ Skip point-in-time joins (causes data leakage)

### 2. Fact Table Granularity

**DO:**
- ✅ Keep atomic/transactional grain for ML (individual events)
- ✅ Create aggregated fact tables separately for BI
- ✅ Include all relevant attributes (avoid joins during training)
- ✅ Add labels (is_fraud, is_churn) directly to fact table
- ✅ Partition fact tables by date for query performance

**DON'T:**
- ❌ Aggregate fact tables (loses row-level features)
- ❌ Omit timestamps (needed for temporal features)
- ❌ Store only foreign keys (join overhead)
- ❌ Create one fact table for all use cases

### 3. Denormalization

**DO:**
- ✅ Denormalize for production training pipelines
- ✅ Keep normalized star schema for exploration
- ✅ Refresh denormalized tables on schedule (daily/weekly)
- ✅ Document denormalization trade-offs
- ✅ Monitor storage costs

**DON'T:**
- ❌ Denormalize prematurely (start with star schema)
- ❌ Update denormalized tables in real-time (too expensive)
- ❌ Denormalize without measuring performance gain
- ❌ Forget to version denormalized tables

### 4. Point-in-Time Correctness

**DO:**
- ✅ Always use point-in-time joins for SCD Type 2 dimensions
- ✅ Test for data leakage (verify no future information)
- ✅ Use event_timestamp (not processing_timestamp) for features
- ✅ Document temporal semantics in data dictionary
- ✅ Create validation queries to detect leakage

**DON'T:**
- ❌ Join on is_current = TRUE (introduces leakage)
- ❌ Use CURRENT_DATE in feature calculations
- ❌ Ignore late-arriving data
- ❌ Assume timestamps are always correct

---

## 📝 Key Takeaways

1. **Adapt star schemas for ML**: Keep dimensional structure but add SCD Type 2, atomic fact tables, and point-in-time correctness.

2. **SCD Type 2 prevents data leakage**: Track attribute history with effective_date and expiration_date. Always use point-in-time joins.

3. **Atomic fact tables for ML**: Store individual transactions/events, not aggregates. Derive features at query time.

4. **Bridge tables for many-to-many**: Model user-product interactions, user-interests, product-categories using bridge tables.

5. **Denormalize for production**: Create wide feature tables for training pipelines. Keep star schema for exploration.

6. **Point-in-time correctness is critical**: Use event timestamps and SCD-aware joins to prevent future information leakage.

7. **Pre-compute aggregates separately**: Create materialized views or aggregate tables for expensive calculations.

8. **Version features for experiments**: Track model versions and associated feature sets for reproducibility.

9. **Handle late-arriving data**: Separate event_timestamp from processing_timestamp. Define cutoff policies for training snapshots.

10. **Balance flexibility and performance**: Star schema = flexible exploration. Denormalized = fast training. Use both.

---

## ✏️ Notes Section

**Personal Insights:**

---

**Questions:**

---

**Action Items:**

---

**Related Projects:**

---

**Code Snippets:**

---

**Further Exploration:**

---

**📌 Tags:** `#dimensional-modeling` `#star-schema` `#scd` `#fact-tables` `#feature-engineering` `#ml-data-modeling` `#point-in-time` `#data-warehousing`

---

**Navigation:**
- ← Previous: [05.01 - Modern Data Warehouses](01.%20Modern%20Data%20Warehouses.md)
- → Next: [05.03 - Analytical Queries for Feature Derivation](03.%20Analytical%20Queries%20for%20Feature%20Derivation.md)
- ↑ Up: [Chapter 05 - Data Warehousing for Analytics & ML](README.md)
- ⌂ Home: [DEforAI Course Index](../../README.md)

---

*Last updated: 2025-10-20*
