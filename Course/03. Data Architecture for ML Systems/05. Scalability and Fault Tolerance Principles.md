# 05. Scalability and Fault Tolerance Principles

**Date:** 2025-10-19
**Status:** #research
**Tags:** #data-engineering #ml-systems #scalability #fault-tolerance #high-availability #load-balancing #caching

---

## 📋 Overview

**Scalability** and **fault tolerance** are fundamental requirements for production ML systems. As user traffic grows and business requirements evolve, ML systems must scale efficiently without degrading performance. Simultaneously, systems must remain operational despite component failures, network issues, and unexpected load spikes.

**Scalability** is the system's ability to handle increasing load by adding resources. ML systems face unique scaling challenges:
- **Model serving**: Thousands of inference requests per second with strict latency requirements (<100ms)
- **Feature computation**: Processing millions of events per second in real-time
- **Model training**: Training on terabytes of data with distributed computing
- **Feature storage**: Serving billions of feature vectors with sub-millisecond latency

**Fault tolerance** ensures systems continue operating correctly despite failures. In distributed ML systems, failures are the norm, not the exception:
- Servers crash
- Networks partition
- Databases become unavailable
- Upstream services fail

This subchapter explores proven patterns for building ML systems that scale horizontally, handle failures gracefully, and maintain high availability.

**Key Concepts**:
- **Horizontal Scaling**: Adding more machines (scale out) vs vertical scaling (bigger machines)
- **Load Balancing**: Distributing requests across multiple instances
- **Caching**: Reducing latency and load with multi-tier caches
- **Database Scaling**: Replication, sharding, read replicas
- **High Availability**: Redundancy, failover, and disaster recovery
- **Auto-Scaling**: Dynamic resource allocation based on load

---

## 🎯 Learning Objectives

By the end of this subchapter, you will:

1. **Distinguish between horizontal and vertical scaling** and select appropriate strategies for ML workloads
2. **Design load balancing strategies** for model serving with sticky sessions and consistent hashing
3. **Implement multi-tier caching** for feature stores to achieve <10ms latency
4. **Scale databases** using replication, sharding, and read replicas
5. **Build fault-tolerant services** with retries, circuit breakers, and bulkheads
6. **Implement high availability** with active-active and active-passive configurations
7. **Configure auto-scaling** for Kubernetes-based ML services using HPA and custom metrics
8. **Apply rate limiting and backpressure** to prevent system overload
9. **Design disaster recovery plans** with backup, restore, and failover procedures
10. **Monitor scalability metrics** (throughput, latency, error rate) to identify bottlenecks

---

## 📚 Core Concepts

### 1. Horizontal vs Vertical Scaling

**Vertical Scaling** (Scale Up):
- Add more CPU, RAM, or storage to existing machines
- **Example**: Upgrade from 4-core to 16-core instance

**Pros**:
- Simpler (no distributed system complexity)
- No code changes required
- Better for stateful applications (databases)

**Cons**:
- Limited by hardware constraints (maximum instance size)
- Single point of failure
- Downtime required for scaling
- Diminishing returns (doubling CPU ≠ doubling performance)

**Horizontal Scaling** (Scale Out):
- Add more machines to distribute load
- **Example**: Scale from 5 instances to 20 instances

**Pros**:
- Nearly unlimited scaling (add more machines)
- Better fault tolerance (failure of one instance doesn't bring down system)
- No downtime for scaling
- Cost-effective (use smaller instances)

**Cons**:
- Requires stateless services or distributed state management
- Increased operational complexity (load balancing, service discovery)
- Network latency between nodes

**ML System Scaling Recommendations**:

| **Component** | **Recommended Approach** | **Rationale** |
|--------------|-------------------------|--------------|
| Model Serving API | Horizontal | Stateless, needs to handle variable load |
| Feature Store (online) | Horizontal | High read throughput requirement |
| PostgreSQL (metadata) | Vertical + Read Replicas | Stateful, complex to shard |
| Redis (cache) | Horizontal (Redis Cluster) | High throughput, key-based partitioning |
| Spark (batch training) | Horizontal | Distributed by design |
| Model Training (single GPU) | Vertical | Larger GPU = faster training |
| Model Training (distributed) | Horizontal | Multi-GPU training across nodes |

---

### 2. Load Balancing Strategies

**Definition**: Distribute incoming requests across multiple service instances to maximize throughput and minimize latency.

**Load Balancing Algorithms**:

1. **Round Robin**: Distribute requests sequentially
   - **Pros**: Simple, fair distribution
   - **Cons**: Doesn't account for instance load or health

2. **Least Connections**: Route to instance with fewest active connections
   - **Pros**: Balances load more evenly
   - **Cons**: Requires connection tracking overhead

3. **Weighted Round Robin**: Assign weights to instances based on capacity
   - **Example**: Send 2x traffic to instances with 2x CPU
   - **Pros**: Accounts for heterogeneous instances
   - **Cons**: Static weights don't adapt to real-time load

4. **IP Hash**: Route based on client IP (consistent routing)
   - **Pros**: Enables session affinity (same client → same instance)
   - **Cons**: Uneven distribution if client IPs are clustered

5. **Consistent Hashing**: Route based on request key (e.g., user_id)
   - **Pros**: Minimizes cache invalidation when instances change
   - **Cons**: Requires application-level key extraction

**Load Balancer Types**:

- **Layer 4 (Transport Layer)**: Route based on IP/port (AWS NLB, HAProxy)
  - **Latency**: <1ms overhead
  - **Use case**: High-throughput, low-latency requirements

- **Layer 7 (Application Layer)**: Route based on HTTP headers, path (AWS ALB, Nginx)
  - **Latency**: 2-5ms overhead
  - **Use case**: Content-based routing, URL path routing

**Sticky Sessions for ML**:
When using in-memory model caching, route the same user to the same instance to maximize cache hits:

```nginx
upstream model_servers {
    ip_hash;  # Route based on client IP
    server model-server-1:8000;
    server model-server-2:8000;
    server model-server-3:8000;
}
```

---

### 3. Caching Strategies

**Principle**: Cache expensive computations to reduce latency and load on backend services.

**Multi-Tier Caching for ML Feature Store**:

```
Request Flow:
1. In-Memory Cache (Application) → <1ms latency
   ↓ (miss)
2. Redis (Distributed Cache) → 1-5ms latency
   ↓ (miss)
3. Database (Source of Truth) → 10-100ms latency
```

**Cache Patterns**:

1. **Cache-Aside** (Lazy Loading):
   - Application checks cache first
   - On miss, load from database and populate cache
   - **Pros**: Only cache what's requested
   - **Cons**: First request is slow (cache miss)

2. **Write-Through**:
   - Write to cache and database simultaneously
   - **Pros**: Cache always consistent with database
   - **Cons**: Write latency increased

3. **Write-Back** (Write-Behind):
   - Write to cache immediately, asynchronously write to database
   - **Pros**: Fast writes
   - **Cons**: Risk of data loss if cache fails before DB write

4. **Refresh-Ahead**:
   - Proactively refresh cache before expiration
   - **Pros**: Minimize cache misses for frequently accessed items
   - **Cons**: Wasted work if items not accessed

**Cache Eviction Policies**:
- **LRU (Least Recently Used)**: Evict oldest accessed items (most common)
- **LFU (Least Frequently Used)**: Evict least accessed items
- **TTL (Time To Live)**: Evict after fixed time

**Optimal TTL for ML Features**:
- Real-time features: 1-5 minutes (fast-changing user behavior)
- Daily features: 24 hours (batch-computed overnight)
- Static features: 7 days (user demographics)

---

### 4. Database Scaling Patterns

**Read Replicas**:
- Asynchronous replication from primary to read-only replicas
- **Use case**: Read-heavy workloads (90% reads, 10% writes)
- **Latency**: Replication lag 1-5 seconds (eventual consistency)

```python
# Write to primary
primary_db.execute("INSERT INTO features (user_id, value) VALUES (?, ?)", (user_id, value))

# Read from replica
replica_db.query("SELECT value FROM features WHERE user_id = ?", (user_id,))
```

**Sharding** (Horizontal Partitioning):
- Split data across multiple databases based on shard key
- **Example**: Shard users by `user_id % num_shards`

```python
def get_shard(user_id: str, num_shards: int = 10):
    shard_id = hash(user_id) % num_shards
    return f"db_shard_{shard_id}"

# Route to appropriate shard
shard = get_shard("user_123")
db = get_connection(shard)
features = db.query("SELECT * FROM features WHERE user_id = 'user_123'")
```

**Pros**:
- Near-linear scaling (10 shards = 10x capacity)
- Each shard is smaller and faster

**Cons**:
- Complex queries across shards (joins, aggregations)
- Rebalancing when adding/removing shards
- Shard key selection is critical (bad key = hot shards)

**Partitioning Strategies**:
- **Range-based**: user_id < 1000 → shard 0, 1000-2000 → shard 1 (risk: uneven distribution)
- **Hash-based**: hash(user_id) % num_shards (even distribution)
- **Geographic**: US users → US DB, EU users → EU DB (data locality)

**Database Replication Topologies**:

1. **Primary-Replica** (most common):
   - 1 primary (writes), N replicas (reads)
   - Failover: Promote replica to primary if primary fails

2. **Multi-Primary** (active-active):
   - Multiple primaries accept writes
   - Conflict resolution required (last-write-wins, CRDTs)
   - **Use case**: Geo-distributed applications

---

### 5. Fault Tolerance Mechanisms

**Redundancy**:
- Run multiple instances of each service
- If one fails, others continue serving traffic

**Retries with Exponential Backoff**:
```python
import time
from functools import wraps

def retry_with_backoff(max_retries=3, base_delay=1):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise e
                    delay = base_delay * (2 ** attempt)  # Exponential backoff
                    time.sleep(delay)
        return wrapper
    return decorator

@retry_with_backoff(max_retries=3, base_delay=1)
def call_feature_service(user_id):
    response = requests.get(f"http://feature-service/user/{user_id}")
    response.raise_for_status()
    return response.json()
```

**Circuit Breaker**:
- Prevent cascading failures by failing fast when service is down
- See [[04. Microservices and Event-Driven Patterns]] for implementation

**Bulkhead Pattern**:
- Isolate resources to prevent one failure from affecting others
- **Example**: Separate thread pools for different services

```python
from concurrent.futures import ThreadPoolExecutor

# Separate thread pools for each external service
feature_service_executor = ThreadPoolExecutor(max_workers=10)
model_service_executor = ThreadPoolExecutor(max_workers=20)

# If feature service hangs, only 10 threads affected
# Model service continues with its 20 threads
```

**Health Checks**:
```python
@app.get("/health")
async def health_check():
    checks = {
        "database": check_database_connection(),
        "redis": check_redis_connection(),
        "model_loaded": model is not None
    }

    all_healthy = all(checks.values())
    return {
        "status": "healthy" if all_healthy else "unhealthy",
        "checks": checks
    }
```

**Graceful Degradation**:
- Continue operating with reduced functionality when dependencies fail

```python
async def get_recommendations(user_id: str):
    try:
        # Try to get personalized recommendations
        user_features = await feature_service.get_features(user_id)
        recommendations = model.predict(user_features)
    except Exception as e:
        logger.warning(f"Personalized recommendations failed: {e}")
        # Fallback to popular items (graceful degradation)
        recommendations = get_popular_items()

    return recommendations
```

---

### 6. High Availability Patterns

**High Availability (HA)**: System remains operational despite component failures.

**Availability Calculation**:
```
Availability = (Total Time - Downtime) / Total Time

99.9% ("three nines") = 43.2 minutes downtime/month
99.99% ("four nines") = 4.32 minutes downtime/month
99.999% ("five nines") = 26 seconds downtime/month
```

**Active-Active Configuration**:
- Multiple instances actively serving traffic
- Load balancer distributes requests
- If one instance fails, others continue

```
Load Balancer
   ↓      ↓      ↓
 API-1  API-2  API-3  (all active)
```

**Active-Passive Configuration**:
- One instance serves traffic (active)
- Backup instance ready to take over (passive)
- Failover when active instance fails

```
Load Balancer
   ↓
 API-1 (active)
 API-2 (passive, standby)
```

**Database High Availability**:

1. **Synchronous Replication** (PostgreSQL with streaming replication):
   - Primary commits transaction only after replica acknowledges
   - **Pros**: Zero data loss
   - **Cons**: Slower writes (wait for replica)

2. **Asynchronous Replication**:
   - Primary commits immediately, replicates asynchronously
   - **Pros**: Fast writes
   - **Cons**: Potential data loss during failover

**Failover Types**:
- **Manual Failover**: Operator manually promotes replica (slow, error-prone)
- **Automatic Failover**: System detects failure and promotes replica (fast, requires careful configuration)

**Example: PostgreSQL Automatic Failover with Patroni**:
```yaml
# patroni.yml
scope: ml-features-db
namespace: /service/
name: postgres-1

restapi:
  listen: 0.0.0.0:8008
  connect_address: postgres-1:8008

postgresql:
  listen: 0.0.0.0:5432
  connect_address: postgres-1:5432
  data_dir: /var/lib/postgresql/data
  parameters:
    max_connections: 100

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576  # 1MB
```

---

### 7. Auto-Scaling

**Principle**: Automatically add/remove instances based on load to optimize cost and performance.

**Metrics for Auto-Scaling**:

1. **CPU Utilization**: Scale up if >70%, scale down if <30%
2. **Memory Utilization**: Scale up if >80%
3. **Request Count**: Scale up if requests/instance > threshold
4. **Custom Metrics**: Inference latency, queue depth, error rate

**Kubernetes Horizontal Pod Autoscaler (HPA)**:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: model-server-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: model-server
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
    scaleUp:
      stabilizationWindowSeconds: 60   # Scale up quickly
```

**Custom Metrics Auto-Scaling**:

```python
from prometheus_client import Gauge

# Expose custom metric
inference_latency_p99 = Gauge('inference_latency_p99_ms', 'P99 inference latency')

# Update metric
inference_latency_p99.set(calculate_p99_latency())

# HPA scales based on this metric
```

**Auto-Scaling Strategies**:

1. **Reactive Scaling**: Scale based on current metrics
   - **Pros**: Simple
   - **Cons**: Lag (scale-up takes minutes)

2. **Predictive Scaling**: Scale based on predicted load
   - **Example**: Scale up before expected traffic spike (Black Friday)
   - **Pros**: Proactive, no latency spike
   - **Cons**: Requires accurate forecasting

3. **Scheduled Scaling**: Scale based on time
   - **Example**: Scale up during business hours, scale down at night
   - **Pros**: Predictable cost
   - **Cons**: Doesn't handle unexpected load

---

### 8. Rate Limiting and Backpressure

**Rate Limiting**: Control request rate to prevent overload.

**Rate Limiting Algorithms**:

1. **Token Bucket**:
   - Bucket holds tokens (e.g., 100 tokens)
   - Each request consumes 1 token
   - Tokens refill at fixed rate (e.g., 10/second)
   - If no tokens available, request rejected

```python
import time

class TokenBucket:
    def __init__(self, capacity: int, refill_rate: float):
        self.capacity = capacity
        self.tokens = capacity
        self.refill_rate = refill_rate
        self.last_refill = time.time()

    def allow_request(self) -> bool:
        self._refill()

        if self.tokens >= 1:
            self.tokens -= 1
            return True
        return False

    def _refill(self):
        now = time.time()
        elapsed = now - self.last_refill
        new_tokens = elapsed * self.refill_rate
        self.tokens = min(self.capacity, self.tokens + new_tokens)
        self.last_refill = now

# Usage
limiter = TokenBucket(capacity=100, refill_rate=10)

if limiter.allow_request():
    # Process request
    pass
else:
    # Return 429 Too Many Requests
    pass
```

2. **Leaky Bucket**:
   - Requests added to queue (bucket)
   - Requests processed at fixed rate (leak)
   - If queue full, reject requests

3. **Fixed Window**:
   - Allow N requests per time window (e.g., 1000 req/minute)
   - **Cons**: Burst at window boundaries

4. **Sliding Window**:
   - Track requests in rolling time window
   - More accurate than fixed window

**Distributed Rate Limiting** (using Redis):

```python
import redis
import time

class DistributedRateLimiter:
    def __init__(self, redis_client, key_prefix="rate_limit"):
        self.redis = redis_client
        self.key_prefix = key_prefix

    def allow_request(self, user_id: str, limit: int = 100, window: int = 60):
        """
        Allow 'limit' requests per 'window' seconds
        """
        key = f"{self.key_prefix}:{user_id}"
        current_time = int(time.time())
        window_start = current_time - window

        # Remove old requests outside window
        self.redis.zremrangebyscore(key, 0, window_start)

        # Count requests in current window
        request_count = self.redis.zcard(key)

        if request_count < limit:
            # Add current request
            self.redis.zadd(key, {current_time: current_time})
            self.redis.expire(key, window)
            return True

        return False

# Usage
limiter = DistributedRateLimiter(redis_client)

if limiter.allow_request("user_123", limit=1000, window=60):
    # Process request
    pass
else:
    # Return 429 Too Many Requests
    pass
```

**Backpressure**: Slow down producers when consumers can't keep up.

```python
# Producer (data ingestion service)
async def ingest_events():
    while True:
        event = await get_next_event()

        # Check consumer lag
        lag = kafka_consumer.lag()

        if lag > 1_000_000:  # 1M message lag threshold
            # Backpressure: slow down ingestion
            await asyncio.sleep(1)

        await kafka_producer.send(event)
```

---

### 9. Disaster Recovery

**RPO (Recovery Point Objective)**: Maximum acceptable data loss
- **Example**: RPO = 1 hour → can lose up to 1 hour of data

**RTO (Recovery Time Objective)**: Maximum acceptable downtime
- **Example**: RTO = 15 minutes → system must be restored within 15 min

**Backup Strategies**:

1. **Full Backup**: Complete copy of all data
   - **Frequency**: Weekly
   - **Restore Time**: Hours

2. **Incremental Backup**: Only changes since last backup
   - **Frequency**: Daily
   - **Restore Time**: Moderate (need full + all incrementals)

3. **Continuous Backup**: Real-time replication
   - **Frequency**: Continuous
   - **Restore Time**: Minutes

**Backup Strategy for ML Systems**:

```yaml
# Backup schedule
- Model artifacts (S3):
    strategy: Versioned storage with lifecycle policy
    retention: Keep all versions for 30 days, then keep monthly snapshots

- Training data (Delta Lake):
    strategy: Time travel (built-in versioning)
    retention: 30 days of history

- Feature store (Redis):
    strategy: RDB snapshots + AOF logs
    frequency: Snapshots every 6 hours, AOF every second
    retention: 7 days

- Metadata database (PostgreSQL):
    strategy: Continuous archiving with WAL
    frequency: Base backup weekly, WAL every 5 minutes
    retention: 30 days of WAL
```

**Disaster Recovery Testing**:
- **Chaos Engineering**: Intentionally inject failures to test recovery
- **Disaster Recovery Drills**: Simulate disasters quarterly

**Example: Automated DR Failover**:

```bash
#!/bin/bash
# dr_failover.sh - Automate disaster recovery failover

# 1. Detect primary datacenter failure
if ! ping -c 3 primary-dc-lb; then
    echo "Primary datacenter unreachable, initiating failover..."

    # 2. Promote secondary database to primary
    patronictl failover ml-features-db --candidate postgres-secondary-1 --force

    # 3. Update DNS to point to secondary datacenter
    aws route53 change-resource-record-sets \
        --hosted-zone-id Z123456789 \
        --change-batch file://dns-failover.json

    # 4. Scale up secondary datacenter instances
    kubectl scale deployment model-server --replicas=20 -n ml-prod

    # 5. Notify team
    curl -X POST https://slack.com/api/chat.postMessage \
        -d "text=Disaster recovery failover completed to secondary DC"

    echo "Failover complete."
else
    echo "Primary datacenter healthy."
fi
```

---

### 10. Monitoring Scalability

**Key Metrics** (RED method):
- **Rate**: Requests per second
- **Errors**: Error rate (%)
- **Duration**: Latency (p50, p95, p99)

**Saturation Metrics**:
- CPU utilization
- Memory utilization
- Disk I/O
- Network bandwidth

**Custom ML Metrics**:
- Model inference latency
- Feature fetch latency
- Cache hit rate
- Model version distribution (for canary deployments)

**Prometheus + Grafana Dashboard**:

```yaml
# prometheus-rules.yml
groups:
  - name: ml-system-alerts
    rules:
      # Alert if p99 latency > 200ms
      - alert: HighInferenceLatency
        expr: histogram_quantile(0.99, rate(inference_latency_seconds_bucket[5m])) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High inference latency detected"

      # Alert if error rate > 1%
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.01
        for: 5m
        labels:
          severity: critical

      # Alert if cache hit rate < 90%
      - alert: LowCacheHitRate
        expr: rate(cache_hits[5m]) / rate(cache_requests[5m]) < 0.9
        for: 10m
        labels:
          severity: warning
```

---

## 💡 Practical Examples

### Example 1: Auto-Scaling Model Serving with Kubernetes

**Scenario**: Model serving API that needs to handle variable load (100-10,000 req/sec).

**Implementation**:

**1. Containerize Model Server**:
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY model_server.py .
COPY model.pkl .

# Health check
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8000/health || exit 1

CMD ["python", "model_server.py"]
```

**2. Kubernetes Deployment**:
```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraud-detector
  labels:
    app: fraud-detector
spec:
  replicas: 3  # Initial replicas
  selector:
    matchLabels:
      app: fraud-detector
  template:
    metadata:
      labels:
        app: fraud-detector
    spec:
      containers:
      - name: model-server
        image: fraud-detector:v1
        ports:
        - containerPort: 8000
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: fraud-detector-service
spec:
  type: LoadBalancer
  selector:
    app: fraud-detector
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
```

**3. Horizontal Pod Autoscaler**:
```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fraud-detector-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fraud-detector
  minReplicas: 3
  maxReplicas: 50
  metrics:
  # Scale based on CPU
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # Scale based on custom metric (requests per pod)
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"  # 100 req/sec per pod

  # Scale based on inference latency
  - type: Pods
    pods:
      metric:
        name: inference_latency_p99_ms
      target:
        type: AverageValue
        averageValue: "100"  # Keep p99 latency < 100ms

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
      policies:
      - type: Percent
        value: 50  # Scale down max 50% of current replicas at a time
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Scale up immediately
      policies:
      - type: Percent
        value: 100  # Can double replicas
        periodSeconds: 15
      - type: Pods
        value: 10  # Or add 10 pods
        periodSeconds: 15
      selectPolicy: Max  # Use most aggressive policy
```

**4. Custom Metrics Exporter**:
```python
# model_server.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from fastapi import FastAPI
import time

app = FastAPI()

# Prometheus metrics
request_count = Counter('http_requests_total', 'Total HTTP requests')
request_latency = Histogram('inference_latency_seconds', 'Inference latency')
inference_latency_p99 = Gauge('inference_latency_p99_ms', 'P99 inference latency')

# Track latencies for p99 calculation
latencies = []

@app.post("/predict")
async def predict(features: dict):
    start_time = time.time()

    # Model inference
    prediction = model.predict(features)

    # Record metrics
    latency = (time.time() - start_time) * 1000  # ms
    latencies.append(latency)

    # Update Prometheus metrics
    request_count.inc()
    request_latency.observe(latency / 1000)  # seconds

    # Update p99 every 100 requests
    if len(latencies) >= 100:
        p99 = calculate_percentile(latencies, 0.99)
        inference_latency_p99.set(p99)
        latencies = latencies[-100:]  # Keep last 100

    return {"prediction": prediction, "latency_ms": latency}

# Start Prometheus metrics server
start_http_server(9090)
```

**5. Load Test to Trigger Auto-Scaling**:
```bash
# locust_load_test.py
from locust import HttpUser, task, between

class ModelUser(HttpUser):
    wait_time = between(0.1, 0.5)  # 0.1-0.5s between requests

    @task
    def predict(self):
        self.client.post("/predict", json={"user_id": "123", "amount": 100.0})

# Run load test
locust -f locust_load_test.py --host http://fraud-detector-service --users 1000 --spawn-rate 100
```

**Expected Behavior**:
- Start with 3 replicas
- As load increases, CPU utilization and request rate rise
- HPA scales up to handle load (max 50 replicas)
- When load decreases, HPA scales down (after 5-min stabilization window)

---

### Example 2: Multi-Tier Caching for Feature Store

**Scenario**: Feature store serving 100K req/sec with <10ms p99 latency.

**Implementation**:

```python
import redis
from functools import lru_cache
import time
import asyncio
from typing import Optional

class MultiTierFeatureStore:
    def __init__(self):
        # Tier 1: In-memory cache (application-level)
        self.memory_cache = {}
        self.memory_cache_size = 10000
        self.memory_cache_ttl = 60  # 1 minute

        # Tier 2: Redis (distributed cache)
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.redis_ttl = 3600  # 1 hour

        # Tier 3: PostgreSQL (source of truth)
        self.db = get_database_connection()

        # Metrics
        self.metrics = {
            "memory_hits": 0,
            "redis_hits": 0,
            "db_hits": 0,
            "total_requests": 0
        }

    async def get_features(self, user_id: str) -> dict:
        self.metrics["total_requests"] += 1
        start_time = time.time()

        # Tier 1: Check in-memory cache
        features = self._check_memory_cache(user_id)
        if features:
            self.metrics["memory_hits"] += 1
            print(f"Memory cache hit: {(time.time() - start_time) * 1000:.2f}ms")
            return features

        # Tier 2: Check Redis
        features = await self._check_redis_cache(user_id)
        if features:
            self.metrics["redis_hits"] += 1
            # Populate memory cache
            self._set_memory_cache(user_id, features)
            print(f"Redis cache hit: {(time.time() - start_time) * 1000:.2f}ms")
            return features

        # Tier 3: Query database
        features = await self._query_database(user_id)
        if features:
            self.metrics["db_hits"] += 1
            # Populate Redis and memory cache
            await self._set_redis_cache(user_id, features)
            self._set_memory_cache(user_id, features)
            print(f"Database hit: {(time.time() - start_time) * 1000:.2f}ms")
            return features

        return {}

    def _check_memory_cache(self, user_id: str) -> Optional[dict]:
        if user_id in self.memory_cache:
            cached_data = self.memory_cache[user_id]
            # Check TTL
            if time.time() - cached_data["timestamp"] < self.memory_cache_ttl:
                return cached_data["features"]
            else:
                # Expired, remove from cache
                del self.memory_cache[user_id]
        return None

    def _set_memory_cache(self, user_id: str, features: dict):
        # Evict oldest if cache full (simple LRU)
        if len(self.memory_cache) >= self.memory_cache_size:
            oldest_key = min(self.memory_cache, key=lambda k: self.memory_cache[k]["timestamp"])
            del self.memory_cache[oldest_key]

        self.memory_cache[user_id] = {
            "features": features,
            "timestamp": time.time()
        }

    async def _check_redis_cache(self, user_id: str) -> Optional[dict]:
        key = f"features:user:{user_id}"
        cached = self.redis_client.hgetall(key)

        if cached:
            return {k: float(v) for k, v in cached.items()}
        return None

    async def _set_redis_cache(self, user_id: str, features: dict):
        key = f"features:user:{user_id}"
        self.redis_client.hset(key, mapping=features)
        self.redis_client.expire(key, self.redis_ttl)

    async def _query_database(self, user_id: str) -> Optional[dict]:
        result = await self.db.fetchone(
            "SELECT purchase_count_30d, avg_amount FROM user_features WHERE user_id = ?",
            (user_id,)
        )

        if result:
            return {
                "purchase_count_30d": result[0],
                "avg_amount": result[1]
            }
        return None

    def get_cache_stats(self):
        total = self.metrics["total_requests"]
        if total == 0:
            return "No requests yet"

        memory_hit_rate = (self.metrics["memory_hits"] / total) * 100
        redis_hit_rate = (self.metrics["redis_hits"] / total) * 100
        db_hit_rate = (self.metrics["db_hits"] / total) * 100

        return {
            "total_requests": total,
            "memory_hit_rate": f"{memory_hit_rate:.2f}%",
            "redis_hit_rate": f"{redis_hit_rate:.2f}%",
            "db_hit_rate": f"{db_hit_rate:.2f}%",
            "effective_hit_rate": f"{(memory_hit_rate + redis_hit_rate):.2f}%"
        }

# Usage
feature_store = MultiTierFeatureStore()

# Benchmark
async def benchmark():
    for i in range(10000):
        user_id = f"user_{i % 1000}"  # 1000 unique users (high cache hit expected)
        features = await feature_store.get_features(user_id)

    print(feature_store.get_cache_stats())

asyncio.run(benchmark())

# Expected output:
# {
#   "total_requests": 10000,
#   "memory_hit_rate": "90.00%",
#   "redis_hit_rate": "9.50%",
#   "db_hit_rate": "0.50%",
#   "effective_hit_rate": "99.50%"
# }
```

**Latency Breakdown**:
- Memory cache hit: <1ms
- Redis cache hit: 2-5ms
- Database hit: 20-50ms

**With 99.5% cache hit rate → average latency <2ms**

---

### Example 3: Database Sharding for User Features

**Scenario**: 100 million users, feature store needs to scale beyond single PostgreSQL instance.

**Sharding Strategy**: Hash-based sharding by `user_id`.

**Implementation**:

```python
import hashlib
from typing import Dict, List
import psycopg2

class ShardedFeatureStore:
    def __init__(self, shard_configs: List[Dict]):
        """
        shard_configs: [
            {"host": "db-shard-0", "port": 5432, "database": "features"},
            {"host": "db-shard-1", "port": 5432, "database": "features"},
            ...
        ]
        """
        self.num_shards = len(shard_configs)
        self.connections = {}

        # Create connection pool for each shard
        for i, config in enumerate(shard_configs):
            self.connections[i] = psycopg2.connect(
                host=config["host"],
                port=config["port"],
                database=config["database"],
                user=config.get("user", "postgres"),
                password=config.get("password", "")
            )

    def _get_shard_id(self, user_id: str) -> int:
        """
        Hash-based sharding: Consistent distribution across shards
        """
        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        shard_id = hash_value % self.num_shards
        return shard_id

    def get_features(self, user_id: str) -> dict:
        shard_id = self._get_shard_id(user_id)
        conn = self.connections[shard_id]

        with conn.cursor() as cursor:
            cursor.execute(
                "SELECT purchase_count_30d, avg_amount FROM user_features WHERE user_id = %s",
                (user_id,)
            )
            result = cursor.fetchone()

            if result:
                return {
                    "purchase_count_30d": result[0],
                    "avg_amount": result[1]
                }
        return {}

    def set_features(self, user_id: str, features: dict):
        shard_id = self._get_shard_id(user_id)
        conn = self.connections[shard_id]

        with conn.cursor() as cursor:
            cursor.execute(
                """
                INSERT INTO user_features (user_id, purchase_count_30d, avg_amount)
                VALUES (%s, %s, %s)
                ON CONFLICT (user_id) DO UPDATE SET
                    purchase_count_30d = EXCLUDED.purchase_count_30d,
                    avg_amount = EXCLUDED.avg_amount
                """,
                (user_id, features["purchase_count_30d"], features["avg_amount"])
            )
            conn.commit()

    def get_shard_stats(self):
        """
        Monitor shard distribution
        """
        stats = {}
        for shard_id, conn in self.connections.items():
            with conn.cursor() as cursor:
                cursor.execute("SELECT COUNT(*) FROM user_features")
                count = cursor.fetchone()[0]
                stats[f"shard_{shard_id}"] = count
        return stats

# Configuration for 10 shards
shard_configs = [
    {"host": f"db-shard-{i}.internal", "port": 5432, "database": "features"}
    for i in range(10)
]

feature_store = ShardedFeatureStore(shard_configs)

# Usage
features = feature_store.get_features("user_123")
feature_store.set_features("user_456", {"purchase_count_30d": 10, "avg_amount": 120.50})

# Monitor shard distribution
print(feature_store.get_shard_stats())
# {'shard_0': 10,123,456, 'shard_1': 9,987,234, ...}  # Should be roughly balanced
```

**Shard Rebalancing** (when adding new shards):

```python
def migrate_to_new_sharding(old_num_shards: int, new_num_shards: int):
    """
    Migrate data from old sharding scheme to new scheme
    """
    old_store = ShardedFeatureStore(old_shard_configs)
    new_store = ShardedFeatureStore(new_shard_configs)

    # For each user in old shards
    for old_shard_id in range(old_num_shards):
        conn = old_store.connections[old_shard_id]
        with conn.cursor() as cursor:
            cursor.execute("SELECT user_id, purchase_count_30d, avg_amount FROM user_features")
            for row in cursor.fetchall():
                user_id, purchase_count, avg_amount = row

                # Compute new shard
                new_shard_id = new_store._get_shard_id(user_id)

                # Insert into new shard
                new_store.set_features(user_id, {
                    "purchase_count_30d": purchase_count,
                    "avg_amount": avg_amount
                })

    print("Migration complete")
```

**Benefits**:
- 10 shards = 10x capacity vs single instance
- Linear scaling (add more shards to scale further)

**Challenges**:
- Cross-shard queries (e.g., COUNT(*) across all users) require fan-out
- Rebalancing when adding shards requires data migration

---

## 🔧 Code Examples

### Code Example 1: Implementing Graceful Shutdown

```python
import signal
import sys
import time
from fastapi import FastAPI

app = FastAPI()

# Track active requests
active_requests = 0
shutting_down = False

@app.middleware("http")
async def track_requests(request, call_next):
    global active_requests

    if shutting_down:
        return Response(status_code=503, content="Service is shutting down")

    active_requests += 1
    try:
        response = await call_next(request)
        return response
    finally:
        active_requests -= 1

@app.get("/predict")
async def predict():
    # Simulate slow request
    time.sleep(2)
    return {"prediction": 0.95}

def graceful_shutdown(signum, frame):
    global shutting_down
    print("Received shutdown signal, entering graceful shutdown...")
    shutting_down = True

    # Wait for active requests to complete (max 30 seconds)
    shutdown_timeout = 30
    elapsed = 0

    while active_requests > 0 and elapsed < shutdown_timeout:
        print(f"Waiting for {active_requests} active requests to complete...")
        time.sleep(1)
        elapsed += 1

    if active_requests > 0:
        print(f"Shutdown timeout reached, {active_requests} requests still active")
    else:
        print("All requests completed, shutting down gracefully")

    sys.exit(0)

# Register signal handlers
signal.signal(signal.SIGTERM, graceful_shutdown)
signal.signal(signal.SIGINT, graceful_shutdown)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Kubernetes Integration**:
```yaml
# deployment.yaml
spec:
  containers:
  - name: model-server
    lifecycle:
      preStop:
        exec:
          command: ["/bin/sh", "-c", "sleep 15"]  # Give time for graceful shutdown
    terminationGracePeriodSeconds: 30  # Max time before SIGKILL
```

---

### Code Example 2: Circuit Breaker with Fallback

```python
from enum import Enum
import time
from typing import Callable, Any

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60, success_threshold=2):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.success_threshold = success_threshold

        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    def call(self, func: Callable, fallback: Callable = None, *args, **kwargs) -> Any:
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time < self.timeout:
                # Circuit open, use fallback
                if fallback:
                    return fallback(*args, **kwargs)
                else:
                    raise Exception("Circuit breaker is OPEN, no fallback provided")
            else:
                # Timeout elapsed, try half-open
                self.state = CircuitState.HALF_OPEN

        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()

            # Use fallback if circuit opens
            if self.state == CircuitState.OPEN and fallback:
                return fallback(*args, **kwargs)
            raise e

    def _on_success(self):
        self.failure_count = 0

        if self.state == CircuitState.HALF_OPEN:
            self.success_count += 1
            if self.success_count >= self.success_threshold:
                self.state = CircuitState.CLOSED
                self.success_count = 0

    def _on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN

        if self.state == CircuitState.HALF_OPEN:
            self.state = CircuitState.OPEN
            self.success_count = 0

# Usage: Protect external model service calls with fallback
class RecommendationService:
    def __init__(self):
        self.circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=30)

    def get_recommendations(self, user_id: str):
        return self.circuit_breaker.call(
            self._get_personalized_recommendations,
            fallback=self._get_popular_items,
            user_id=user_id
        )

    def _get_personalized_recommendations(self, user_id: str):
        # Call external ML service (may fail)
        response = requests.post("http://ml-service/recommend", json={"user_id": user_id})
        response.raise_for_status()
        return response.json()["recommendations"]

    def _get_popular_items(self, user_id: str):
        # Fallback: Return popular items (doesn't require ML service)
        return ["item_1", "item_2", "item_3"]

# If ML service fails 3 times:
# - Circuit opens
# - Subsequent requests immediately return popular items (fallback)
# - After 30 seconds, circuit tries half-open (test if service recovered)
```

---

### Code Example 3: Blue-Green Deployment for Model Updates

```yaml
# Kubernetes Blue-Green Deployment

# Blue deployment (current production)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraud-detector-blue
  labels:
    app: fraud-detector
    version: blue
spec:
  replicas: 10
  selector:
    matchLabels:
      app: fraud-detector
      version: blue
  template:
    metadata:
      labels:
        app: fraud-detector
        version: blue
    spec:
      containers:
      - name: model-server
        image: fraud-detector:v1  # Old model version
        ports:
        - containerPort: 8000

---
# Green deployment (new version, not yet receiving traffic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraud-detector-green
  labels:
    app: fraud-detector
    version: green
spec:
  replicas: 10
  selector:
    matchLabels:
      app: fraud-detector
      version: green
  template:
    metadata:
      labels:
        app: fraud-detector
        version: green
    spec:
      containers:
      - name: model-server
        image: fraud-detector:v2  # New model version
        ports:
        - containerPort: 8000

---
# Service (currently points to blue)
apiVersion: v1
kind: Service
metadata:
  name: fraud-detector-service
spec:
  selector:
    app: fraud-detector
    version: blue  # Routes to blue deployment
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
```

**Deployment Process**:

```bash
# 1. Deploy green (new version) alongside blue
kubectl apply -f fraud-detector-green.yaml

# 2. Wait for green to be healthy
kubectl rollout status deployment/fraud-detector-green

# 3. Test green deployment directly
kubectl port-forward deployment/fraud-detector-green 8001:8000
curl http://localhost:8001/predict  # Verify new model works

# 4. Switch traffic to green (update service selector)
kubectl patch service fraud-detector-service -p '{"spec":{"selector":{"version":"green"}}}'

# 5. Monitor metrics (error rate, latency)
# If issues detected, instantly rollback:
kubectl patch service fraud-detector-service -p '{"spec":{"selector":{"version":"blue"}}}'

# 6. If green is stable, delete blue
kubectl delete deployment fraud-detector-blue
```

**Benefits**:
- Zero downtime deployment
- Instant rollback (just update service selector)
- Full testing of new version before switching traffic

---

## ✅ Best Practices

### 1. Design for Horizontal Scalability

**Principle**: Build stateless services that can be scaled horizontally.

**Guidelines**:
- Store state in external stores (Redis, database)
- Avoid in-memory session storage
- Use load balancers for traffic distribution

---

### 2. Set Realistic SLAs

**Principle**: Define and monitor Service Level Agreements.

**Example SLA**:
```yaml
model_serving_sla:
  availability: 99.9%  # 43.2 min downtime/month
  latency_p99: 100ms
  error_rate: < 0.1%
```

---

### 3. Implement Health Checks

**Principle**: Expose health endpoints for orchestration systems.

- **Liveness**: Is the service running?
- **Readiness**: Can it accept traffic?

---

### 4. Use Circuit Breakers

**Principle**: Fail fast when dependencies are unavailable.

---

### 5. Cache Aggressively

**Principle**: Use multi-tier caching for frequently accessed data.

---

### 6. Monitor Key Metrics

**Principle**: Track RED (Rate, Errors, Duration) and saturation metrics.

---

### 7. Test at Scale

**Principle**: Load test before production deployment.

**Tools**: Locust, k6, JMeter

---

### 8. Implement Graceful Degradation

**Principle**: Continue operating with reduced functionality when dependencies fail.

---

### 9. Plan for Disaster Recovery

**Principle**: Define RPO/RTO and test recovery procedures.

---

### 10. Auto-Scale Proactively

**Principle**: Use predictive scaling for expected load spikes.

---

## ⚠️ Common Pitfalls

### 1. Not Planning for Failures

**Pitfall**: Assuming services never fail.

**Solution**: Implement retries, circuit breakers, timeouts.

---

### 2. Over-Caching

**Pitfall**: Caching stale data leading to incorrect predictions.

**Solution**: Set appropriate TTL, implement cache invalidation.

---

### 3. Ignoring Cache Stampede

**Pitfall**: When cache expires, all requests hit database simultaneously.

**Solution**: Staggered expiration, refresh-ahead strategy.

---

### 4. Not Testing Auto-Scaling

**Pitfall**: Auto-scaling misconfigured, doesn't scale when needed.

**Solution**: Load test to verify scaling behavior.

---

### 5. Synchronous Database Replication Everywhere

**Pitfall**: Using synchronous replication for all data = slow writes.

**Solution**: Use asynchronous replication for non-critical data.

---

### 6. Poor Shard Key Selection

**Pitfall**: Uneven shard distribution (hot shards).

**Solution**: Use hash-based sharding for even distribution.

---

### 7. No Rate Limiting

**Pitfall**: Single user can overload system.

**Solution**: Implement per-user rate limiting.

---

### 8. Not Monitoring Cache Hit Rate

**Pitfall**: Low cache hit rate → high database load.

**Solution**: Monitor and optimize cache hit rate (target >90%).

---

### 9. Ignoring Graceful Shutdown

**Pitfall**: Killing pods abruptly drops active requests.

**Solution**: Implement graceful shutdown (wait for active requests).

---

### 10. No Disaster Recovery Testing

**Pitfall**: DR plan fails when actually needed.

**Solution**: Run DR drills quarterly.

---

## 🏋️ Hands-On Exercises

### Exercise 1: Implement Multi-Tier Caching (Intermediate, 8-10 hours)

**Objective**: Build a feature store with in-memory + Redis + database caching.

**Tasks**:
1. Implement three-tier cache (memory, Redis, PostgreSQL)
2. Measure cache hit rate at each tier
3. Benchmark latency (aim for <5ms p99 with >95% cache hit)
4. Implement cache warming (preload popular users)

**Validation**:
- Cache hit rate >95%
- p99 latency <5ms
- Handle cache invalidation correctly

---

### Exercise 2: Configure Auto-Scaling (Intermediate, 6-8 hours)

**Objective**: Set up Kubernetes HPA for model serving.

**Tasks**:
1. Deploy model server to Kubernetes
2. Configure HPA with CPU and custom metrics
3. Load test to trigger scaling (100 → 1000 req/sec)
4. Monitor scaling behavior

**Validation**:
- HPA scales up when load increases
- HPA scales down after load decreases
- Latency remains <100ms during scaling

---

### Exercise 3: Database Sharding (Advanced, 12-15 hours)

**Objective**: Implement hash-based sharding for user features.

**Tasks**:
1. Set up 5 PostgreSQL instances (shards)
2. Implement sharding logic (hash user_id)
3. Load 1M user records across shards
4. Verify even distribution
5. Implement cross-shard query (COUNT(*) across all users)

**Validation**:
- Shards roughly balanced (±5%)
- Query performance scales linearly

---

### Exercise 4: Chaos Engineering (Advanced, 10-12 hours)

**Objective**: Test fault tolerance by injecting failures.

**Tasks**:
1. Deploy ML system (feature service + model service)
2. Inject failures:
   - Kill random pods
   - Introduce network latency
   - Make database unavailable
3. Verify system resilience:
   - Auto-recovery
   - Circuit breakers activate
   - Graceful degradation

**Tools**: Chaos Mesh, Gremlin

**Validation**:
- System remains available (>99%)
- Errors logged but users not affected
- Auto-recovery within SLA

---

## 🔗 Related Concepts

### Within This Course:
- [[01. Architectural Patterns for ML Systems]] - Overall system architecture
- [[04. Microservices and Event-Driven Patterns]] - Distributed system patterns
- [[03. Batch vs Streaming Considerations]] - Scaling batch vs streaming

### External Connections:
- **Kubernetes**: [[Kubernetes Autoscaling]], [[Pod Disruption Budgets]]
- **Caching**: [[Redis Best Practices]], [[CDN Strategies]]
- **Databases**: [[PostgreSQL Replication]], [[Database Sharding]]
- **Monitoring**: [[Prometheus]], [[Grafana Dashboards]]

---

## 📚 Further Reading

### Essential Books:

1. **"Designing Data-Intensive Applications" by Martin Kleppmann (O'Reilly, 2017)**
   - Chapters on replication, partitioning, and fault tolerance
   - Fundamental distributed systems concepts

2. **"Site Reliability Engineering" by Google (O'Reilly, 2016)**
   - SLIs, SLOs, error budgets
   - Production best practices at Google scale

3. **"The Art of Scalability" by Martin L. Abbott & Michael T. Fisher (Addison-Wesley, 2015)**
   - Scalability patterns and principles
   - Organizational scaling

4. **"Release It!, 2nd Edition" by Michael Nygard (Pragmatic Bookshelf, 2018)**
   - Stability patterns (circuit breakers, bulkheads, timeouts)
   - Production-ready systems

### Key Papers:

1. **"The Google File System" (Google, 2003)**
   - Scalable distributed file system

2. **"Dynamo: Amazon's Highly Available Key-value Store" (Amazon, 2007)**
   - Eventual consistency, consistent hashing

### Blogs:

1. **AWS Architecture Blog - "Auto Scaling Best Practices"**
2. **Netflix Tech Blog - "Chaos Engineering"**
3. **Uber Engineering - "Scalable ML Infrastructure"**

### Documentation:

1. **Kubernetes Documentation - Horizontal Pod Autoscaling**
   - https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/

2. **Redis Documentation - Partitioning**
   - https://redis.io/topics/partitioning

3. **PostgreSQL Documentation - High Availability**
   - https://www.postgresql.org/docs/current/high-availability.html

---

## 📝 Key Takeaways

1. **Horizontal scaling enables near-unlimited capacity**: Add more machines to scale ML serving, feature stores, and training infrastructure.

2. **Multi-tier caching reduces latency to <5ms**: Combine in-memory, Redis, and database caching for sub-millisecond feature serving.

3. **Auto-scaling optimizes cost and performance**: Kubernetes HPA dynamically adjusts replicas based on CPU, memory, and custom metrics.

4. **Circuit breakers prevent cascading failures**: Fail fast when dependencies are unavailable to maintain system stability.

5. **Load balancing distributes traffic efficiently**: Use consistent hashing for cache affinity, round-robin for even distribution.

6. **Database sharding scales beyond single instance limits**: Hash-based sharding distributes 100M+ users across multiple PostgreSQL instances.

7. **Graceful degradation maintains availability**: Continue operating with reduced functionality (e.g., popular items) when ML service fails.

8. **Rate limiting prevents system overload**: Implement token bucket algorithm to limit requests per user/API key.

9. **Disaster recovery requires planning and testing**: Define RPO/RTO, implement automated failover, and run DR drills quarterly.

10. **Monitoring is essential for scalability**: Track RED metrics (Rate, Errors, Duration) and saturation to identify bottlenecks early.

---

## ✏️ Notes Section

**Personal Insights**:

**Questions to Explore**:

**Related Projects**:

---

*Last updated: 2025-10-19*
*Part of: [[DEforAI - Data Engineering for AI/ML]]*
*Chapter: [[03. Data Architecture for ML Systems]]*
