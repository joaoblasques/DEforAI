# 03. Batch vs Streaming Considerations

**Date:** 2025-10-19
**Status:** #research
**Tags:** #data-engineering #ml-systems #batch-processing #streaming #architecture #performance #latency

---

## 📋 Overview

Choosing between batch and streaming processing architectures is one of the most critical architectural decisions in ML systems. This choice fundamentally impacts system complexity, latency, cost, and operational overhead.

**Batch processing** excels at processing large volumes of data efficiently with high throughput, making it ideal for model training, historical analysis, and use cases where latency is measured in hours or days. **Streaming processing** enables real-time or near-real-time decision-making with latencies in milliseconds to seconds, essential for fraud detection, recommendation systems, and dynamic pricing.

Modern ML systems increasingly adopt **hybrid architectures** that combine batch and streaming processing to balance cost-efficiency with low-latency requirements. Understanding the trade-offs between these approaches enables data engineers to design systems that meet business requirements while maintaining operational sustainability.

**Key Dimensions**:
- **Latency**: Batch (hours-days) vs Streaming (milliseconds-seconds)
- **Throughput**: Batch optimizes for volume, streaming for velocity
- **Complexity**: Batch is simpler, streaming requires sophisticated orchestration
- **Cost**: Batch leverages economies of scale, streaming incurs continuous resource costs
- **State Management**: Streaming requires complex windowing and state handling

---

## 🎯 Learning Objectives

By the end of this subchapter, you will:

1. **Understand the fundamental trade-offs** between batch and streaming processing for ML systems
2. **Evaluate latency requirements** and map them to appropriate processing paradigms
3. **Design hybrid architectures** that combine batch and streaming where appropriate
4. **Calculate cost models** for batch vs streaming infrastructure
5. **Implement streaming windowing operations** for real-time feature computation
6. **Handle late-arriving data** in streaming pipelines with watermarks and triggers
7. **Optimize batch jobs** for throughput and resource utilization
8. **Select appropriate technologies** (Spark, Flink, Kafka, Beam) based on use case requirements
9. **Manage state** in stateful streaming applications for ML
10. **Test and validate** streaming pipelines for correctness and performance

---

## 📚 Core Concepts

### 1. Batch Processing Fundamentals

**Definition**: Processing accumulated data in discrete, scheduled jobs (hourly, daily, weekly).

**Characteristics**:
- **High Throughput**: Optimized for processing large volumes efficiently
- **Resource Efficiency**: Leverages cluster resources fully during execution windows
- **Simplicity**: Easier to reason about, debug, and maintain
- **Deterministic**: Rerunning the same input produces the same output
- **Complete Data View**: Access to full dataset enables complex aggregations

**When to Use Batch**:
- Model training (retraining weekly/daily with historical data)
- Batch feature computation (computing features overnight for next-day serving)
- Historical analysis and reporting
- Data quality checks and validation
- ETL pipelines with SLA measured in hours

**Example Technologies**: Apache Spark (batch mode), Apache Beam (batch runner), dbt, Airflow

---

### 2. Streaming Processing Fundamentals

**Definition**: Continuously processing data as it arrives, often with subsecond latency.

**Characteristics**:
- **Low Latency**: Results available within milliseconds to seconds
- **Continuous Computation**: Always-on processing of infinite data streams
- **Event-Driven**: Reacts to events as they occur
- **State Management**: Maintains state across events for aggregations
- **Complexity**: Requires handling out-of-order data, late arrivals, exactly-once semantics

**When to Use Streaming**:
- Real-time fraud detection (transaction scoring in <100ms)
- Online feature computation (real-time user embeddings)
- Anomaly detection (detecting unusual patterns immediately)
- Real-time recommendations (personalized content based on current session)
- Monitoring and alerting (drift detection, SLA violations)

**Example Technologies**: Apache Flink, Apache Kafka Streams, Apache Beam (streaming runner), AWS Kinesis Data Analytics

---

### 3. Latency Requirements Spectrum

Understanding latency requirements helps select the right architecture:

| **Use Case** | **Latency Requirement** | **Architecture** | **Example** |
|-------------|------------------------|------------------|-------------|
| Model Retraining | Days to weeks | Batch | Weekly retrain on historical data |
| Daily Feature Updates | Hours | Batch | Nightly feature computation |
| Hourly Aggregations | Minutes to hours | Micro-batch | Hourly user activity features |
| Real-time Features | Seconds | Streaming | Session-based features |
| Online Inference | <100ms | Streaming + Precomputed | Fraud detection scoring |
| Ultra-low Latency | <10ms | In-memory + Cached | High-frequency trading ML |

**Latency vs Complexity Trade-off**: Lower latency requirements exponentially increase system complexity and operational costs.

---

### 4. Throughput vs Latency Trade-offs

**Batch Optimization** (maximize throughput):
- Process large chunks of data together
- Leverage parallelism across nodes
- Use columnar formats (Parquet, ORC) for I/O efficiency
- Trade latency for cost efficiency

**Streaming Optimization** (minimize latency):
- Process records individually or in micro-batches
- Maintain continuous pipeline execution
- Use in-memory state stores (RocksDB, Redis)
- Trade cost for responsiveness

**Example**: Processing 1 billion records
- **Batch**: 1 billion records in 1 hour = 277K records/sec throughput, 1-hour latency
- **Streaming**: Continuous processing at 10K records/sec = real-time latency, higher cost

---

### 5. Windowing in Streaming Systems

**Windowing** divides infinite streams into finite chunks for aggregation.

**Window Types**:

1. **Tumbling Windows**: Fixed-size, non-overlapping
   - Use case: Hourly aggregations (count transactions per hour)
   - Example: [0:00-1:00), [1:00-2:00), [2:00-3:00)

2. **Sliding Windows**: Fixed-size, overlapping
   - Use case: Moving averages (average over last 5 minutes, updated every minute)
   - Example: Window size = 5 min, slide = 1 min

3. **Session Windows**: Dynamic size based on inactivity gap
   - Use case: User session analysis (group events until 30 min of inactivity)
   - Example: User clicks until 30-min gap triggers session close

4. **Global Windows**: All data in one window (requires custom triggers)
   - Use case: Stateful processing where window boundaries are application-defined

**Watermarks**: Mechanism to track event-time progress in streaming systems
- **Purpose**: Determine when to close a window and emit results
- **Trade-off**: Late watermarks = more complete results but higher latency

---

### 6. State Management in Streaming

**Stateful Operations** require maintaining state across events:
- Aggregations (count, sum, average)
- Joins between streams
- Machine learning model serving with context
- Sessionization

**State Backend Options**:
1. **In-Memory** (Flink MemoryStateBackend): Fast but limited by memory
2. **RocksDB** (Flink RocksDBStateBackend): Disk-backed, scales to large state
3. **External State Store** (Redis, DynamoDB): Shared state across instances

**State Size Challenges**:
- Large state (>100GB) impacts checkpoint performance
- State recovery after failure increases downtime
- State growth over time requires compaction strategies

---

### 7. Cost Models

**Batch Processing Costs**:
- **Infrastructure**: Pay for cluster time during job execution (elastic scaling)
- **Storage**: Data lakes (S3, GCS) are cheap (~$0.023/GB/month)
- **Compute**: Spot/preemptible instances reduce costs by 70-90%
- **Example**: Daily 4-hour Spark job on 20 nodes = 80 node-hours/day

**Streaming Processing Costs**:
- **Infrastructure**: 24/7 running clusters (less elastic)
- **Kafka/Kinesis**: ~$0.015/hour per shard + data transfer costs
- **Flink Cluster**: Continuous TaskManager + JobManager resource usage
- **State Storage**: RocksDB state stored on disk, S3 for checkpoints
- **Example**: 3-node Flink cluster running 24/7 = 2,160 node-hours/month

**Cost Comparison** (1 TB/day processing):
- Batch (4 hours/day on 10 nodes): ~$300-500/month
- Streaming (3 nodes 24/7): ~$1,500-2,500/month

**Cost Optimization Strategies**:
- Use batch for non-latency-sensitive workloads
- Implement tiered architectures (hot/warm/cold data)
- Leverage auto-scaling for variable load patterns
- Consider managed services (Databricks, AWS Kinesis) for operational efficiency

---

### 8. Complexity Considerations

**Batch Complexity**:
- **Simpler Mental Model**: Finite input → process → finite output
- **Easier Debugging**: Rerun job with same input to reproduce issues
- **Testing**: Unit test transformations with sample datasets
- **Orchestration**: Airflow DAGs are straightforward

**Streaming Complexity**:
- **Infinite Data Model**: Requires thinking about unbounded streams
- **Out-of-Order Data**: Events arrive late, requiring watermarking
- **Exactly-Once Semantics**: Complex coordination (Kafka transactions, Flink checkpoints)
- **State Management**: Maintaining and recovering distributed state
- **Testing**: Harder to test time-based logic and windowing

**Operational Complexity**:
- Batch: Retry failed jobs, debug with logs
- Streaming: Monitor lag, handle backpressure, manage state size, recover from failures

---

### 9. Hybrid Architectures

**Combining Batch and Streaming**:

**Pattern 1: Speed Layer + Batch Layer (Lambda Architecture)**
- Batch: Compute comprehensive features overnight
- Streaming: Compute incremental updates in real-time
- Serving: Merge both layers at query time

**Pattern 2: Batch Training + Streaming Inference**
- Batch: Train models on historical data (weekly)
- Streaming: Serve predictions with real-time features

**Pattern 3: Streaming Aggregation + Batch Enrichment**
- Streaming: Real-time aggregations (click counts)
- Batch: Enrich with slow-changing dimensions (user demographics)

**Pattern 4: Near-Real-Time with Micro-Batching**
- Process small batches every 5-15 minutes
- Balances latency and throughput
- Example: Spark Structured Streaming with trigger intervals

---

### 10. Technology Selection Matrix

| **Requirement** | **Batch Technologies** | **Streaming Technologies** |
|----------------|------------------------|---------------------------|
| Large-scale batch training | Apache Spark, Databricks | N/A |
| Real-time aggregation | N/A | Apache Flink, Kafka Streams |
| Sub-second latency | N/A | Apache Flink, Kinesis Analytics |
| SQL interface | dbt, Spark SQL | Flink SQL, ksqlDB |
| Unified batch + streaming | Apache Beam, Spark Structured Streaming | Apache Beam, Spark Structured Streaming |
| Managed service | AWS Glue, Databricks | AWS Kinesis, GCP Dataflow |
| Complex stateful logic | Spark (limited) | Flink (excellent state management) |
| Exactly-once guarantees | Spark (batch mode) | Flink, Kafka Streams |

**Apache Flink vs Kafka Streams**:
- **Flink**: Better for complex event processing, advanced windowing, large state
- **Kafka Streams**: Simpler deployment (library, not cluster), good for Kafka-centric architectures

**Spark vs Flink**:
- **Spark**: Better batch performance, micro-batching streaming, unified API
- **Flink**: True streaming, better latency, superior state management

---

## 💡 Practical Examples

### Example 1: Fraud Detection - Hybrid Batch + Streaming

**Scenario**: Credit card fraud detection requiring <100ms inference latency

**Batch Layer** (Daily):
```python
# Compute historical user features overnight (Spark batch job)
def compute_user_historical_features(transactions_df, date):
    """
    Batch compute features like:
    - Average transaction amount (30/90 days)
    - Transaction count by merchant category
    - Geographic diversity score
    """
    user_features = transactions_df.filter(
        (F.col("date") >= F.date_sub(F.lit(date), 90))
    ).groupBy("user_id").agg(
        F.avg("amount").alias("avg_amount_90d"),
        F.count("*").alias("transaction_count_90d"),
        F.countDistinct("merchant_category").alias("merchant_diversity"),
        F.countDistinct("city").alias("geo_diversity_90d"),
        F.stddev("amount").alias("amount_stddev_90d")
    )

    # Write to feature store (batch table)
    user_features.write.format("delta").mode("overwrite") \
        .partitionBy("date") \
        .save("s3://features/user_historical/")

    return user_features
```

**Streaming Layer** (Real-time):
```python
# Apache Flink streaming job for real-time features
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, EnvironmentSettings

env = StreamExecutionEnvironment.get_execution_environment()
settings = EnvironmentSettings.new_instance().in_streaming_mode().build()
table_env = StreamTableEnvironment.create(env, environment_settings=settings)

# Define Kafka source for transaction stream
table_env.execute_sql("""
    CREATE TABLE transactions (
        user_id STRING,
        amount DOUBLE,
        merchant_category STRING,
        city STRING,
        event_time TIMESTAMP(3),
        WATERMARK FOR event_time AS event_time - INTERVAL '30' SECOND
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'transactions',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# Compute real-time features (last 1 hour, updated every 1 min)
table_env.execute_sql("""
    CREATE TABLE user_realtime_features AS
    SELECT
        user_id,
        HOP_START(event_time, INTERVAL '1' MINUTE, INTERVAL '1' HOUR) AS window_start,
        COUNT(*) AS txn_count_1h,
        AVG(amount) AS avg_amount_1h,
        MAX(amount) AS max_amount_1h,
        COUNT(DISTINCT merchant_category) AS merchant_diversity_1h
    FROM transactions
    GROUP BY user_id, HOP(event_time, INTERVAL '1' MINUTE, INTERVAL '1' HOUR)
""")

# Sink to Redis for low-latency serving
table_env.execute_sql("""
    CREATE TABLE redis_sink (
        user_id STRING,
        window_start TIMESTAMP(3),
        txn_count_1h BIGINT,
        avg_amount_1h DOUBLE,
        max_amount_1h DOUBLE,
        merchant_diversity_1h BIGINT,
        PRIMARY KEY (user_id) NOT ENFORCED
    ) WITH (
        'connector' = 'redis',
        'host' = 'localhost',
        'port' = '6379',
        'key-pattern' = 'fraud:user:${user_id}'
    )
""")

table_env.execute_sql("""
    INSERT INTO redis_sink
    SELECT * FROM user_realtime_features
""")
```

**Serving Layer** (Merge at inference):
```python
import redis
from delta import DeltaTable

class FraudFeatureService:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379)
        self.delta_table = DeltaTable.forPath(spark, "s3://features/user_historical/")

    def get_features(self, user_id: str, current_date: str):
        # Get batch features (historical, computed daily)
        batch_features = self.delta_table.toDF().filter(
            (F.col("user_id") == user_id) & (F.col("date") == current_date)
        ).collect()[0].asDict()

        # Get streaming features (real-time, updated every minute)
        realtime_key = f"fraud:user:{user_id}"
        realtime_features = self.redis_client.hgetall(realtime_key)
        realtime_features = {k.decode(): float(v.decode()) for k, v in realtime_features.items()}

        # Merge features
        merged = {**batch_features, **realtime_features}

        return merged

# Usage in FastAPI inference endpoint
@app.post("/predict/fraud")
async def predict_fraud(user_id: str, transaction_data: dict):
    start = time.time()

    features = feature_service.get_features(user_id, current_date=today)
    features.update(transaction_data)  # Add current transaction details

    fraud_score = fraud_model.predict_proba([features])[0, 1]

    latency_ms = (time.time() - start) * 1000
    logger.info(f"Fraud prediction latency: {latency_ms:.2f}ms")

    return {"fraud_score": fraud_score, "latency_ms": latency_ms}
```

**Why Hybrid?**:
- Batch provides rich historical context (90-day patterns) at low cost
- Streaming provides immediate behavioral signals (last hour activity)
- Serving layer merges both for comprehensive fraud detection

---

### Example 2: Recommendation System - Streaming Session Features

**Scenario**: E-commerce recommendations based on current browsing session

**Streaming Processing** (Flink Session Windows):
```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.window import SessionWindowTimeGapExtractor
from pyflink.common import Types

env = StreamExecutionEnvironment.get_execution_environment()

# Define click stream
click_stream = env.from_source(
    kafka_source,  # Kafka source configuration
    WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(30)),
    "click-events"
)

# Session window: Group clicks until 30 minutes of inactivity
def compute_session_features(click_events):
    return click_events \
        .key_by(lambda event: event['user_id']) \
        .window(SessionWindows.with_gap(Duration.of_minutes(30))) \
        .process(SessionFeatureProcessor())

class SessionFeatureProcessor(ProcessWindowFunction):
    def process(self, key, context, elements):
        clicks = list(elements)

        # Compute session-level features
        session_features = {
            'user_id': key,
            'session_start': min(c['timestamp'] for c in clicks),
            'session_end': max(c['timestamp'] for c in clicks),
            'session_duration_sec': (max(c['timestamp'] for c in clicks) -
                                      min(c['timestamp'] for c in clicks)).total_seconds(),
            'total_clicks': len(clicks),
            'unique_products_viewed': len(set(c['product_id'] for c in clicks)),
            'categories_browsed': list(set(c['category'] for c in clicks)),
            'add_to_cart_count': sum(1 for c in clicks if c['event_type'] == 'add_to_cart'),
            'avg_time_per_product': (max(c['timestamp'] for c in clicks) -
                                      min(c['timestamp'] for c in clicks)).total_seconds() / len(set(c['product_id'] for c in clicks))
        }

        # Write to Redis for real-time serving
        yield session_features

# Execute streaming job
session_features_stream = compute_session_features(click_stream)

# Sink to Redis
session_features_stream.add_sink(
    RedisSink(
        redis_config={'host': 'localhost', 'port': 6379},
        key_pattern='session:{user_id}'
    )
)

env.execute("Session Feature Computation")
```

**Serving** (Real-time recommendations):
```python
@app.get("/recommendations/{user_id}")
async def get_recommendations(user_id: str):
    # Get current session features from Redis (computed in real-time by Flink)
    session_key = f"session:{user_id}"
    session_features = redis_client.hgetall(session_key)

    if not session_features:
        # No active session, fall back to batch user profile
        return get_batch_recommendations(user_id)

    # Use session features for context-aware recommendations
    categories_browsed = session_features.get('categories_browsed', [])
    unique_products_viewed = int(session_features.get('unique_products_viewed', 0))

    # Real-time recommendation logic
    if unique_products_viewed > 5 and session_features.get('add_to_cart_count', 0) == 0:
        # User is browsing extensively but not adding to cart → recommend popular items in browsed categories
        recommendations = recommend_popular_in_categories(categories_browsed)
    else:
        # Standard collaborative filtering
        recommendations = recommend_collaborative(user_id, session_features)

    return {"recommendations": recommendations, "session_based": True}
```

**Why Streaming?**:
- Session features capture immediate user intent
- Low latency (<1s) enables dynamic recommendation updates
- Session windows naturally align with user behavior patterns

---

### Example 3: Batch Model Training with Streaming Feature Updates

**Scenario**: Weekly model retraining + daily streaming feature refresh

**Batch Training** (Spark):
```python
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier

def train_model_weekly():
    """
    Train model every Sunday on full historical dataset
    """
    # Load last 6 months of training data
    training_data = spark.read.format("delta") \
        .load("s3://ml-data/training/") \
        .filter(F.col("date") >= F.date_sub(F.current_date(), 180))

    # Feature engineering (batch computation)
    features = training_data.groupBy("user_id").agg(
        F.avg("purchase_amount").alias("avg_purchase"),
        F.count("*").alias("purchase_count"),
        F.max("purchase_amount").alias("max_purchase"),
        F.datediff(F.current_date(), F.max("purchase_date")).alias("days_since_purchase")
    )

    # Join with labels
    train_df = features.join(labels, "user_id")

    # Train model
    rf = RandomForestClassifier(numTrees=100, featuresCol="features", labelCol="will_churn")
    model = rf.fit(train_df)

    # Save model
    model.write().overwrite().save("s3://ml-models/churn-predictor/")

    # Log to MLflow
    mlflow.log_model(model, "churn-predictor")
    mlflow.log_metric("training_samples", train_df.count())

# Schedule with Airflow
train_model_dag = DAG('weekly_model_training', schedule_interval='@weekly')
```

**Streaming Feature Updates** (Flink):
```python
# Update features daily as new purchases stream in
def update_features_streaming():
    """
    Incrementally update user features as purchases arrive
    """
    table_env.execute_sql("""
        CREATE TABLE purchase_stream (
            user_id STRING,
            purchase_amount DOUBLE,
            purchase_date TIMESTAMP(3),
            WATERMARK FOR purchase_date AS purchase_date - INTERVAL '1' HOUR
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'purchases',
            'properties.bootstrap.servers' = 'localhost:9092'
        )
    """)

    # Incremental aggregation (running totals)
    table_env.execute_sql("""
        CREATE TABLE user_features_incremental AS
        SELECT
            user_id,
            COUNT(*) AS purchase_count,
            AVG(purchase_amount) AS avg_purchase,
            MAX(purchase_amount) AS max_purchase,
            MAX(purchase_date) AS last_purchase_date
        FROM purchase_stream
        GROUP BY user_id
    """)

    # Upsert to Delta Lake (merge new aggregations with existing)
    table_env.execute_sql("""
        INSERT INTO delta_features
        SELECT * FROM user_features_incremental
        ON DUPLICATE KEY UPDATE
            purchase_count = user_features_incremental.purchase_count,
            avg_purchase = user_features_incremental.avg_purchase
    """)
```

**Benefits**:
- Weekly batch training processes full historical data for model quality
- Streaming feature updates keep user profiles fresh daily
- Reduces training frequency (cost savings) while maintaining feature freshness

---

## 🔧 Code Examples

### Code Example 1: Windowing Operations in Flink

```python
from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic
from pyflink.table import StreamTableEnvironment, EnvironmentSettings
from pyflink.table.window import Tumble, Slide, Session

env = StreamExecutionEnvironment.get_execution_environment()
env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
settings = EnvironmentSettings.new_instance().in_streaming_mode().build()
t_env = StreamTableEnvironment.create(env, environment_settings=settings)

# Create source table with event time
t_env.execute_sql("""
    CREATE TABLE sensor_readings (
        sensor_id STRING,
        temperature DOUBLE,
        humidity DOUBLE,
        event_time TIMESTAMP(3),
        WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'sensor-data',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json',
        'scan.startup.mode' = 'latest-offset'
    )
""")

# 1. Tumbling Window (5-minute non-overlapping windows)
tumbling_agg = t_env.sql_query("""
    SELECT
        sensor_id,
        TUMBLE_START(event_time, INTERVAL '5' MINUTE) AS window_start,
        TUMBLE_END(event_time, INTERVAL '5' MINUTE) AS window_end,
        AVG(temperature) AS avg_temp,
        MAX(temperature) AS max_temp,
        MIN(temperature) AS min_temp,
        COUNT(*) AS reading_count
    FROM sensor_readings
    GROUP BY sensor_id, TUMBLE(event_time, INTERVAL '5' MINUTE)
""")

# 2. Sliding Window (10-minute window, sliding every 2 minutes)
sliding_agg = t_env.sql_query("""
    SELECT
        sensor_id,
        HOP_START(event_time, INTERVAL '2' MINUTE, INTERVAL '10' MINUTE) AS window_start,
        HOP_END(event_time, INTERVAL '2' MINUTE, INTERVAL '10' MINUTE) AS window_end,
        AVG(temperature) AS avg_temp_10min
    FROM sensor_readings
    GROUP BY sensor_id, HOP(event_time, INTERVAL '2' MINUTE, INTERVAL '10' MINUTE)
""")

# 3. Session Window (dynamic windows based on 30-minute inactivity gap)
session_agg = t_env.sql_query("""
    SELECT
        sensor_id,
        SESSION_START(event_time, INTERVAL '30' MINUTE) AS session_start,
        SESSION_END(event_time, INTERVAL '30' MINUTE) AS session_end,
        COUNT(*) AS readings_in_session,
        AVG(temperature) AS avg_temp_session
    FROM sensor_readings
    GROUP BY sensor_id, SESSION(event_time, INTERVAL '30' MINUTE)
""")

# Sink results to separate Kafka topics
t_env.execute_sql("""
    CREATE TABLE tumbling_sink (
        sensor_id STRING,
        window_start TIMESTAMP(3),
        window_end TIMESTAMP(3),
        avg_temp DOUBLE,
        max_temp DOUBLE,
        min_temp DOUBLE,
        reading_count BIGINT
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'tumbling-aggregates',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

tumbling_agg.execute_insert("tumbling_sink")
```

**Explanation**:
- **Watermarks** (`WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND`): Flink waits up to 10 seconds for late events before closing a window
- **Tumbling**: Use for periodic snapshots (e.g., hourly metrics)
- **Sliding**: Use for moving averages (e.g., "average over last 10 minutes, updated every 2 minutes")
- **Session**: Use for user behavior analysis (e.g., group clicks until 30-min inactivity)

---

### Code Example 2: Batch Processing with Spark (Optimized)

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Initialize Spark with optimized configurations
spark = SparkSession.builder \
    .appName("Batch Feature Engineering") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.files.maxPartitionBytes", "128MB") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.dynamicAllocation.enabled", "true") \
    .getOrCreate()

def compute_user_features_batch(date: str):
    """
    Batch compute user features with optimizations:
    - Broadcast join for small dimension tables
    - Partition pruning for date-partitioned data
    - Caching intermediate results
    - Adaptive query execution
    """

    # Read transactions (partitioned by date for efficient pruning)
    transactions = spark.read.format("delta") \
        .load("s3://data/transactions/") \
        .filter(
            (F.col("date") >= F.date_sub(F.lit(date), 90)) &
            (F.col("date") <= F.lit(date))
        )

    # Read user dimension (small table, broadcast join)
    users = spark.read.format("delta").load("s3://data/users/")
    users_broadcast = F.broadcast(users)  # Broadcast hint for small table

    # Cache transactions since we'll use it multiple times
    transactions.cache()

    # 1. Time-based aggregations with window functions
    user_time_features = transactions.groupBy("user_id").agg(
        # Last 30 days
        F.sum(F.when(F.col("date") >= F.date_sub(F.lit(date), 30), F.col("amount")).otherwise(0)).alias("amount_30d"),
        F.count(F.when(F.col("date") >= F.date_sub(F.lit(date), 30), 1)).alias("txn_count_30d"),

        # Last 90 days
        F.sum(F.col("amount")).alias("amount_90d"),
        F.count("*").alias("txn_count_90d"),
        F.avg("amount").alias("avg_amount_90d"),
        F.stddev("amount").alias("stddev_amount_90d"),

        # Recency
        F.datediff(F.lit(date), F.max("date")).alias("days_since_last_txn"),

        # Diversity metrics
        F.countDistinct("merchant_id").alias("merchant_diversity"),
        F.countDistinct("category").alias("category_diversity")
    )

    # 2. Trend features (comparing recent vs historical behavior)
    user_trend_features = transactions.groupBy("user_id").agg(
        (F.sum(F.when(F.col("date") >= F.date_sub(F.lit(date), 30), F.col("amount")).otherwise(0)) /
         (F.sum(F.col("amount")) / 3)).alias("spending_trend_ratio")  # Last 30d vs avg monthly over 90d
    )

    # 3. Sequence features (using window functions)
    window_spec = Window.partitionBy("user_id").orderBy(F.col("date").desc())

    sequence_features = transactions.withColumn("rank", F.row_number().over(window_spec)) \
        .filter(F.col("rank") <= 5) \
        .groupBy("user_id").agg(
            F.collect_list("amount").alias("last_5_amounts"),
            F.collect_list("category").alias("last_5_categories")
        )

    # 4. Join all feature sets
    user_features = user_time_features \
        .join(user_trend_features, "user_id", "left") \
        .join(sequence_features, "user_id", "left") \
        .join(users_broadcast, "user_id", "left")  # Broadcast join

    # Add metadata
    user_features = user_features.withColumn("feature_date", F.lit(date)) \
        .withColumn("created_at", F.current_timestamp())

    # Write to Delta Lake with optimizations
    user_features.write.format("delta") \
        .mode("overwrite") \
        .partitionBy("feature_date") \
        .option("overwriteSchema", "true") \
        .option("optimizeWrite", "true") \
        .option("autoCompact", "true") \
        .save("s3://features/user-features/")

    # Unpersist cache
    transactions.unpersist()

    print(f"Computed features for {user_features.count()} users on {date}")

# Execute batch job
compute_user_features_batch("2025-10-19")
```

**Optimizations**:
- **Adaptive Query Execution**: Spark dynamically optimizes query plan based on runtime statistics
- **Broadcast Join**: Small dimension tables (<10MB) are broadcast to all executors
- **Partition Pruning**: Only read relevant date partitions
- **Caching**: Reuse intermediate DataFrames
- **Dynamic Allocation**: Cluster scales based on workload

---

### Code Example 3: Handling Late Data with Watermarks

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
from pyflink.table.expressions import col, lit
from datetime import datetime, timedelta

env = StreamExecutionEnvironment.get_execution_environment()
env.enable_checkpointing(60000)  # Checkpoint every 60 seconds
t_env = StreamTableEnvironment.create(env)

# Define table with watermark strategy
t_env.execute_sql("""
    CREATE TABLE events (
        user_id STRING,
        event_type STRING,
        amount DOUBLE,
        event_time TIMESTAMP(3),
        processing_time AS PROCTIME(),
        -- Watermark: Allow up to 1 minute of lateness
        WATERMARK FOR event_time AS event_time - INTERVAL '1' MINUTE
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'events',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json',
        'json.timestamp-format.standard' = 'ISO-8601'
    )
""")

# Windowed aggregation with late data handling
t_env.execute_sql("""
    CREATE TABLE event_aggregates (
        user_id STRING,
        window_start TIMESTAMP(3),
        window_end TIMESTAMP(3),
        event_count BIGINT,
        total_amount DOUBLE
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'event-aggregates',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# Compute aggregates with tumbling window
t_env.execute_sql("""
    INSERT INTO event_aggregates
    SELECT
        user_id,
        TUMBLE_START(event_time, INTERVAL '5' MINUTE) AS window_start,
        TUMBLE_END(event_time, INTERVAL '5' MINUTE) AS window_end,
        COUNT(*) AS event_count,
        SUM(amount) AS total_amount
    FROM events
    GROUP BY user_id, TUMBLE(event_time, INTERVAL '5' MINUTE)
""")

# Advanced: Side output for late events (events arriving after watermark)
t_env.execute_sql("""
    CREATE TABLE late_events (
        user_id STRING,
        event_type STRING,
        amount DOUBLE,
        event_time TIMESTAMP(3),
        processing_time TIMESTAMP(3),
        lateness_seconds BIGINT
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'late-events',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# Capture late events (arrive after watermark has passed their window)
t_env.execute_sql("""
    INSERT INTO late_events
    SELECT
        user_id,
        event_type,
        amount,
        event_time,
        PROCTIME() AS processing_time,
        TIMESTAMPDIFF(SECOND, event_time, PROCTIME()) AS lateness_seconds
    FROM events
    WHERE event_time < PROCTIME() - INTERVAL '1' MINUTE  -- Arrived late
""")
```

**Watermark Trade-offs**:
- **Short watermark (10s)**: Low latency but may drop late events
- **Long watermark (5min)**: More complete results but higher latency
- **Adaptive watermarks**: Adjust based on observed latency distribution (advanced Flink feature)

**Monitoring Late Events**:
```python
# Separate Flink job to monitor late event percentage
t_env.execute_sql("""
    CREATE VIEW late_event_metrics AS
    SELECT
        TUMBLE_START(PROCTIME(), INTERVAL '1' MINUTE) AS window_start,
        COUNT(*) AS late_event_count,
        AVG(lateness_seconds) AS avg_lateness_sec,
        MAX(lateness_seconds) AS max_lateness_sec
    FROM late_events
    GROUP BY TUMBLE(PROCTIME(), INTERVAL '1' MINUTE)
""")
```

**Decision Rule**: If late events exceed 1% of total events, increase watermark interval.

---

### Code Example 4: Cost-Optimized Hybrid Architecture

```python
class HybridFeatureArchitecture:
    """
    Cost-optimized hybrid architecture:
    - Batch: Compute expensive features daily (large-scale aggregations)
    - Streaming: Compute cheap real-time features (simple counts, sums)
    - Serving: Merge at query time with caching
    """

    def __init__(self):
        self.spark = SparkSession.builder.appName("BatchFeatures").getOrCreate()
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.feature_cache = {}  # In-memory cache for frequently accessed features
        self.cache_ttl = 300  # 5 minutes

    def compute_batch_features_daily(self, date: str):
        """
        Batch job: Compute expensive features overnight
        Cost: ~$50/day for 10-node Spark cluster (4 hours)
        """
        transactions = self.spark.read.format("delta") \
            .load("s3://data/transactions/") \
            .filter(F.col("date") >= F.date_sub(F.lit(date), 90))

        # Expensive aggregations (require shuffles, full dataset scans)
        batch_features = transactions.groupBy("user_id").agg(
            # Statistical features (requires full dataset)
            F.percentile_approx("amount", [0.25, 0.5, 0.75]).alias("amount_quartiles"),
            F.stddev("amount").alias("amount_stddev"),
            F.skewness("amount").alias("amount_skewness"),

            # Time-series features
            F.collect_list(F.struct("date", "amount")).alias("daily_spending_ts"),

            # Graph-based features (merchant co-occurrence)
            F.countDistinct("merchant_id").alias("merchant_count"),

            # Category preference distribution
            F.collect_list(F.struct("category", "amount")).alias("category_distribution")
        )

        # Write to Delta Lake (queryable by serving layer)
        batch_features.write.format("delta").mode("overwrite") \
            .partitionBy("date") \
            .save("s3://features/batch-features/")

        # Also cache in Redis for fast serving (expire after 25 hours)
        batch_features_pd = batch_features.toPandas()
        for _, row in batch_features_pd.iterrows():
            key = f"batch:user:{row['user_id']}"
            self.redis_client.hset(key, mapping=row.to_dict())
            self.redis_client.expire(key, 90000)  # 25 hours

    def compute_streaming_features_realtime(self):
        """
        Streaming job: Compute cheap real-time features
        Cost: ~$150/month for 3-node Flink cluster (24/7)
        """
        from pyflink.table import StreamTableEnvironment

        t_env = StreamTableEnvironment.create(env)

        # Simple aggregations (no expensive shuffles)
        t_env.execute_sql("""
            INSERT INTO redis_realtime_features
            SELECT
                user_id,
                COUNT(*) AS txn_count_1h,
                SUM(amount) AS total_amount_1h,
                MAX(event_time) AS last_event_time
            FROM transaction_stream
            GROUP BY user_id, TUMBLE(event_time, INTERVAL '1' HOUR)
        """)

    def get_features_with_caching(self, user_id: str, current_date: str) -> dict:
        """
        Serving layer: Merge batch + streaming with multi-tier caching
        Target latency: <10ms (p99)
        """
        cache_key = f"{user_id}:{current_date}"

        # Tier 1: In-memory cache (fastest, <1ms)
        if cache_key in self.feature_cache:
            cache_entry = self.feature_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                return cache_entry['features']

        # Tier 2: Redis (fast, 1-5ms)
        batch_key = f"batch:user:{user_id}"
        realtime_key = f"realtime:user:{user_id}"

        batch_features = self.redis_client.hgetall(batch_key)
        realtime_features = self.redis_client.hgetall(realtime_key)

        # Tier 3: Delta Lake fallback (slow, 50-200ms)
        if not batch_features:
            batch_features = self._query_delta_lake(user_id, current_date)

        # Merge features
        merged_features = {**batch_features, **realtime_features}

        # Update in-memory cache
        self.feature_cache[cache_key] = {
            'features': merged_features,
            'timestamp': time.time()
        }

        return merged_features

    def _query_delta_lake(self, user_id: str, date: str) -> dict:
        """Fallback: Query Delta Lake directly (slower)"""
        df = self.spark.read.format("delta") \
            .load("s3://features/batch-features/") \
            .filter((F.col("user_id") == user_id) & (F.col("date") == date)) \
            .limit(1)

        if df.count() == 0:
            return {}

        return df.collect()[0].asDict()

# Cost breakdown:
# - Batch (4 hours/day, 10 nodes): ~$50/day = $1,500/month
# - Streaming (24/7, 3 nodes): ~$150/month
# - Redis (cache.r5.large): ~$100/month
# - Total: ~$1,750/month
#
# Compare to pure streaming (24/7, 15 nodes for same throughput): ~$3,500/month
# Savings: ~50% cost reduction
```

---

## ✅ Best Practices

### 1. Start with Batch, Add Streaming Only When Necessary

**Principle**: Batch processing is simpler, cheaper, and easier to maintain. Add streaming only when latency requirements justify the added complexity.

**Decision Framework**:
- **Latency requirement > 1 hour**: Use batch
- **Latency requirement 5-60 minutes**: Use micro-batching (Spark Structured Streaming)
- **Latency requirement < 5 minutes**: Use streaming (Flink, Kafka Streams)
- **Latency requirement < 1 second**: Use streaming + caching (Redis)

**Example**: Start with daily batch model retraining. If model staleness becomes an issue, move to hourly micro-batching before committing to full streaming.

---

### 2. Use Watermarks Carefully

**Principle**: Watermarks balance completeness vs latency. Too aggressive watermarks drop late data; too conservative watermarks increase latency.

**Best Practices**:
- **Monitor late event percentage**: Track how many events arrive after watermark
- **Set watermark based on observed latency distribution**: P99 latency is a good starting point
- **Use side outputs for late events**: Capture late events in separate stream for analysis
- **Adjust watermarks based on monitoring**: Increase if late events exceed threshold (e.g., 1%)

**Example**:
```python
# Good: Watermark based on observed P99 event latency (30 seconds)
WATERMARK FOR event_time AS event_time - INTERVAL '30' SECOND

# Bad: Overly conservative watermark (unnecessarily high latency)
WATERMARK FOR event_time AS event_time - INTERVAL '10' MINUTE
```

---

### 3. Design for Idempotency

**Principle**: Both batch and streaming jobs should be idempotent (rerunning produces same result).

**Batch Idempotency**:
- Use `mode("overwrite")` for full partition rewrites
- Include date/version in output paths
- Use Delta Lake MERGE for upserts

**Streaming Idempotency**:
- Enable exactly-once semantics (Kafka transactions, Flink checkpoints)
- Use idempotent sinks (upserts, not appends)
- Include deduplication logic for event sources without guarantees

**Example**:
```python
# Idempotent batch write (Delta Lake merge)
new_features.write.format("delta") \
    .mode("overwrite") \
    .option("replaceWhere", f"date = '{target_date}'") \
    .save("s3://features/")

# Idempotent streaming sink (upsert to database)
stream.add_sink(
    JDBCSink.sink(
        "INSERT INTO features (user_id, feature_value) VALUES (?, ?) "
        "ON CONFLICT (user_id) DO UPDATE SET feature_value = EXCLUDED.feature_value",
        jdbc_params
    )
)
```

---

### 4. Optimize State Size in Streaming

**Principle**: Unbounded state growth causes OOM errors and slow checkpoints.

**State Management Strategies**:
- **TTL (Time-To-Live)**: Expire old state automatically
```python
state_backend.set_ttl(StateTtlConfig.new_builder(Time.hours(24)).build())
```
- **Windowing**: Use windows to bound state size
- **State Compaction**: Use RocksDB compaction for large state
- **State Backend Selection**: In-memory for <1GB, RocksDB for >1GB

**Monitoring**:
- Track state size per operator
- Alert if state size grows unbounded
- Benchmark checkpoint duration (<1 minute ideal)

---

### 5. Implement Backpressure Handling

**Principle**: Streaming systems must handle variable load to avoid data loss or system crashes.

**Backpressure Strategies**:
- **Kafka Consumer Lag**: Monitor lag and scale consumers
- **Flink Backpressure Monitoring**: Built-in backpressure metrics
- **Rate Limiting**: Limit ingestion rate during high load
- **Auto-Scaling**: Scale Flink TaskManagers based on CPU/memory

**Example** (Kafka consumer lag monitoring):
```python
# Prometheus metrics for Kafka lag
kafka_consumer_lag = Gauge('kafka_consumer_lag', 'Consumer lag in messages')

def monitor_consumer_lag():
    lag = consumer.lag()  # Get lag from Kafka admin API
    kafka_consumer_lag.set(lag)

    if lag > 1000000:  # 1M message lag threshold
        logger.warning(f"High consumer lag: {lag}")
        # Trigger auto-scaling or alert
```

---

### 6. Separate Compute and Storage

**Principle**: Decouple processing engines from storage for flexibility and cost optimization.

**Architecture**:
- **Storage**: Delta Lake, Apache Iceberg on S3/GCS (cheap, durable)
- **Compute**: Ephemeral Spark/Flink clusters (scale up/down)

**Benefits**:
- Run multiple processing engines on same data (Spark, Flink, Presto)
- Scale compute independently of storage
- Reduce costs by shutting down compute when idle

**Example**:
```python
# Multiple engines reading from same Delta Lake table
spark_df = spark.read.format("delta").load("s3://data/features/")
flink_table = t_env.from_path("delta.`s3://data/features/`")
presto_query = "SELECT * FROM delta.s3://data/features/"
```

---

### 7. Test Streaming Pipelines Thoroughly

**Principle**: Streaming bugs are harder to debug than batch; invest in testing.

**Testing Strategies**:
- **Unit Tests**: Test windowing logic, aggregations with synthetic data
- **Integration Tests**: Test end-to-end with embedded Kafka/Flink
- **Time Travel Tests**: Simulate late events, out-of-order events
- **Performance Tests**: Benchmark throughput and latency

**Example** (Flink unit test):
```python
import unittest
from pyflink.table import EnvironmentSettings, TableEnvironment

class TestStreamingAggregation(unittest.TestCase):
    def setUp(self):
        self.t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())

    def test_tumbling_window_aggregation(self):
        # Create test data
        self.t_env.execute_sql("""
            CREATE TABLE test_events (
                user_id STRING,
                amount DOUBLE,
                event_time TIMESTAMP(3),
                WATERMARK FOR event_time AS event_time
            ) WITH (
                'connector' = 'datagen',
                'number-of-rows' = '100'
            )
        """)

        # Apply aggregation
        result = self.t_env.sql_query("""
            SELECT user_id, COUNT(*) AS cnt
            FROM test_events
            GROUP BY user_id, TUMBLE(event_time, INTERVAL '5' MINUTE)
        """)

        # Validate results
        self.assertGreater(result.execute().collect().__len__(), 0)
```

---

### 8. Monitor End-to-End Latency

**Principle**: Track latency from event generation to final output (not just processing time).

**Metrics to Monitor**:
- **Event-to-Processing Latency**: Time from event generation to processing
- **Processing Latency**: Time spent in processing logic
- **End-to-End Latency**: Event generation → final output
- **Consumer Lag**: Messages waiting to be processed

**Implementation**:
```python
# Embed timestamp in events
event = {
    'user_id': 'user123',
    'amount': 100.0,
    'event_time': datetime.utcnow().isoformat(),  # Event generation time
    'ingestion_time': None  # Populated by ingestion service
}

# Calculate latency in processing
def process_event(event):
    event_time = datetime.fromisoformat(event['event_time'])
    processing_time = datetime.utcnow()
    latency_ms = (processing_time - event_time).total_seconds() * 1000

    # Log to monitoring system
    latency_histogram.observe(latency_ms)

    if latency_ms > 1000:  # Alert if >1 second latency
        logger.warning(f"High latency: {latency_ms}ms for event {event['user_id']}")
```

---

### 9. Use Managed Services for Operational Simplicity

**Principle**: Managed services reduce operational overhead at the cost of flexibility and potentially higher expense.

**Trade-offs**:

| **Self-Managed (EMR, EKS)** | **Managed (Databricks, Kinesis)** |
|----------------------------|-----------------------------------|
| Full control over configuration | Limited configuration options |
| Lower cost (if optimized) | Higher cost but predictable |
| Requires dedicated ops team | Minimal operational overhead |
| Custom integrations | Pre-built integrations |
| Cluster management overhead | Auto-scaling, monitoring included |

**Recommendation**:
- **Small teams (<5 engineers)**: Use managed services (Databricks, AWS Kinesis)
- **Large teams with dedicated ops**: Self-managed for cost optimization
- **Hybrid**: Managed for streaming (complex ops), self-managed for batch (simpler)

---

### 10. Document Latency SLAs

**Principle**: Clearly define and document latency requirements to guide architectural decisions.

**SLA Template**:
```yaml
feature_pipeline_sla:
  feature_type: user_real_time_features
  latency_target:
    p50: 500ms
    p95: 1000ms
    p99: 2000ms
  freshness_requirement: <5 minutes
  data_completeness: 99.9%
  allowed_late_events: 1%

  monitoring:
    metrics:
      - end_to_end_latency
      - consumer_lag
      - late_event_percentage
    alerts:
      - trigger: p99_latency > 3000ms
        severity: warning
      - trigger: consumer_lag > 1000000
        severity: critical
```

---

## ⚠️ Common Pitfalls

### 1. Overengineering with Streaming When Batch Suffices

**Pitfall**: Implementing complex streaming pipelines when business requirements allow hourly or daily updates.

**Example**:
- **Bad**: Building a Flink streaming pipeline for model retraining when model is retrained weekly
- **Good**: Use Airflow batch DAG to retrain weekly; switch to streaming only if business needs <1 hour staleness

**Solution**: Always validate latency requirements before choosing streaming. Ask: "What is the business impact of 1-hour delay vs real-time?"

---

### 2. Ignoring Late Events

**Pitfall**: Setting aggressive watermarks that drop late events without monitoring impact.

**Example**:
- **Bad**: `WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND` drops events delayed >5s
- **Impact**: 10% of mobile app events arrive late due to offline usage → model training on incomplete data

**Solution**:
- Monitor late event percentage
- Use side outputs to capture and analyze late events
- Adjust watermarks based on observed latency distribution

---

### 3. Not Planning for State Growth

**Pitfall**: Stateful streaming jobs with unbounded state leading to OOM errors.

**Example**:
```python
# Bad: Unbounded global aggregation (state grows forever)
SELECT user_id, COUNT(*) FROM events GROUP BY user_id
```

**Solution**:
```python
# Good: Window-based aggregation (state bounded by window size)
SELECT user_id, COUNT(*)
FROM events
GROUP BY user_id, TUMBLE(event_time, INTERVAL '1' DAY)
```

---

### 4. Mixing Event Time and Processing Time

**Pitfall**: Confusing event time (when event occurred) with processing time (when event was processed).

**Example**:
- **Bad**: Using processing time for aggregations when analyzing user behavior
- **Impact**: Late-arriving events assigned to wrong time buckets

**Solution**: Always use event time for business logic; use processing time only for monitoring.

```python
# Good: Event time-based aggregation
GROUP BY TUMBLE(event_time, INTERVAL '1' HOUR)

# Bad: Processing time-based aggregation (non-deterministic)
GROUP BY TUMBLE(PROCTIME(), INTERVAL '1' HOUR)
```

---

### 5. Not Testing Backpressure Scenarios

**Pitfall**: Streaming pipelines fail under load spikes due to untested backpressure handling.

**Example**:
- **Scenario**: Black Friday traffic spike 10x normal load → Flink job crashes
- **Cause**: Insufficient resources + no backpressure testing

**Solution**:
- Load test with 2-5x expected traffic
- Implement rate limiting and auto-scaling
- Monitor backpressure metrics in production

---

### 6. Inefficient Batch Jobs Due to Data Skew

**Pitfall**: Batch jobs with extreme data skew cause some partitions to take 10x longer.

**Example**:
```python
# Bad: Skewed join (some users have 1M transactions, others have 10)
user_features = transactions.groupBy("user_id").agg(...)
```

**Solution**:
```python
# Good: Salting to distribute skewed keys
from pyspark.sql.functions import rand

transactions_salted = transactions.withColumn("salt", (rand() * 10).cast("int"))
user_features = transactions_salted.groupBy("user_id", "salt").agg(...) \
    .groupBy("user_id").agg(...)  # Re-aggregate to remove salt
```

---

### 7. Not Implementing Idempotency

**Pitfall**: Rerunning streaming or batch jobs produces different results or duplicates data.

**Example**:
- **Bad**: Appending to output table on every run
- **Impact**: Duplicate features after job retry

**Solution**: Use upserts (Delta Lake MERGE, database UPSERT) instead of appends.

---

### 8. Underestimating Streaming Operational Complexity

**Pitfall**: Assuming streaming is "just faster batch" without accounting for operational overhead.

**Hidden Costs**:
- 24/7 on-call for streaming pipeline failures
- Complex debugging (checkpoints, state recovery)
- Kafka cluster management (rebalancing, partition management)
- Monitoring and alerting infrastructure

**Solution**: Start small with streaming (1-2 critical pipelines), build operational maturity before scaling.

---

### 9. Not Monitoring Consumer Lag

**Pitfall**: Kafka consumer lag grows unbounded, eventually causing pipeline failure or data loss.

**Example**:
- **Scenario**: Consumer processes 1K events/sec, producer sends 1.5K events/sec
- **Impact**: Lag grows by 500 events/sec → 1.8M lag after 1 hour → exceed Kafka retention → data loss

**Solution**:
- Monitor consumer lag continuously
- Alert when lag exceeds threshold (e.g., 100K messages)
- Auto-scale consumers based on lag

---

### 10. Poor Cost Visibility

**Pitfall**: Lack of cost tracking leads to unexpected cloud bills (especially with always-on streaming).

**Example**:
- **Scenario**: Streaming cluster running 24/7 without monitoring → $5K/month surprise bill
- **Cause**: Over-provisioned cluster (100 cores when 20 suffice)

**Solution**:
- Tag resources with cost center and project
- Set up billing alerts (AWS Budgets, GCP Budgets)
- Right-size clusters based on actual usage
- Consider spot/preemptible instances for cost savings

---

## 🏋️ Hands-On Exercises

### Exercise 1: Implement Windowing in Flink (Intermediate, 8-10 hours)

**Objective**: Build a streaming aggregation pipeline with multiple window types and understand watermarking.

**Scenario**: You have a stream of e-commerce clickstream events. Implement:
1. **Tumbling window**: Count clicks per user every 5 minutes
2. **Sliding window**: Calculate average session duration (10-min window, 2-min slide)
3. **Session window**: Group clicks into sessions (30-min inactivity gap)

**Tasks**:

1. **Set up Flink environment**:
```bash
# Start local Flink cluster
./bin/start-cluster.sh

# Start Kafka
docker run -d -p 9092:9092 --name kafka wurstmeister/kafka
```

2. **Generate synthetic clickstream data** (Python script):
```python
from kafka import KafkaProducer
import json, time, random
from datetime import datetime, timedelta

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

users = [f"user_{i}" for i in range(100)]

while True:
    event = {
        'user_id': random.choice(users),
        'event_type': random.choice(['click', 'view', 'add_to_cart']),
        'product_id': f"product_{random.randint(1, 1000)}",
        'event_time': (datetime.utcnow() - timedelta(seconds=random.randint(0, 60))).isoformat()
    }
    producer.send('clickstream', value=event)
    time.sleep(random.uniform(0.01, 0.1))  # Variable rate
```

3. **Implement Flink job** with all three window types (see Code Example 1 for reference)

4. **Introduce late events** (modify data generator to inject events with 2-minute delay)

5. **Experiment with watermarks**:
   - Start with 10-second watermark
   - Monitor late event percentage
   - Adjust watermark to 30 seconds and observe difference

**Validation**:
- Verify tumbling windows are non-overlapping
- Confirm sliding windows overlap correctly
- Check session windows merge consecutive events
- Measure percentage of late events dropped

**Stretch Goal**: Implement side output for late events and analyze their distribution.

---

### Exercise 2: Optimize a Slow Batch Job (Intermediate, 6-8 hours)

**Objective**: Take an inefficient Spark batch job and apply optimizations to improve performance.

**Scenario**: You have a slow feature engineering job that processes 1TB of transaction data. Current runtime: 4 hours. Target: <1 hour.

**Initial (Slow) Code**:
```python
# Slow job (no optimizations)
transactions = spark.read.parquet("s3://data/transactions/")  # 1TB unpartitioned data

user_features = transactions.groupBy("user_id").agg(
    F.count("*").alias("txn_count"),
    F.sum("amount").alias("total_amount")
)

# Slow join (broadcast not used)
users = spark.read.parquet("s3://data/users/")  # 1GB
result = user_features.join(users, "user_id")

result.write.parquet("s3://output/features/")  # Unoptimized write
```

**Tasks**:

1. **Enable Spark UI** and analyze execution plan:
```python
spark.sparkContext.setLogLevel("WARN")
# Open Spark UI: http://localhost:4040
```

2. **Apply optimizations**:
   - Add partition pruning (partition by date)
   - Use broadcast join for small tables
   - Enable adaptive query execution
   - Cache intermediate results
   - Optimize shuffle partitions
   - Use columnar format (Parquet with compression)

3. **Measure improvements**:
   - Baseline runtime
   - Runtime after each optimization
   - Final runtime

4. **Handle data skew**:
   - Identify skewed keys (users with 10x more transactions)
   - Apply salting technique

**Validation**:
- Achieve <1 hour runtime (target: 50% reduction)
- Verify correctness (compare output with baseline)
- Document optimization impact (create table showing runtime for each step)

**Deliverables**:
- Optimized Spark code
- Performance comparison table
- Spark UI screenshots showing improved plan

---

### Exercise 3: Build a Cost-Optimized Hybrid Pipeline (Advanced, 12-15 hours)

**Objective**: Design and implement a cost-optimized hybrid architecture balancing batch and streaming.

**Scenario**: Build a user recommendation system with:
- **Batch**: Compute collaborative filtering features daily (expensive, run overnight)
- **Streaming**: Track real-time session behavior (cheap, run 24/7)
- **Serving**: Merge both for recommendations with <50ms latency

**Tasks**:

1. **Batch Pipeline** (Spark):
   - Compute user-item collaborative filtering matrix (daily)
   - Features: user similarity scores, popular items in user cohort
   - Write to Delta Lake

2. **Streaming Pipeline** (Flink):
   - Track session clicks in real-time (tumbling 5-min windows)
   - Features: items viewed in session, add-to-cart count
   - Write to Redis (TTL = 1 hour)

3. **Serving Layer** (FastAPI):
   - Implement multi-tier caching (in-memory → Redis → Delta Lake)
   - Merge batch + streaming features
   - Return recommendations with latency <50ms (p99)

4. **Cost Analysis**:
   - Calculate monthly cost for batch cluster (4 hours/day)
   - Calculate monthly cost for streaming cluster (24/7)
   - Compare to pure streaming alternative
   - Document cost savings

5. **Load Testing**:
   - Use Locust or JMeter to test 1000 req/sec
   - Measure p50, p95, p99 latency
   - Verify cache hit rate >90%

**Validation**:
- Recommendations reflect both historical preferences (batch) and current session (streaming)
- Latency p99 <50ms
- Cost <$2000/month
- Cache hit rate >90%

**Deliverables**:
- Complete pipeline code (batch + streaming + serving)
- Cost breakdown spreadsheet
- Load testing report with latency distribution

---

### Exercise 4: Handle Late Events in Production (Advanced, 10-12 hours)

**Objective**: Design a production-grade streaming pipeline that gracefully handles late events.

**Scenario**: IoT sensor data arriving from offline devices with variable latency (5% arrive >1 minute late).

**Tasks**:

1. **Implement main pipeline** (Flink):
   - Aggregate sensor readings (average temperature per 5-min window)
   - Set watermark to 1 minute
   - Sink aggregates to Kafka topic `sensor-aggregates`

2. **Implement late event side output**:
   - Capture events arriving after watermark
   - Sink to separate Kafka topic `late-sensor-events`
   - Calculate lateness distribution

3. **Build reconciliation process**:
   - Batch job to recompute aggregates including late events
   - Compare with original streaming aggregates
   - Identify windows with >5% difference

4. **Monitoring dashboard** (Grafana + Prometheus):
   - Metric: Late event percentage
   - Metric: Average lateness
   - Metric: Windows requiring reconciliation
   - Alert if late events >10%

5. **Adaptive watermark**:
   - Implement logic to adjust watermark based on observed lateness
   - If late events <1%: decrease watermark (lower latency)
   - If late events >5%: increase watermark (more completeness)

**Validation**:
- <1% data loss (late events properly handled)
- Reconciliation identifies all windows with incomplete data
- Dashboard shows real-time lateness metrics

**Deliverables**:
- Flink pipeline with late event handling
- Batch reconciliation job
- Grafana dashboard JSON
- Report analyzing late event patterns

---

## 🔗 Related Concepts

### Within This Course:
- [[02. Lambda vs Kappa Architectures for ML]] - Architectural patterns combining batch and streaming
- [[01. Architectural Patterns for ML Systems]] - Overall system architecture considerations
- [[04. Model Training and Serving Data Flows]] - Batch training vs streaming inference patterns
- [[05. Monitoring and Feedback Loops]] - Detecting data drift in batch vs streaming contexts

### External Connections:
- **Data Storage**: [[02. Data Storage and Versioning Strategies]] - How storage choices impact batch vs streaming performance
- **Feature Engineering**: [[03. Feature Engineering Workflows]] - Computing features in batch vs streaming modes
- **Orchestration**: [[10. Workflow Orchestration]] - Scheduling batch jobs vs managing streaming pipelines
- **Infrastructure**: [[Cloud Data Architecture]] - Cost optimization for batch and streaming workloads

---

## 📚 Further Reading

### Essential Books:

1. **"Streaming Systems" by Tyler Akidau et al. (O'Reilly, 2018)**
   - Comprehensive guide to streaming concepts
   - Covers watermarks, windowing, triggers in depth
   - From Google Dataflow team (Apache Beam creators)

2. **"Designing Data-Intensive Applications" by Martin Kleppmann (O'Reilly, 2017)**
   - Chapter 11: Stream Processing
   - Compares batch and streaming paradigms
   - Discusses event time, processing time, state management

3. **"Stream Processing with Apache Flink" by Fabian Hueske & Vasiliki Kalavri (O'Reilly, 2019)**
   - Hands-on Flink implementation
   - Covers stateful stream processing, windowing, exactly-once semantics

4. **"Learning Spark, 3rd Edition" by Jules S. Damji et al. (O'Reilly, 2023)**
   - Spark Structured Streaming for micro-batching
   - Batch optimization techniques

### Key Papers:

1. **"The Dataflow Model" (Google, 2015)**
   - Foundational paper on unified batch + streaming
   - Introduced watermarks, triggers, accumulation modes
   - https://research.google/pubs/pub43864/

2. **"Lambda Architecture" by Nathan Marz**
   - Original description of batch + speed layer architecture
   - http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html

3. **"Questioning the Lambda Architecture" by Jay Kreps (2014)**
   - Critique of Lambda, proposes Kappa architecture
   - https://www.oreilly.com/radar/questioning-the-lambda-architecture/

4. **"Millwheel: Fault-Tolerant Stream Processing at Internet Scale" (Google, 2013)**
   - Low-latency streaming at scale
   - Exactly-once delivery guarantees

### Blogs and Articles:

1. **Confluent Blog - "Kafka Streams vs. Flink"**
   - https://www.confluent.io/blog/kafka-streams-vs-flink/
   - Practical comparison of streaming frameworks

2. **Databricks Blog - "Structured Streaming in Apache Spark"**
   - https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html
   - Micro-batching approach

3. **Netflix Tech Blog - "Keystone Real-time Stream Processing Platform"**
   - https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a
   - Production streaming architecture at Netflix scale

4. **Uber Engineering - "AthenaX: Uber's Streaming Platform"**
   - https://eng.uber.com/athenax/
   - Lessons from production streaming deployments

### Documentation:

1. **Apache Flink Documentation**
   - https://flink.apache.org/
   - Comprehensive streaming concepts guide

2. **Apache Spark Structured Streaming Guide**
   - https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
   - Micro-batching and streaming APIs

3. **Apache Kafka Streams Documentation**
   - https://kafka.apache.org/documentation/streams/
   - Stream processing library guide

4. **Google Cloud Dataflow (Apache Beam) Documentation**
   - https://cloud.google.com/dataflow/docs
   - Unified batch and streaming programming model

### Video Courses:

1. **"Apache Flink Training" by Ververica (Free)**
   - https://training.ververica.com/
   - Hands-on Flink streaming development

2. **"Stream Processing with Apache Kafka" by Confluent**
   - https://developer.confluent.io/learn-kafka/
   - Kafka Streams and ksqlDB tutorials

3. **"Databricks Academy - Structured Streaming"**
   - https://www.databricks.com/learn/training/
   - Spark Structured Streaming deep dive

---

## 📝 Key Takeaways

1. **Latency drives architecture choice**: Batch for >1 hour, micro-batching for 5-60 min, streaming for <5 min latency requirements.

2. **Batch is simpler and cheaper**: Start with batch processing and add streaming only when latency requirements justify the added complexity and cost.

3. **Streaming requires sophisticated state management**: Unbounded state growth is a common failure mode; use windowing, TTL, and state compaction.

4. **Watermarks balance completeness vs latency**: Monitor late event percentage and adjust watermarks based on observed event-time latency distribution.

5. **Hybrid architectures optimize cost**: Combine expensive batch jobs (historical features) with cheap streaming jobs (real-time signals) for cost-effective ML systems.

6. **Event time ≠ processing time**: Always use event time for business logic to ensure deterministic, correct results; use processing time only for monitoring.

7. **Idempotency is critical**: Design both batch and streaming jobs to be rerunnable without producing duplicates or incorrect results.

8. **Operational complexity of streaming is significant**: 24/7 monitoring, consumer lag management, state recovery, and backpressure handling require dedicated operational expertise.

9. **Cost visibility prevents surprises**: Tag resources, monitor spending, right-size clusters, and consider managed services to control cloud costs.

10. **Test streaming pipelines thoroughly**: Unit test windowing logic, integration test with embedded Kafka/Flink, and load test for backpressure scenarios before production deployment.

---

## ✏️ Notes Section

**Personal Insights**:

**Questions to Explore**:

**Related Projects**:

---

*Last updated: 2025-10-19*
*Part of: [[DEforAI - Data Engineering for AI/ML]]
*Chapter: [[03. Data Architecture for ML Systems]]*
