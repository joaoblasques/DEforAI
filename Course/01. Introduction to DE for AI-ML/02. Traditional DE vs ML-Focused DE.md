# 02. Traditional DE vs ML-Focused DE

**Chapter:** Introduction to Data Engineering for AI/ML
**Topic:** Key differences and new requirements for ML-focused data engineering

---

## 📋 Overview

This subchapter compares traditional data engineering with ML-focused data engineering, highlighting the unique challenges, requirements, and architectural patterns that emerge when supporting machine learning workloads.

---

## 🎯 Learning Objectives

- Identify key differences between traditional and ML-focused data engineering
- Understand new requirements introduced by ML workloads
- Recognize when traditional DE patterns need adaptation for ML
- Learn ML-specific data engineering patterns

---

## 📚 Core Concepts

### Comparison Table

A comprehensive comparison of traditional data engineering vs ML-focused data engineering across 15 dimensions:

| Aspect | Traditional DE | ML-Focused DE | Why It Matters |
|--------|---------------|---------------|----------------|
| **Primary Goal** | Business analytics, reporting, dashboards | Model training & real-time inference | ML requires both historical analysis (training) AND low-latency serving |
| **Data Freshness** | Daily/hourly batches often acceptable | Real-time (<100ms) for many use cases | Fraud detection, recommendations need instant features |
| **Schema Changes** | Managed migrations, can break downstream | Must maintain backward compatibility | Breaking changes invalidate trained models |
| **Data Quality** | Business rules validation (nulls, ranges) | Statistical validation, distribution checks, drift detection | Models sensitive to distribution changes |
| **Reproducibility** | Query reproducibility sufficient | Complete pipeline + data + code reproducibility | Must reproduce exact training conditions |
| **Versioning** | Code versioning (Git) | Data + features + models + code versioning | Experiments require all components versioned |
| **Latency** | Seconds to minutes acceptable | Sub-second for online inference | User-facing ML can't wait for batch jobs |
| **Access Patterns** | Analytical queries (aggregations, scans) | Point lookups (serving) + batch reads (training) | Dual access pattern demands dual storage |
| **Monitoring** | Pipeline success/failure, data volume | + Data drift, feature drift, model performance, prediction latency | Model degradation requires data monitoring |
| **Storage** | Data warehouses (Snowflake, BigQuery) | Warehouses + Feature stores + Model registries | Multiple specialized storage systems |
| **Processing** | Batch ETL/ELT (daily, hourly) | Batch (training) + Streaming (serving) | Must support both modes with consistency |
| **Testing** | Data quality tests, pipeline tests | + Feature parity tests, model validation tests | Training-serving skew requires special testing |
| **Deployment** | Deploy code/queries | Deploy code + models + features + infrastructure | More complex deployment choreography |
| **Team Collaboration** | DE ↔ Analysts | DE ↔ Data Scientists ↔ ML Engineers | Requires tight collaboration across 3+ roles |
| **Cost Model** | Storage + compute (predictable) | Storage + compute + serving infrastructure (variable) | Online serving can be expensive at scale |

### Key Differences Explained

#### 1. Online vs Offline Processing

**Traditional DE: Batch-First Mindset**

Traditional data engineering is predominantly batch-oriented, optimized for historical analysis and reporting:

- **Batch Processing Dominates**
  - Daily, hourly, or weekly ETL/ELT jobs
  - Example: "Refresh sales dashboard every night at midnight"
  - Spark/Airflow for large-scale batch processing
  - Acceptable latency: minutes to hours

- **Data Warehouse as Central Hub**
  - Snowflake, BigQuery, Redshift store processed data
  - Optimized for analytical queries (aggregations, joins)
  - Users query warehouses directly via SQL or BI tools
  - Schema-on-write with strict data modeling

- **Predictable Refresh Cycles**
  - Known schedules (every day at 2 AM)
  - Dependency management via DAGs (Airflow)
  - Backfilling is expensive but manageable
  - SLA: Data available by morning

**ML-Focused DE: Dual-Mode Architecture**

ML systems require BOTH batch processing (for training) AND real-time processing (for serving), creating architectural complexity:

- **Batch Mode for Training**
  - Historical data processing for model training
  - Can be slow (hours to days acceptable)
  - Process months/years of data
  - Example: "Train churn model on 12 months of user behavior"

- **Real-Time Mode for Inference**
  - Low-latency feature serving (<100ms)
  - Process live data streams (Kafka, Kinesis)
  - Online feature stores (Redis, DynamoDB)
  - Example: "Serve fraud detection features in 50ms"

- **The Critical Challenge: Consistency**
  - Same features must be computed identically in both modes
  - Training uses Spark batch job, serving uses streaming
  - Different code paths lead to "training-serving skew"
  - Solution: Feature stores abstract away dual computation

**Example:**
```python
# Traditional: One batch job suffices
def compute_user_metrics():
    """Run daily at 2 AM"""
    df = spark.read.parquet("s3://data/users/")
    metrics = df.groupBy("user_id").agg(...)
    metrics.write.to_warehouse("user_metrics")

# ML: Need BOTH batch and streaming
# Batch for training
def compute_training_features():
    df = spark.read.parquet("s3://data/users/")
    features = df.groupBy("user_id").agg(...)
    feature_store.write_offline(features)  # For training

# Streaming for serving
def compute_serving_features(user_stream):
    features = user_stream.groupBy("user_id").agg(...)
    feature_store.write_online(features)  # For inference

# Both must produce identical results!
```

#### 2. Feature Engineering

**Traditional DE: Transformations for Readability**

Traditional transformations focus on making data understandable for humans and business logic:

- **Purpose:** Create dimensions and facts for reporting
  - Denormalize tables for faster queries
  - Create aggregated summary tables
  - Apply business rules (e.g., "revenue = quantity * price")
  - Optimize for SQL query performance

- **Characteristics:**
  - Transform once, query many times
  - Human-readable column names and values
  - Business logic encoded in SQL
  - Refreshed on schedule (daily/hourly)

- **Example:** Star schema for sales analytics
  ```sql
  -- Traditional: Create denormalized fact table
  CREATE TABLE fact_sales AS
  SELECT
      s.sale_id,
      s.sale_date,
      c.customer_name,        -- Denormalized
      p.product_category,     -- Denormalized
      s.quantity * s.price AS revenue
  FROM sales s
  JOIN customers c ON s.customer_id = c.id
  JOIN products p ON s.product_id = p.id;
  ```

**ML-Focused DE: Features as Model Inputs**

ML feature engineering is fundamentally different—features are statistical transformations optimized for model performance, not human interpretation:

- **Purpose:** Create predictive features for models
  - Engineer features data scientists request
  - Optimize for model accuracy, not readability
  - Handle temporal ordering carefully
  - Enable feature reuse across models

- **Characteristics:**
  - **Point-in-time correctness**: Features must only use historical data
  - **Versioning**: Features are versioned alongside models
  - **Monitoring**: Track feature distributions for drift
  - **Testing**: Validate feature computation logic
  - **Documentation**: Catalog features with definitions and owners

- **Feature Types:**
  - Aggregations over time windows (7-day purchase count)
  - Ratios and derived metrics (click-through rate)
  - Categorical encodings (one-hot, target encoding)
  - Embeddings (user/product vectors)
  - Interaction features (user × product)

- **Example:** ML feature pipeline
  ```python
  # ML: Feature engineering with point-in-time correctness
  def compute_user_features(events_df, as_of_date):
      """
      Compute features as of specific date (no future leakage!)
      """
      # Only use events before as_of_date
      historical = events_df[events_df['timestamp'] < as_of_date]

      features = historical.groupBy('user_id').agg(
          # Aggregations over historical data
          count('event_id').alias('total_events'),
          sum(when(col('event_type') == 'purchase', 1).otherwise(0)).alias('purchase_count'),
          avg('purchase_amount').alias('avg_purchase'),

          # Temporal features
          datediff(as_of_date, max('timestamp')).alias('days_since_last_event'),

          # Behavioral features
          (count('event_id') / datediff(as_of_date, min('timestamp'))).alias('event_frequency')
      )

      return features
  ```

**Key Difference:** Traditional DE creates tables for queries; ML DE creates features for predictions.

#### 3. Data Versioning

**Traditional DE: Code Versioning + SCDs**

Traditional data engineering versions the ETL/ELT code but typically doesn't version the data itself:

- **What's Versioned:**
  - ETL/ELT code in Git
  - Database schema migrations
  - SQL queries and transformations

- **Slowly Changing Dimensions (SCDs)**
  - Type 1: Overwrite (no history)
  - Type 2: Add new row with timestamp (full history)
  - Type 3: Add column for previous value (limited history)
  - Handles changing dimension attributes

- **Historical Snapshots**
  - Daily snapshots of key tables
  - Expensive to store full copies
  - Used for compliance, auditing
  - Example: "Sales table as of Dec 31 each year"

- **Limitations:**
  - Can't reproduce exact query results from past
  - Data changes over time (updates, deletes)
  - No link between code version and data version

**ML-Focused DE: Version Everything**

ML requires complete reproducibility—experiments must be exactly reproducible months later:

- **What's Versioned:**
  - **Data:** Complete datasets (DVC, LakeFS, Delta Lake time travel)
  - **Features:** Feature definitions and computed values
  - **Models:** Model artifacts, weights, architecture
  - **Code:** Training code, pipeline code, serving code
  - **Config:** Hyperparameters, environment, dependencies
  - **Metadata:** Metrics, timestamps, experiment notes

- **Why It Matters:**
  - **Debugging:** "Why did model performance drop in v23?"
  - **Compliance:** "What data was used to train this model?"
  - **Science:** "Can we reproduce the result from the paper?"
  - **Rollback:** "Deploy previous model version with its training data"

- **Tools and Approaches:**
  ```python
  # DVC: Version datasets like code
  dvc add data/train_set.parquet
  git add data/train_set.parquet.dvc
  git commit -m "Training data for experiment #42"
  git tag exp-42

  # LakeFS: Git-like versioning for data lakes
  lakefs commit -m "Production feature snapshot 2025-10-18"
  lakefs tag production-v1.2

  # Delta Lake: Time travel queries
  df = spark.read.format("delta") \
      .option("versionAsOf", 23) \
      .load("s3://features/user_features")

  # MLflow: Track experiments with data versions
  with mlflow.start_run():
      mlflow.log_param("data_version", "v23")
      mlflow.log_param("feature_version", "v12")
      mlflow.log_artifact("train_set.parquet.dvc")
      model = train_model(data)
      mlflow.log_model(model, "model")
  ```

- **Model-Data-Code Triplets:**
  Every model is linked to specific versions of:
  - Training data version
  - Feature code version
  - Model training code version
  - Hyperparameter configuration

**Key Difference:** Traditional DE versions code; ML DE versions code + data + models as linked triplets.

#### 4. Training-Serving Skew

**Traditional DE: Not a Concern**

In traditional data engineering, there's no distinction between "training" and "serving":

- Analysts run the same SQL queries on the same warehouse
- BI dashboards query the same tables
- Reports use the same aggregations
- No concept of "online" vs "offline" modes
- One codebase, one execution path

**ML-Focused DE: The Critical Challenge**

Training-serving skew is one of the most insidious problems in production ML—models perform well offline but fail in production:

**What Is It?**
- Features computed differently during training vs serving
- Results in model degradation despite no changes to model code
- Often silent—no errors, just poor performance

**Why It Happens:**

1. **Different Code Paths**
   ```python
   # Training: Data scientist's notebook
   training_features = df.groupby('user_id').agg({
       'purchase_amount': 'mean'
   })

   # Serving: Production service (different code!)
   serving_features = db.query(
       "SELECT user_id, AVG(purchase_amount) FROM purchases GROUP BY user_id"
   )
   # Subtle differences cause skew
   ```

2. **Different Time Windows**
   ```python
   # Training: Uses all historical data
   features = df.groupby('user_id').agg({'amount': 'sum'})

   # Serving: Only uses recent data (performance reasons)
   features = df[df['date'] > today - 30].groupby('user_id').agg({'amount': 'sum'})
   # Features no longer match!
   ```

3. **Different Data Freshness**
   - Training: Uses complete, cleaned batch data
   - Serving: Uses real-time, potentially noisy data
   - Missing values handled differently

4. **Different Libraries/Versions**
   - Training: pandas 1.3.0
   - Serving: pandas 2.0.0
   - Numeric differences accumulate

**The Solution: Feature Stores**

Feature stores solve training-serving skew by ensuring identical feature computation:

```python
# Define feature ONCE
@feature_view(
    entities=[user],
    ttl=timedelta(days=1),
    online=True,  # Enable for serving
    offline=True  # Enable for training
)
def user_purchase_features(df: pd.DataFrame):
    return df.groupby('user_id').agg({
        'purchase_amount': 'mean',
        'purchase_count': 'count'
    })

# Training: Fetch from offline store
training_data = feature_store.get_historical_features(
    entity_df=users_df,
    features=['user_purchase_features:*']
)

# Serving: Fetch from online store (same computation!)
serving_features = feature_store.get_online_features(
    features=['user_purchase_features:*'],
    entity_rows=[{'user_id': '12345'}]
)

# Guaranteed consistency!
```

**Best Practices:**
- Use feature stores to centralize computation
- Create shared feature libraries used by both training and serving
- Implement parity tests that compare batch and online features
- Monitor feature distributions in production vs training
- Version features alongside models

**Key Difference:** Traditional DE has one code path; ML DE must maintain consistency across two fundamentally different execution modes.

---

## 💡 Practical Examples

### Example 1: User Features - From Analytics to ML

**Traditional Approach: Daily Batch Aggregation**

In traditional data engineering, user metrics are computed once daily for dashboards and reports:

```sql
-- Traditional: Batch ETL creating user aggregates for analytics
-- Runs once daily at 2 AM via Airflow

CREATE TABLE user_metrics AS
SELECT
    user_id,
    COUNT(*) as total_orders,
    SUM(amount) as lifetime_value,
    MAX(order_date) as last_order_date,
    AVG(amount) as avg_order_value,
    -- Simple aggregations for reports
    COUNT(DISTINCT product_id) as unique_products_purchased
FROM orders
WHERE order_status = 'completed'
GROUP BY user_id;

-- Refreshed daily (full recalculation)
-- Used by: Sales dashboard, customer segmentation reports
-- Latency: 24-hour old data is acceptable
-- Access pattern: SQL queries for analytics
```

**Characteristics:**
- ✅ Simple: One SQL query, easy to understand
- ✅ Sufficient: Daily refresh meets business needs
- ✅ Maintainable: Analysts can modify the query
- ❌ Stale: Up to 24 hours old
- ❌ Single mode: Only batch, no real-time
- ❌ Not versioned: Can't reproduce historical values

**ML-Focused Approach: Dual-Mode Feature Engineering**

ML systems need the same features for both training (batch) and inference (real-time), with guaranteed consistency:

```python
from feast import FeatureView, Entity, Field, FileSource
from feast.types import Float32, Int64
from datetime import timedelta
import pandas as pd

# Define user entity
user = Entity(name="user_id", description="User identifier")

# Define feature view with DUAL mode
@feature_view(
    name="user_purchase_features",
    entities=[user],
    ttl=timedelta(days=1),  # How long features are valid
    online=True,   # Enable low-latency serving
    offline=True,  # Enable batch training access
)
class UserPurchaseFeatures:
    """
    User purchase behavior features
    Same logic for training and serving!
    """
    user_id: Field(dtype=Int64)
    total_orders: Field(dtype=Int64, description="Total completed orders")
    lifetime_value: Field(dtype=Float32, description="Sum of all purchase amounts")
    avg_order_value: Field(dtype=Float32, description="Average purchase amount")
    days_since_last_order: Field(dtype=Int64, description="Recency")
    purchase_frequency: Field(dtype=Float32, description="Orders per day active")

# Feature computation logic (shared!)
def compute_user_features(orders_df: pd.DataFrame, as_of_timestamp) -> pd.DataFrame:
    """
    Compute features with point-in-time correctness
    Used by BOTH batch and streaming pipelines
    """
    # Filter to historical data only (no future leakage)
    historical = orders_df[orders_df['timestamp'] < as_of_timestamp]

    # Compute features
    features = historical.groupby('user_id').agg({
        'order_id': 'count',  # total_orders
        'amount': ['sum', 'mean'],  # lifetime_value, avg_order_value
    })

    features.columns = ['total_orders', 'lifetime_value', 'avg_order_value']

    # Temporal features
    features['days_since_last_order'] = (
        as_of_timestamp - historical.groupby('user_id')['timestamp'].max()
    ).dt.days

    # Derived features
    user_lifetime_days = (
        historical.groupby('user_id')['timestamp'].max() -
        historical.groupby('user_id')['timestamp'].min()
    ).dt.days
    features['purchase_frequency'] = features['total_orders'] / user_lifetime_days

    return features

# BATCH: For model training (daily job)
def batch_pipeline():
    """Compute features for all users, write to offline store"""
    orders = load_historical_orders()  # All history
    features = compute_user_features(orders, datetime.now())
    feature_store.write_to_offline_store(features)

# STREAMING: For real-time inference
def streaming_pipeline(order_stream):
    """
    Process streaming orders, update online store
    Uses SAME compute_user_features function!
    """
    for batch in order_stream:
        features = compute_user_features(batch, datetime.now())
        feature_store.write_to_online_store(features)  # Redis/DynamoDB

# Usage in training (batch)
training_features = feature_store.get_historical_features(
    entity_df=training_users,
    features=["user_purchase_features:*"]
)
model.fit(training_features, labels)

# Usage in serving (real-time, <50ms)
serving_features = feature_store.get_online_features(
    features=["user_purchase_features:*"],
    entity_rows=[{"user_id": 12345}]
)
prediction = model.predict(serving_features)
```

**Characteristics:**
- ✅ Real-time: Features served in <100ms
- ✅ Consistent: Same code for training and serving
- ✅ Versioned: Features tied to model versions
- ✅ Monitored: Track drift between training and production
- ✅ Reusable: Multiple models can use same features
- ❌ Complex: Requires feature store infrastructure
- ❌ Learning curve: Team must learn new patterns

**Key Differences:**
| Traditional | ML-Focused |
|------------|------------|
| Daily batch sufficient | Need both batch + real-time |
| One SQL query | Dual-mode pipeline + feature store |
| 24-hour latency OK | <100ms latency required |
| Direct table access | Feature store abstraction |
| Analysts own | Data engineers + data scientists collaborate |

### Example 2: Data Quality - From Business Rules to Statistical Validation

**Traditional Approach: Business Rule Validation**

Traditional data quality focuses on business logic violations:

```python
# Traditional: Business rule validation
def validate_order_data(orders_df):
    """
    Validate business rules before loading to warehouse
    Focus: Prevent invalid data from entering system
    """
    errors = []

    # 1. Required fields must be present
    if orders_df['user_id'].isnull().any():
        errors.append("user_id cannot be null")

    if orders_df['order_id'].isnull().any():
        errors.append("order_id cannot be null")

    # 2. Value ranges must be valid
    if (orders_df['amount'] <= 0).any():
        errors.append("Order amount must be positive")

    if (orders_df['quantity'] <= 0).any():
        errors.append("Quantity must be positive")

    # 3. Categorical values must be in allowed set
    valid_statuses = ['pending', 'completed', 'cancelled', 'refunded']
    if not orders_df['status'].isin(valid_statuses).all():
        errors.append(f"Invalid order status. Must be one of {valid_statuses}")

    # 4. Referential integrity
    valid_users = get_valid_user_ids()
    if not orders_df['user_id'].isin(valid_users).all():
        errors.append("Found orders with invalid user_ids")

    # 5. Logical constraints
    if (orders_df['shipped_date'] < orders_df['order_date']).any():
        errors.append("Ship date cannot be before order date")

    if len(errors) > 0:
        raise ValueError(f"Data quality check failed:\n" + "\n".join(errors))

    return True
```

**Focus:** Prevent obviously broken data from entering the system.

**ML-Focused Approach: Statistical + Business Validation**

ML requires statistical validation to detect subtle distribution changes that break models:

```python
# ML: Statistical validation + business rules
import numpy as np
from scipy import stats
from typing import Dict

class MLDataQualityValidator:
    """
    Validates features for ML with statistical tests
    Detects distribution drift that degrades models
    """

    def __init__(self, reference_stats: Dict):
        """
        Args:
            reference_stats: Statistics from training data
        """
        self.reference_stats = reference_stats

    def validate_features(self, features_df: pd.DataFrame) -> Dict:
        """
        Comprehensive validation: business rules + statistical tests
        """
        results = {
            'passed': True,
            'failures': [],
            'warnings': []
        }

        # 1. BUSINESS RULES (same as traditional)
        if (features_df['amount'] <= 0).any():
            results['passed'] = False
            results['failures'].append("Negative amounts detected")

        if features_df['user_id'].isnull().any():
            results['passed'] = False
            results['failures'].append("Null user_ids detected")

        # 2. STATISTICAL VALIDATION (ML-specific)

        # Check for mean drift (>10% change is suspicious)
        current_mean = features_df['amount'].mean()
        reference_mean = self.reference_stats['amount_mean']
        mean_drift = abs(current_mean - reference_mean) / reference_mean

        if mean_drift > 0.10:
            results['warnings'].append(
                f"Mean drift: {mean_drift:.2%} (current: {current_mean:.2f}, "
                f"reference: {reference_mean:.2f})"
            )

        # Check for std drift (variance change)
        current_std = features_df['amount'].std()
        reference_std = self.reference_stats['amount_std']
        std_ratio = current_std / reference_std

        if std_ratio < 0.5 or std_ratio > 2.0:
            results['warnings'].append(
                f"Standard deviation changed significantly: {std_ratio:.2f}x"
            )

        # 3. DISTRIBUTION SHIFT DETECTION

        # Kolmogorov-Smirnov test for distribution drift
        reference_distribution = self.reference_stats['amount_distribution']
        current_distribution = features_df['amount'].values

        ks_statistic, p_value = stats.ks_2samp(
            reference_distribution,
            current_distribution
        )

        if p_value < 0.01:  # Significant distribution change
            results['passed'] = False
            results['failures'].append(
                f"Distribution drift detected (KS test p-value: {p_value:.4f})"
            )

        # 4. OUTLIER DETECTION

        # Z-score method
        z_scores = np.abs(stats.zscore(features_df['amount']))
        outlier_rate = (z_scores > 3).mean()

        if outlier_rate > 0.05:  # More than 5% outliers is suspicious
            results['warnings'].append(
                f"High outlier rate: {outlier_rate:.2%}"
            )

        # Isolation Forest (ML-based anomaly detection)
        from sklearn.ensemble import IsolationForest
        iso_forest = IsolationForest(contamination=0.01)
        anomalies = iso_forest.fit_predict(features_df[['amount']])
        anomaly_rate = (anomalies == -1).mean()

        if anomaly_rate > 0.02:
            results['warnings'].append(
                f"Anomaly detection flagged {anomaly_rate:.2%} of records"
            )

        # 5. FEATURE CORRELATION CHANGES

        # Check if feature relationships changed
        current_corr = features_df[['amount', 'quantity']].corr().iloc[0, 1]
        reference_corr = self.reference_stats['amount_quantity_corr']

        if abs(current_corr - reference_corr) > 0.2:
            results['warnings'].append(
                f"Feature correlation changed: {reference_corr:.2f} → {current_corr:.2f}"
            )

        # 6. CLASS BALANCE (for classification tasks)
        if 'label' in features_df.columns:
            current_pos_rate = features_df['label'].mean()
            reference_pos_rate = self.reference_stats['positive_rate']

            if abs(current_pos_rate - reference_pos_rate) > 0.05:
                results['warnings'].append(
                    f"Class balance changed: {reference_pos_rate:.2%} → {current_pos_rate:.2%}"
                )

        # 7. MISSING VALUE RATE

        for col in features_df.columns:
            current_null_rate = features_df[col].isnull().mean()
            reference_null_rate = self.reference_stats.get(f'{col}_null_rate', 0)

            if current_null_rate > reference_null_rate + 0.05:
                results['warnings'].append(
                    f"{col}: Missing value rate increased to {current_null_rate:.2%}"
                )

        return results

# Usage
reference_stats = {
    'amount_mean': 127.50,
    'amount_std': 45.20,
    'amount_distribution': training_data['amount'].values,
    'amount_quantity_corr': 0.65,
    'positive_rate': 0.15,
    'quantity_null_rate': 0.02
}

validator = MLDataQualityValidator(reference_stats)

# Validate production features before serving
validation_result = validator.validate_features(production_features)

if not validation_result['passed']:
    # Critical failures - don't serve model!
    alert_team(validation_result['failures'])
    raise ValueError("Feature validation failed!")

if validation_result['warnings']:
    # Non-critical warnings - log but continue
    log_warnings(validation_result['warnings'])
```

**Key Differences:**
| Traditional | ML-Focused |
|------------|------------|
| Business rule violations | Business rules + statistical drift |
| Binary pass/fail | Pass/fail + warnings for drift |
| Schema and value constraints | + Distribution tests |
| Validates current batch | Compares against training distribution |
| Catches data errors | Catches errors + model degradation signals |
| Alert on failures | Alert on failures + drift warnings |

---

## 🔧 New Requirements for ML

[TO BE FILLED]

### 1. Low-Latency Feature Serving
- Sub-100ms response times for online inference
- In-memory feature stores (Redis, DynamoDB)
- Precomputed features

### 2. Point-in-Time Correctness
- Features must reflect data available at prediction time
- Avoid data leakage from future
- Complex temporal joins

### 3. Feature Monitoring
- Distribution drift detection
- Feature importance tracking
- Correlation with model performance

### 4. Experiment Reproducibility
- Version datasets, not just code
- Track all hyperparameters
- Reproducible random seeds

### 5. Online-Offline Consistency
- Identical feature computation logic
- Testing framework for parity
- Continuous validation

---

## ✅ Best Practices

[TO BE FILLED]

### For ML Data Engineering

1. **Use Feature Stores**
   - Centralize feature computation
   - Ensure training-serving consistency
   - Enable feature reuse

2. **Version Everything**
   - Data (DVC, LakeFS)
   - Features (feature store)
   - Models (model registry)
   - Create reproducible experiments

3. **Monitor Distributions**
   - Track feature statistics
   - Alert on drift
   - Link to model performance

4. **Test Feature Pipelines**
   - Unit test transformations
   - Integration test end-to-end
   - Validate online-offline parity

5. **Design for Both Modes**
   - Plan for batch training
   - Plan for real-time serving
   - Abstract common logic

---

## ⚠️ Common Pitfalls

[TO BE FILLED]

1. **Ignoring Training-Serving Skew**
   - Different code paths for training and serving
   - Solution: Feature store or shared library

2. **Not Versioning Data**
   - Can't reproduce experiments
   - Solution: DVC, LakeFS, or lakehouse features

3. **Batch-Only Thinking**
   - Designs don't scale to real-time
   - Solution: Design streaming-first where appropriate

4. **Inadequate Monitoring**
   - Only monitoring model, not data
   - Solution: Comprehensive data quality monitoring

---

## 🏋️ Hands-On Exercises

### Exercise 1: Comparison Matrix
[TO BE FILLED]
- Create detailed comparison table for your organization
- Identify gaps between current state and ML requirements
- Propose migration path

### Exercise 2: Architecture Translation
[TO BE FILLED]
- Take an existing traditional DE pipeline
- Redesign it for ML workload (e.g., real-time recommendations)
- Identify new components needed (feature store, online serving)

### Exercise 3: Tool Research
[TO BE FILLED]
- Research 3 feature store solutions
- Compare capabilities for your use case
- Create evaluation matrix

---

## 🔗 Related Concepts

- [[01. The Role of Data Engineering in ML|Role of DE in ML]]
- [[03. The ML Data Engineering Tech Stack|ML DE Tech Stack]]
- [[01_Projects/DEforAI/Course/02. ML Data Pipeline Lifecycle/README|Chapter 2: ML Data Pipeline Lifecycle]]
- [[01_Projects/DEforAI/Course/10. Feature Stores/README|Chapter 10: Feature Stores]]

---

## 📚 Further Reading

[TO BE FILLED]

### Articles
- "Hidden Technical Debt in Machine Learning Systems" (Google)
- "Rules of Machine Learning" (Google)
- "Data Cascades in High-Stakes AI" (Paper on data quality in ML)

### Documentation
- Feature store documentation (Feast, Tecton)
- MLOps platform documentation

---

## 📝 Key Takeaways

[TO BE FILLED]

1. ML introduces fundamentally new requirements beyond traditional DE
2. Training-serving consistency is a critical challenge
3. Real-time requirements demand new architectures
4. Data versioning and reproducibility are essential
5. Feature stores are key enabler for ML data engineering
6. Monitoring must include data quality and drift detection

---

## ✏️ Notes Section

[Use this space for your personal notes and insights]

---

*Created: October 18, 2025*
*Last Updated: October 18, 2025*
*Status: Template - To be completed*
