# 03. Feature Engineering Workflows

**Chapter:** ML Data Pipeline Lifecycle
**Topic:** Designing reproducible, scalable feature transformation pipelines for ML

---

## üìã Overview

Feature engineering is the process of transforming raw data into meaningful signals (features) that ML models can learn from. In production systems, this isn't a one-time notebook operation‚Äîit's a continuous, automated workflow requiring consistency across training and serving environments.

**Key Industry Insight (2024-2025):** The Feature-Training-Inference (FTI) pipeline pattern has emerged as the standard for production ML, separating feature computation, model training, and inference into independently developed and operated pipelines. Feature stores like Feast and Tecton have become central infrastructure for managing this complexity.

**Critical Challenge:** Training-serving skew‚Äîwhere features computed during training differ from those computed during inference‚Äîis one of the most common and difficult-to-debug causes of model failure in production.

---

## üéØ Learning Objectives

After completing this subchapter, you will be able to:
- Design feature transformation pipelines that maintain training-serving consistency
- Implement feature stores (Feast, Tecton) for offline and online feature serving
- Build automated feature validation and monitoring systems
- Handle feature dependencies and transformations in DAGs
- Version features alongside models for reproducibility
- Debug and prevent training-serving skew
- Implement point-in-time correct feature retrieval

---

## üìö Core Concepts

### 1. Feature Transformation Types

#### A. Batch Transformations

**Definition:** Features computed periodically (hourly, daily) on historical data.

**Characteristics:**
- High throughput, high latency acceptable
- Complex aggregations (90-day rolling averages, percentiles)
- Scheduled execution (Airflow, Prefect)
- Stored in offline feature store

**Use Cases:**
- Training dataset generation
- User lifetime value calculations
- Historical behavior aggregations
- Batch prediction workloads

**Example:**
```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

spark = SparkSession.builder.appName("BatchFeatures").getOrCreate()

# Compute batch features: user purchase patterns
def compute_user_purchase_features(transactions_df, computation_date):
    """
    Compute user purchase features for a specific date.
    Features include 7-day, 30-day, and 90-day aggregations.
    """

    # Define time windows
    window_7d = Window.partitionBy("user_id").orderBy("transaction_date") \
        .rangeBetween(-7*86400, 0)  # 7 days in seconds

    window_30d = Window.partitionBy("user_id").orderBy("transaction_date") \
        .rangeBetween(-30*86400, 0)

    window_90d = Window.partitionBy("user_id").orderBy("transaction_date") \
        .rangeBetween(-90*86400, 0)

    # Compute rolling aggregations
    features_df = transactions_df \
        .withColumn("transactions_7d", F.count("*").over(window_7d)) \
        .withColumn("transactions_30d", F.count("*").over(window_30d)) \
        .withColumn("transactions_90d", F.count("*").over(window_90d)) \
        .withColumn("total_spent_7d", F.sum("amount").over(window_7d)) \
        .withColumn("total_spent_30d", F.sum("amount").over(window_30d)) \
        .withColumn("avg_basket_size_30d", F.avg("amount").over(window_30d)) \
        .withColumn("max_purchase_90d", F.max("amount").over(window_90d))

    # Filter to computation date (point-in-time)
    features_df = features_df.filter(F.col("transaction_date") == computation_date)

    # Aggregate to user level
    user_features = features_df.groupBy("user_id").agg(
        F.first("transactions_7d").alias("transactions_7d"),
        F.first("transactions_30d").alias("transactions_30d"),
        F.first("transactions_90d").alias("transactions_90d"),
        F.first("total_spent_7d").alias("total_spent_7d"),
        F.first("total_spent_30d").alias("total_spent_30d"),
        F.first("avg_basket_size_30d").alias("avg_basket_size_30d"),
        F.first("max_purchase_90d").alias("max_purchase_90d")
    ).withColumn("feature_timestamp", F.lit(computation_date))

    return user_features

# Usage
transactions = spark.read.parquet("s3://raw-data/transactions/")
user_features = compute_user_purchase_features(transactions, "2025-10-18")

# Write to offline feature store
user_features.write.format("delta") \
    .mode("append") \
    .partitionBy("feature_timestamp") \
    .save("s3://feature-store/offline/user_purchase_features/")
```

---

#### B. Streaming Transformations

**Definition:** Features computed in real-time or near-real-time from event streams.

**Characteristics:**
- Low latency (<100ms to seconds)
- Simple transformations (windowed aggregations, lookups)
- Continuous execution (Flink, Spark Streaming, KSQL)
- Materialized to online feature store

**Use Cases:**
- Real-time fraud detection
- Dynamic pricing
- Live recommendation systems
- Low-latency prediction services

**Example:**
```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *

spark = SparkSession.builder \
    .appName("StreamingFeatures") \
    .config("spark.sql.streaming.schemaInference", "true") \
    .getOrCreate()

# Define schema for click events
click_schema = StructType([
    StructField("user_id", StringType(), False),
    StructField("product_id", StringType(), False),
    StructField("timestamp", TimestampType(), False),
    StructField("session_id", StringType(), False)
])

# Read from Kafka stream
click_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "user-clicks") \
    .load() \
    .select(F.from_json(F.col("value").cast("string"), click_schema).alias("data")) \
    .select("data.*")

# Compute streaming features: clicks in last 5 minutes
streaming_features = click_stream \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        F.window("timestamp", "5 minutes", "1 minute"),
        "user_id"
    ).agg(
        F.count("*").alias("clicks_last_5min"),
        F.countDistinct("product_id").alias("unique_products_viewed_5min"),
        F.countDistinct("session_id").alias("active_sessions_5min")
    ).select(
        "user_id",
        "clicks_last_5min",
        "unique_products_viewed_5min",
        "active_sessions_5min",
        F.current_timestamp().alias("feature_timestamp")
    )

# Write to online feature store (DynamoDB via foreachBatch)
def write_to_dynamodb(batch_df, batch_id):
    """Write features to DynamoDB for low-latency serving"""
    import boto3

    dynamodb = boto3.resource('dynamodb', region_name='us-east-1')
    table = dynamodb.Table('ml_features')

    for row in batch_df.collect():
        table.put_item(
            Item={
                'user_id': row.user_id,
                'clicks_last_5min': int(row.clicks_last_5min),
                'unique_products_viewed_5min': int(row.unique_products_viewed_5min),
                'active_sessions_5min': int(row.active_sessions_5min),
                'feature_timestamp': row.feature_timestamp.isoformat(),
                'ttl': int(time.time()) + 3600  # 1 hour TTL
            }
        )

# Start streaming query
query = streaming_features.writeStream \
    .foreachBatch(write_to_dynamodb) \
    .outputMode("update") \
    .option("checkpointLocation", "s3://checkpoints/streaming_features/") \
    .start()

query.awaitTermination()
```

---

#### C. On-Demand Transformations

**Definition:** Features computed at request time during inference.

**Characteristics:**
- Computed per prediction request
- Very low latency (<10ms)
- Simple lookups or calculations
- Stateless transformations

**Use Cases:**
- Contextual features (time of day, device type)
- Request-specific calculations
- Feature crosses at inference time
- Privacy-sensitive features

**Example:**
```python
from datetime import datetime
import numpy as np

class OnDemandFeatureComputer:
    """
    Compute features at inference time from request context
    """

    def __init__(self):
        # Lookup tables loaded at initialization
        self.category_embeddings = self._load_category_embeddings()
        self.location_stats = self._load_location_stats()

    def compute_contextual_features(self, request_data: dict) -> dict:
        """
        Compute features from request context in <10ms
        """
        features = {}

        # Time-based features
        timestamp = datetime.fromisoformat(request_data['timestamp'])
        features['hour_of_day'] = timestamp.hour
        features['day_of_week'] = timestamp.weekday()
        features['is_weekend'] = int(timestamp.weekday() >= 5)
        features['is_business_hours'] = int(9 <= timestamp.hour <= 17)

        # Device features
        features['is_mobile'] = int('mobile' in request_data.get('user_agent', '').lower())
        features['is_ios'] = int('ios' in request_data.get('platform', '').lower())

        # Location-based features
        location = request_data.get('location', 'unknown')
        features['location_avg_order_value'] = self.location_stats.get(
            location, {}
        ).get('avg_order_value', 0.0)

        # Embedding lookup
        category = request_data.get('category', 'unknown')
        features['category_embedding'] = self.category_embeddings.get(
            category,
            np.zeros(64)  # 64-dim embedding
        ).tolist()

        # Feature crosses
        features['hour_x_is_mobile'] = features['hour_of_day'] * features['is_mobile']
        features['weekend_x_location'] = features['is_weekend'] * hash(location) % 100

        return features

    def _load_category_embeddings(self):
        # Load pre-computed embeddings
        return {}  # Simplified

    def _load_location_stats(self):
        # Load location statistics
        return {}  # Simplified

# Usage in prediction service
feature_computer = OnDemandFeatureComputer()

def predict(request_data: dict):
    # Get precomputed features from feature store
    user_id = request_data['user_id']
    offline_features = feature_store.get_online_features(
        entity_rows=[{"user_id": user_id}],
        features=["user_features:transactions_30d", "user_features:total_spent_30d"]
    )

    # Compute on-demand features
    online_features = feature_computer.compute_contextual_features(request_data)

    # Combine and predict
    all_features = {**offline_features, **online_features}
    prediction = model.predict([all_features])

    return prediction
```

---

### 2. Feature Store Architecture

#### Core Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Feature Registry                        ‚îÇ
‚îÇ  (Metadata: definitions, owners, SLAs, lineage, schemas)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üë
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Feature Pipelines ‚îÇ              ‚îÇ   Feature Serving    ‚îÇ
‚îÇ                   ‚îÇ              ‚îÇ                      ‚îÇ
‚îÇ - Batch (Spark)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ  - Online Store      ‚îÇ
‚îÇ - Streaming(Flink)‚îÇ      ‚îÇ       ‚îÇ    (Redis/DynamoDB)  ‚îÇ
‚îÇ - On-Demand       ‚îÇ      ‚îÇ       ‚îÇ  - Offline Store     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ       ‚îÇ    (S3/Snowflake)    ‚îÇ
                           ‚Üì       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Storage   ‚îÇ
                    ‚îÇ            ‚îÇ
                    ‚îÇ - Offline  ‚îÇ ‚Üê Training datasets
                    ‚îÇ - Online   ‚îÇ ‚Üê Inference features
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Capabilities:**

1. **Dual Storage:**
   - **Offline Store:** S3, Snowflake, BigQuery (columnar, historical)
   - **Online Store:** Redis, DynamoDB, Cassandra (key-value, <10ms latency)

2. **Feature Registry:**
   - Centralized catalog of all features
   - Ownership, documentation, SLAs
   - Data lineage and dependencies
   - Schema and type definitions

3. **Point-in-Time Joins:**
   - Prevent data leakage in training
   - Ensure features match labels' timestamps
   - Temporal correctness guarantees

4. **Materialization:**
   - Batch: Scheduled backfills to offline store
   - Streaming: Continuous updates to online store
   - Incremental: Only compute new/changed features

---

#### Feast Feature Store Implementation

```python
from feast import Entity, FeatureView, Field, FeatureStore, FileSource
from feast.types import Float32, Int64, String, UnixTimestamp
from datetime import timedelta
import pandas as pd

# 1. Define entity (what features describe)
user = Entity(
    name="user",
    join_keys=["user_id"],
    description="User entity for e-commerce ML models"
)

# 2. Define data source (offline)
user_features_source = FileSource(
    path="s3://feature-data/user_purchase_features/",
    timestamp_field="feature_timestamp",
    created_timestamp_column="created_timestamp"
)

# 3. Define feature view (logical grouping)
user_purchase_features = FeatureView(
    name="user_purchase_features",
    entities=[user],
    ttl=timedelta(days=365),  # How long features are valid
    schema=[
        Field(name="transactions_7d", dtype=Int64),
        Field(name="transactions_30d", dtype=Int64),
        Field(name="transactions_90d", dtype=Int64),
        Field(name="total_spent_7d", dtype=Float32),
        Field(name="total_spent_30d", dtype=Float32),
        Field(name="avg_basket_size_30d", dtype=Float32),
        Field(name="max_purchase_90d", dtype=Float32),
        Field(name="favorite_category", dtype=String)
    ],
    source=user_features_source,
    online=True,  # Enable online serving
    tags={"team": "ml-platform", "project": "churn-prediction"}
)

# 4. Initialize feature store
store = FeatureStore(repo_path="feature_repo/")

# 5. Apply (register) features
store.apply([user, user_purchase_features])

# 6. Materialize to online store (once or scheduled)
from datetime import datetime
store.materialize(
    start_date=datetime(2025, 1, 1),
    end_date=datetime(2025, 10, 18)
)

# --- TRAINING: Get historical features (point-in-time correct) ---
entity_df = pd.DataFrame({
    "user_id": ["user_001", "user_002", "user_003"],
    "event_timestamp": [
        datetime(2025, 9, 1),
        datetime(2025, 9, 15),
        datetime(2025, 10, 1)
    ]
})

training_df = store.get_historical_features(
    entity_df=entity_df,
    features=[
        "user_purchase_features:transactions_30d",
        "user_purchase_features:total_spent_30d",
        "user_purchase_features:avg_basket_size_30d"
    ]
).to_df()

print("Training features (point-in-time):")
print(training_df)

# --- SERVING: Get online features for inference ---
online_features = store.get_online_features(
    entity_rows=[
        {"user_id": "user_001"},
        {"user_id": "user_002"}
    ],
    features=[
        "user_purchase_features:transactions_30d",
        "user_purchase_features:total_spent_30d"
    ]
).to_dict()

print("\nOnline features (low-latency):")
print(online_features)
```

---

### 3. Training-Serving Consistency

#### The Problem: Training-Serving Skew

**Definition:** Features computed during training differ from features computed during inference, causing model degradation.

**Common Causes:**

1. **Different code implementations:**
   ```python
   # Training (Python/Pandas)
   def compute_feature_training(df):
       return df.groupby('user_id')['amount'].mean()

   # Serving (Java microservice)
   // Different rounding, null handling, or logic
   double computeFeatureServing(List<Transaction> txns) {
       return txns.stream()
           .mapToDouble(t -> t.getAmount())
           .average()
           .orElse(0.0);  // Different default!
   }
   ```

2. **Different data sources:**
   - Training: Historical batch data from data warehouse
   - Serving: Real-time data from production database

3. **Different time windows:**
   - Training: Features computed with all available historical data
   - Serving: Features computed with limited lookback window

4. **Data quality issues:**
   - Training: Cleaned, validated data
   - Serving: Raw production data with missing/malformed values

**Impact:**
- Model performs well offline (AUC: 0.92)
- Model performs poorly online (AUC: 0.67)
- Silent degradation‚Äîhard to detect without monitoring

---

#### The Solution: Unified Feature Pipelines

**Principle:** Write feature transformation logic once, execute it consistently across training and serving.

**Implementation Pattern:**

```python
# feature_definitions.py - Single source of truth
class UserPurchaseFeatures:
    """
    Feature transformations defined once, used everywhere
    """

    @staticmethod
    def compute_features(transactions: pd.DataFrame, as_of_date: datetime) -> pd.DataFrame:
        """
        Compute user purchase features as of a specific date.

        This function is used for both:
        1. Batch feature computation (training)
        2. Online feature computation (serving)
        """
        # Filter to data before as_of_date (point-in-time correctness)
        transactions = transactions[transactions['timestamp'] <= as_of_date]

        # Define time windows
        date_7d_ago = as_of_date - timedelta(days=7)
        date_30d_ago = as_of_date - timedelta(days=30)
        date_90d_ago = as_of_date - timedelta(days=90)

        # Compute features
        features = transactions.groupby('user_id').apply(lambda group: pd.Series({
            'transactions_7d': len(group[group['timestamp'] >= date_7d_ago]),
            'transactions_30d': len(group[group['timestamp'] >= date_30d_ago]),
            'transactions_90d': len(group[group['timestamp'] >= date_90d_ago]),
            'total_spent_30d': group[group['timestamp'] >= date_30d_ago]['amount'].sum(),
            'avg_basket_size_30d': group[group['timestamp'] >= date_30d_ago]['amount'].mean(),
            'max_purchase_90d': group[group['timestamp'] >= date_90d_ago]['amount'].max()
        })).reset_index()

        # Handle nulls consistently
        features = features.fillna({
            'transactions_7d': 0,
            'transactions_30d': 0,
            'transactions_90d': 0,
            'total_spent_30d': 0.0,
            'avg_basket_size_30d': 0.0,
            'max_purchase_90d': 0.0
        })

        return features


# batch_pipeline.py - Training feature computation
def compute_training_features(date: str):
    """Batch pipeline for training dataset generation"""
    transactions = spark.read.parquet("s3://data/transactions/")
    transactions_pd = transactions.toPandas()

    # Use shared feature logic
    features = UserPurchaseFeatures.compute_features(
        transactions_pd,
        as_of_date=datetime.fromisoformat(date)
    )

    # Write to offline feature store
    features.to_parquet(f"s3://features/user_purchase/{date}.parquet")
    return features


# serving.py - Inference feature computation
def get_features_for_prediction(user_id: str):
    """Online serving for inference"""
    # Fetch recent transactions for user
    transactions = database.query(
        f"SELECT * FROM transactions WHERE user_id = '{user_id}' "
        f"AND timestamp >= NOW() - INTERVAL '90 days'"
    )
    transactions_pd = pd.DataFrame(transactions)

    # Use same feature logic as training
    features = UserPurchaseFeatures.compute_features(
        transactions_pd,
        as_of_date=datetime.now()
    )

    return features[features['user_id'] == user_id].iloc[0].to_dict()
```

**Benefits:**
- Single implementation, no logic drift
- Consistent null handling, rounding, defaults
- Testable in isolation
- Versioned with Git

---

### 4. Point-in-Time Correctness

**Problem:** Using future information during training causes data leakage.

**Example of Leakage:**

```python
# ‚ùå WRONG: Data leakage - using all data to compute features
def create_training_data_wrong(transactions, labels):
    # Compute lifetime stats using ALL transactions
    user_stats = transactions.groupby('user_id').agg({
        'amount': ['sum', 'mean', 'max']
    })

    # Join with labels from September 2025
    training_data = labels.merge(user_stats, on='user_id')

    # Problem: user_stats includes October, November data
    # Model learns from "future" which won't be available at inference!
    return training_data


# ‚úÖ CORRECT: Point-in-time join
def create_training_data_correct(transactions, labels):
    """
    For each label, use only data available BEFORE that label's timestamp
    """
    training_data = []

    for _, label_row in labels.iterrows():
        user_id = label_row['user_id']
        label_timestamp = label_row['timestamp']

        # Only use transactions BEFORE the label timestamp
        past_transactions = transactions[
            (transactions['user_id'] == user_id) &
            (transactions['timestamp'] < label_timestamp)
        ]

        # Compute features from past data only
        features = {
            'user_id': user_id,
            'total_spent': past_transactions['amount'].sum(),
            'avg_purchase': past_transactions['amount'].mean(),
            'num_purchases': len(past_transactions),
            'label': label_row['label'],
            'label_timestamp': label_timestamp
        }

        training_data.append(features)

    return pd.DataFrame(training_data)
```

**Feast's Automatic Point-in-Time Joins:**

```python
# Feast handles this automatically!
entity_df = pd.DataFrame({
    "user_id": ["user_001", "user_002"],
    "event_timestamp": [
        datetime(2025, 9, 1, 14, 30),  # Label timestamp
        datetime(2025, 9, 15, 10, 15)
    ]
})

# Feast retrieves features as they existed at event_timestamp
training_df = store.get_historical_features(
    entity_df=entity_df,
    features=["user_purchase_features:total_spent_30d"]
).to_df()

# For user_001 at Sept 1, uses transactions before Sept 1
# For user_002 at Sept 15, uses transactions before Sept 15
```

---

### 5. Feature Validation and Monitoring

#### Schema Validation

```python
from pydantic import BaseModel, Field, validator
from typing import List, Optional
from datetime import datetime

class UserFeatures(BaseModel):
    """Schema for user purchase features with validation"""

    user_id: str = Field(..., min_length=1, max_length=100)
    transactions_7d: int = Field(..., ge=0, le=10000)
    transactions_30d: int = Field(..., ge=0, le=50000)
    total_spent_30d: float = Field(..., ge=0.0, le=1000000.0)
    avg_basket_size_30d: float = Field(..., ge=0.0)
    favorite_category: Optional[str] = None
    feature_timestamp: datetime

    @validator('transactions_7d', 'transactions_30d')
    def validate_transaction_relationship(cls, v, values):
        """7-day count should be <= 30-day count"""
        if 'transactions_30d' in values and 'transactions_7d' in values:
            if values['transactions_7d'] > values['transactions_30d']:
                raise ValueError("7-day transactions cannot exceed 30-day transactions")
        return v

    @validator('feature_timestamp')
    def validate_freshness(cls, v):
        """Features should not be too stale"""
        age = datetime.now() - v
        if age.days > 7:
            raise ValueError(f"Features are {age.days} days old, max allowed is 7")
        return v

# Usage in feature pipeline
def validate_features(features_df: pd.DataFrame) -> pd.DataFrame:
    """Validate all features match schema"""
    validated = []
    errors = []

    for idx, row in features_df.iterrows():
        try:
            validated_row = UserFeatures(**row.to_dict())
            validated.append(validated_row.dict())
        except Exception as e:
            errors.append({"row_idx": idx, "error": str(e)})

    if errors:
        print(f"‚ùå Validation failed for {len(errors)} rows:")
        for error in errors[:10]:  # Show first 10
            print(f"  Row {error['row_idx']}: {error['error']}")

    return pd.DataFrame(validated)
```

---

#### Distribution Monitoring

```python
import numpy as np
from scipy import stats

class FeatureMonitor:
    """Monitor feature distributions for drift"""

    def __init__(self, feature_names: List[str]):
        self.feature_names = feature_names
        self.reference_stats = {}  # Store reference distributions

    def fit_reference(self, features_df: pd.DataFrame):
        """Compute reference statistics from training data"""
        for feature in self.feature_names:
            self.reference_stats[feature] = {
                'mean': features_df[feature].mean(),
                'std': features_df[feature].std(),
                'min': features_df[feature].min(),
                'max': features_df[feature].max(),
                'percentiles': features_df[feature].quantile([0.25, 0.5, 0.75]).to_dict(),
                'distribution': features_df[feature].values  # For KS test
            }

    def detect_drift(self, current_features_df: pd.DataFrame, threshold: float = 0.05) -> dict:
        """
        Detect distribution drift using Kolmogorov-Smirnov test
        """
        drift_report = {}

        for feature in self.feature_names:
            reference_dist = self.reference_stats[feature]['distribution']
            current_dist = current_features_df[feature].values

            # Kolmogorov-Smirnov test
            ks_statistic, p_value = stats.ks_2samp(reference_dist, current_dist)

            # Statistical comparison
            current_mean = current_features_df[feature].mean()
            reference_mean = self.reference_stats[feature]['mean']
            mean_shift_pct = ((current_mean - reference_mean) / reference_mean) * 100

            drift_report[feature] = {
                'drifted': p_value < threshold,
                'p_value': p_value,
                'ks_statistic': ks_statistic,
                'mean_shift_pct': mean_shift_pct,
                'current_mean': current_mean,
                'reference_mean': reference_mean
            }

        return drift_report

    def alert_if_drift(self, drift_report: dict):
        """Send alerts for drifted features"""
        drifted_features = [f for f, stats in drift_report.items() if stats['drifted']]

        if drifted_features:
            print(f"‚ö†Ô∏è  DRIFT DETECTED in {len(drifted_features)} features:")
            for feature in drifted_features:
                stats_info = drift_report[feature]
                print(f"  - {feature}:")
                print(f"      Mean shift: {stats_info['mean_shift_pct']:.2f}%")
                print(f"      KS statistic: {stats_info['ks_statistic']:.4f}")
                print(f"      p-value: {stats_info['p_value']:.4f}")

            # Send to monitoring system
            self._send_alert(drifted_features, drift_report)

    def _send_alert(self, drifted_features, drift_report):
        """Send alert to monitoring system (PagerDuty, Slack, etc.)"""
        # Implementation depends on monitoring stack
        pass

# Usage
monitor = FeatureMonitor(feature_names=['transactions_30d', 'total_spent_30d'])

# Fit on training data (once)
monitor.fit_reference(training_features_df)

# Monitor production features (scheduled hourly/daily)
production_features = fetch_recent_features(hours=24)
drift_report = monitor.detect_drift(production_features)
monitor.alert_if_drift(drift_report)
```

---

## üí° Practical Examples

### Example 1: E-Commerce Churn Prediction Features

**Scenario:** Predict customer churn for subscription service

**Feature Pipeline:**

```python
from feast import FeatureStore
import pandas as pd

class ChurnPredictionFeatures:
    """
    Feature engineering for churn prediction model
    """

    def __init__(self):
        self.feature_store = FeatureStore(repo_path=".")

    def compute_behavioral_features(self, user_id: str, as_of_date: datetime) -> dict:
        """
        Compute user behavioral features for churn prediction
        """
        # Get historical activity data
        activity = self._get_user_activity(user_id, as_of_date)

        # Engagement features
        features = {
            'days_since_last_login': (as_of_date - activity['last_login']).days,
            'logins_last_30d': len(activity['logins_30d']),
            'avg_session_duration_30d': np.mean(activity['session_durations_30d']),
            'feature_usage_diversity': len(set(activity['features_used_30d'])),

            # Billing features
            'days_until_renewal': (activity['next_billing_date'] - as_of_date).days,
            'payment_failures_90d': activity['payment_failures_90d'],
            'total_spent_ltv': activity['lifetime_value'],

            # Support features
            'support_tickets_30d': activity['support_tickets_30d'],
            'unresolved_issues': activity['unresolved_issues'],

            # Social features
            'referrals_made': activity['referrals_count'],
            'teammates_count': activity['team_size']
        }

        # Derived features
        features['engagement_score'] = self._compute_engagement_score(features)
        features['churn_risk_signals'] = self._count_churn_signals(features)

        return features

    def _compute_engagement_score(self, features: dict) -> float:
        """Composite engagement score"""
        score = (
            min(features['logins_last_30d'] / 20, 1.0) * 0.3 +
            min(features['avg_session_duration_30d'] / 30, 1.0) * 0.3 +
            min(features['feature_usage_diversity'] / 10, 1.0) * 0.2 +
            (1.0 if features['referrals_made'] > 0 else 0.0) * 0.2
        )
        return score

    def _count_churn_signals(self, features: dict) -> int:
        """Count number of churn risk signals"""
        signals = 0

        if features['days_since_last_login'] > 14:
            signals += 1
        if features['logins_last_30d'] < 5:
            signals += 1
        if features['payment_failures_90d'] > 0:
            signals += 1
        if features['support_tickets_30d'] > 3:
            signals += 1
        if features['days_until_renewal'] < 7:
            signals += 1

        return signals

    def create_training_dataset(self, user_ids: List[str], label_dates: List[datetime]):
        """
        Create point-in-time correct training dataset
        """
        training_data = []

        for user_id, label_date in zip(user_ids, label_dates):
            # Get features as of label_date (no future data)
            features = self.compute_behavioral_features(user_id, label_date)

            # Get label (churned in next 30 days?)
            label = self._get_churn_label(user_id, label_date)

            training_data.append({
                'user_id': user_id,
                'label_date': label_date,
                'churned': label,
                **features
            })

        return pd.DataFrame(training_data)
```

---

### Example 2: Real-Time Fraud Detection Features

**Scenario:** Detect fraudulent transactions in real-time

**Streaming Feature Pipeline:**

```python
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Define streaming query for fraud features
def create_fraud_detection_features(transactions_stream):
    """
    Compute fraud detection features from transaction stream
    """

    # User-level velocity features (5-minute windows)
    user_velocity = transactions_stream \
        .withWatermark("timestamp", "10 minutes") \
        .groupBy(
            F.window("timestamp", "5 minutes", "1 minute"),
            "user_id"
        ).agg(
            F.count("*").alias("transactions_5min"),
            F.sum("amount").alias("total_amount_5min"),
            F.countDistinct("merchant_id").alias("unique_merchants_5min"),
            F.countDistinct("device_id").alias("unique_devices_5min"),
            F.countDistinct("ip_address").alias("unique_ips_5min")
        )

    # Merchant-level anomaly features
    merchant_stats = transactions_stream \
        .withWatermark("timestamp", "1 hour") \
        .groupBy(
            F.window("timestamp", "1 hour"),
            "merchant_id"
        ).agg(
            F.count("*").alias("merchant_txn_count_1h"),
            F.avg("amount").alias("merchant_avg_amount_1h"),
            F.stddev("amount").alias("merchant_std_amount_1h")
        )

    # Device fingerprint features
    device_features = transactions_stream \
        .groupBy("device_id").agg(
            F.countDistinct("user_id").alias("users_per_device"),
            F.countDistinct("ip_address").alias("ips_per_device")
        )

    # Cross-feature anomaly detection
    enriched_stream = transactions_stream \
        .join(user_velocity, ["user_id", "window"]) \
        .join(merchant_stats, ["merchant_id", "window"]) \
        .join(device_features, "device_id") \
        .withColumn(
            "amount_vs_merchant_avg_ratio",
            F.col("amount") / F.col("merchant_avg_amount_1h")
        ).withColumn(
            "is_velocity_spike",
            (F.col("transactions_5min") > 10).cast("int")
        ).withColumn(
            "is_multi_user_device",
            (F.col("users_per_device") > 3).cast("int")
        ).withColumn(
            "fraud_score",
            (
                F.col("is_velocity_spike") * 0.4 +
                F.col("is_multi_user_device") * 0.3 +
                (F.col("amount_vs_merchant_avg_ratio") > 5).cast("int") * 0.3
            )
        )

    return enriched_stream

# Write features to online store for immediate inference
fraud_features = create_fraud_detection_features(transaction_stream)

fraud_features.writeStream \
    .foreachBatch(lambda df, epoch_id: write_to_redis(df)) \
    .outputMode("append") \
    .option("checkpointLocation", "s3://checkpoints/fraud_features/") \
    .start()
```

---

### Example 3: Recommendation System Features

**Scenario:** Personalized product recommendations

**Hybrid Feature Pipeline (Batch + Streaming + On-Demand):**

```python
class RecommendationFeatures:
    """
    Multi-stage feature pipeline for recommendations
    """

    def __init__(self, feature_store: FeatureStore):
        self.feature_store = feature_store
        self.user_embeddings = self._load_user_embeddings()
        self.product_embeddings = self._load_product_embeddings()

    def get_features_for_recommendation(
        self,
        user_id: str,
        candidate_products: List[str],
        context: dict
    ) -> pd.DataFrame:
        """
        Combine batch, streaming, and on-demand features
        """
        # 1. Batch features from feature store (offline, precomputed)
        batch_features = self.feature_store.get_online_features(
            entity_rows=[{"user_id": user_id}],
            features=[
                "user_features:favorite_categories",
                "user_features:avg_price_range",
                "user_features:purchase_frequency",
                "user_features:lifetime_value"
            ]
        ).to_dict()

        # 2. Streaming features (real-time behavioral signals)
        streaming_features = self._get_streaming_features(user_id)

        # 3. On-demand features (request context + candidate products)
        recommendations = []

        for product_id in candidate_products:
            # Product features
            product_features = self._get_product_features(product_id)

            # Contextual features
            contextual = {
                'hour_of_day': datetime.now().hour,
                'is_weekend': datetime.now().weekday() >= 5,
                'device_type': context.get('device_type', 'unknown'),
                'page_context': context.get('page', 'home')
            }

            # Interaction features (computed on-demand)
            user_emb = self.user_embeddings.get(user_id, np.zeros(128))
            product_emb = self.product_embeddings.get(product_id, np.zeros(128))

            interaction_features = {
                'user_product_similarity': cosine_similarity(user_emb, product_emb),
                'price_vs_user_avg': product_features['price'] / batch_features['avg_price_range'],
                'category_match': int(product_features['category'] in batch_features['favorite_categories'])
            }

            # Combine all features
            all_features = {
                'user_id': user_id,
                'product_id': product_id,
                **batch_features,
                **streaming_features,
                **product_features,
                **contextual,
                **interaction_features
            }

            recommendations.append(all_features)

        return pd.DataFrame(recommendations)

    def _get_streaming_features(self, user_id: str) -> dict:
        """Get real-time behavioral features from Redis"""
        import redis

        r = redis.Redis(host='localhost', port=6379)
        features = r.hgetall(f"user_realtime:{user_id}")

        return {
            'views_last_hour': int(features.get(b'views_last_hour', 0)),
            'clicks_last_hour': int(features.get(b'clicks_last_hour', 0)),
            'cart_items_count': int(features.get(b'cart_items', 0)),
            'session_duration_minutes': float(features.get(b'session_duration', 0))
        }
```

---

## üîß Code Examples

### Complete Feature Pipeline with Airflow Orchestration

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from datetime import datetime, timedelta
from feast import FeatureStore
import pandas as pd

# Feature computation logic
def compute_user_features(execution_date: str, **context):
    """
    Compute batch user features for a specific date
    """
    from pyspark.sql import SparkSession

    spark = SparkSession.builder.appName("UserFeatures").getOrCreate()

    # Load raw data
    transactions = spark.read.parquet(f"s3://raw-data/transactions/date={execution_date}/")
    user_profiles = spark.read.parquet("s3://raw-data/user_profiles/")

    # Compute features
    user_features = transactions.groupBy("user_id").agg(
        F.count("*").alias("transaction_count"),
        F.sum("amount").alias("total_spent"),
        F.avg("amount").alias("avg_transaction_size"),
        F.collect_set("category").alias("categories_purchased")
    )

    # Join with user profiles
    enriched_features = user_features.join(user_profiles, "user_id", "left")

    # Write to S3
    output_path = f"s3://feature-store/user_features/date={execution_date}/"
    enriched_features.write.mode("overwrite").parquet(output_path)

    return output_path


def validate_features(execution_date: str, **context):
    """
    Validate computed features meet quality standards
    """
    feature_path = f"s3://feature-store/user_features/date={execution_date}/"
    df = pd.read_parquet(feature_path)

    # Validation checks
    assert df['user_id'].is_unique, "Duplicate user_ids found"
    assert df['transaction_count'].min() >= 0, "Negative transaction counts"
    assert df['total_spent'].min() >= 0, "Negative spending amounts"
    assert len(df) > 1000, f"Too few users: {len(df)}"

    print(f"‚úÖ Validation passed: {len(df)} users, {df.columns.tolist()}")
    return True


def materialize_to_feast(execution_date: str, **context):
    """
    Materialize features to Feast online store
    """
    store = FeatureStore(repo_path="/opt/feast/")

    # Materialize features up to execution_date
    store.materialize(
        start_date=datetime.fromisoformat(execution_date) - timedelta(days=1),
        end_date=datetime.fromisoformat(execution_date)
    )

    print(f"‚úÖ Materialized features to online store for {execution_date}")


# Define Airflow DAG
default_args = {
    'owner': 'ml-platform',
    'depends_on_past': True,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'user_feature_pipeline',
    default_args=default_args,
    description='Daily batch user feature computation',
    schedule_interval='@daily',
    catchup=False,
    tags=['features', 'batch', 'production']
) as dag:

    compute_task = PythonOperator(
        task_id='compute_user_features',
        python_callable=compute_user_features,
        op_kwargs={'execution_date': '{{ ds }}'}
    )

    validate_task = PythonOperator(
        task_id='validate_features',
        python_callable=validate_features,
        op_kwargs={'execution_date': '{{ ds }}'}
    )

    materialize_task = PythonOperator(
        task_id='materialize_to_feast',
        python_callable=materialize_to_feast,
        op_kwargs={'execution_date': '{{ ds }}'}
    )

    # Define dependencies
    compute_task >> validate_task >> materialize_task
```

---

## ‚úÖ Best Practices

### 1. Define Features as Code
- Store feature definitions in version control (Git)
- Use declarative frameworks (Feast, Tecton)
- Test feature transformations in CI/CD
- Document feature semantics and ownership

### 2. Maintain Training-Serving Parity
- Write feature logic once, use everywhere
- Validate features match across environments
- Use feature stores for consistent serving
- Monitor for drift and skew

### 3. Implement Point-in-Time Correctness
- Never use future data in training
- Use feature stores' temporal joins
- Validate label-feature temporal alignment
- Test for data leakage systematically

### 4. Version Features with Models
- Track feature versions in model metadata
- Support backward compatibility
- Deprecate features gracefully
- Maintain feature changelog

### 5. Monitor Feature Quality
- Schema validation on every batch
- Distribution drift detection
- Freshness monitoring (SLAs)
- Null/missing value alerts

### 6. Optimize for Performance
- Batch: Partition by date, use columnar formats (Parquet)
- Streaming: Minimize state size, use windowing
- Online: Cache frequently accessed features, use Redis/DynamoDB
- Precompute expensive features offline

### 7. Design for Reusability
- Create feature libraries/registries
- Share features across teams and models
- Document feature definitions thoroughly
- Implement feature discovery tools

### 8. Test Feature Pipelines
- Unit tests for transformation logic
- Integration tests for end-to-end flows
- Data quality tests in production
- Regression tests for feature values

---

## ‚ö†Ô∏è Common Pitfalls

### 1. Training-Serving Skew (Most Common!)

**Problem:** Different feature implementations in training and serving lead to silent model degradation.

**Example:**
```python
# ‚ùå BAD: Different implementations
# Training (Python)
train_features['avg_purchase'] = df.groupby('user_id')['amount'].mean().fillna(0)

# Serving (Java)
double avgPurchase = transactions.isEmpty() ? null : mean(transactions);
// Different null handling!
```

**Solution:**
```python
# ‚úÖ GOOD: Single source of truth
class Features:
    @staticmethod
    def avg_purchase(transactions: List[float]) -> float:
        return sum(transactions) / len(transactions) if transactions else 0.0

# Use in both training and serving
train_features['avg_purchase'] = df['transactions'].apply(Features.avg_purchase)
serving_feature = Features.avg_purchase(user_transactions)
```

---

### 2. Data Leakage Through Point-in-Time Violations

**Problem:** Using data from the future when computing training features.

**Example:**
```python
# ‚ùå BAD: Computes features using ALL data, including future
def create_features_wrong(df):
    # This uses global statistics including future data!
    df['user_avg_purchase'] = df.groupby('user_id')['amount'].transform('mean')
    return df

labels_sept = df[df['date'] == '2025-09-01']
features = create_features_wrong(df)  # Uses Oct, Nov data!
```

**Solution:**
```python
# ‚úÖ GOOD: Point-in-time correct features
def create_features_correct(df, as_of_date):
    # Only use data up to as_of_date
    past_data = df[df['date'] <= as_of_date]
    user_stats = past_data.groupby('user_id')['amount'].mean()

    # Join only for as_of_date
    df_asof = df[df['date'] == as_of_date]
    df_asof['user_avg_purchase'] = df_asof['user_id'].map(user_stats)

    return df_asof
```

---

### 3. Ignoring Feature Staleness

**Problem:** Serving stale features in production without monitoring freshness.

**Example:**
```python
# ‚ùå BAD: No freshness check
def get_features(user_id):
    features = redis.hgetall(f"user:{user_id}")
    return features  # Could be days old!
```

**Solution:**
```python
# ‚úÖ GOOD: Validate freshness and fallback
def get_features(user_id, max_age_hours=24):
    features = redis.hgetall(f"user:{user_id}")

    if not features:
        return get_default_features(user_id)

    # Check freshness
    feature_time = datetime.fromisoformat(features['timestamp'])
    age_hours = (datetime.now() - feature_time).total_seconds() / 3600

    if age_hours > max_age_hours:
        logger.warning(f"Stale features for {user_id}: {age_hours:.1f}h old")
        # Fallback or recompute
        return compute_features_on_demand(user_id)

    return features
```

---

### 4. Not Handling Missing Features Gracefully

**Problem:** Model crashes when features are missing in production.

**Example:**
```python
# ‚ùå BAD: Assumes all features exist
def predict(user_id):
    features = feature_store.get_online_features(user_id)
    # KeyError if feature missing!
    vector = [features['f1'], features['f2'], features['f3']]
    return model.predict([vector])
```

**Solution:**
```python
# ‚úÖ GOOD: Default values for missing features
def predict(user_id):
    features = feature_store.get_online_features(user_id)

    # Define defaults matching training
    defaults = {'f1': 0.0, 'f2': 0.0, 'f3': 0.0}

    vector = [features.get(f, defaults[f]) for f in ['f1', 'f2', 'f3']]

    # Log if defaults were used
    if any(f not in features for f in ['f1', 'f2', 'f3']):
        logger.warning(f"Missing features for {user_id}, using defaults")

    return model.predict([vector])
```

---

### 5. Duplicating Feature Logic Across Teams

**Problem:** Multiple teams reimplement same features, leading to inconsistencies.

**Example:**
```python
# Team A's feature
def compute_user_activity_team_a(user_id):
    return db.query(f"SELECT COUNT(*) FROM events WHERE user_id='{user_id}' AND date >= NOW() - 30")

# Team B's feature (slightly different!)
def compute_user_activity_team_b(user_id):
    return db.query(f"SELECT COUNT(*) FROM events WHERE user_id='{user_id}' AND timestamp >= NOW() - INTERVAL '30 days'")
    # Different: uses timestamp vs date!
```

**Solution:**
```python
# ‚úÖ GOOD: Centralized feature registry
# features/user_activity.py
@feast.feature
class UserActivity:
    """
    Shared feature definition used by all teams
    """
    name = "user_activity_30d"
    entity = "user"
    description = "Count of user events in last 30 days"
    owner = "ml-platform-team"

    def compute(self, events_df, as_of_date):
        cutoff_date = as_of_date - timedelta(days=30)
        return events_df[events_df['date'] >= cutoff_date].groupby('user_id').size()

# All teams use the same definition
feature_store.get_online_features(["UserActivity"])
```

---

### 6. Not Testing for Distribution Shift

**Problem:** Features drift in production but no monitoring in place.

**Solution:** Implement continuous monitoring (see Practical Examples section for FeatureMonitor class).

---

## üèãÔ∏è Hands-On Exercises

### Exercise 1: Build End-to-End Feature Pipeline with Feast

**Difficulty:** Advanced
**Time:** 8-10 hours
**Objective:** Implement complete feature engineering workflow with training-serving parity

**Tasks:**

1. **Set up Feast:**
   - Install Feast: `pip install feast[aws]`
   - Initialize repo: `feast init feature_repo`
   - Configure offline store (S3/Parquet) and online store (Redis/DynamoDB)

2. **Define features:**
   - Create user transaction features (7d, 30d, 90d aggregations)
   - Create product features (popularity, rating, category embeddings)
   - Define feature views with proper TTLs

3. **Implement batch pipeline:**
   - Compute historical features with Spark
   - Write to offline store (S3)
   - Schedule with Airflow (daily at 2 AM)

4. **Materialize to online store:**
   - Incremental materialization
   - Monitor materialization lag

5. **Build training pipeline:**
   - Use `get_historical_features` with point-in-time joins
   - Create training dataset for 3 months of data
   - Validate no data leakage

6. **Build serving pipeline:**
   - REST API using FastAPI
   - Fetch online features for inference
   - Measure p99 latency (<20ms)

**Validation:**
```python
# Test training-serving consistency
train_features = store.get_historical_features(
    entity_df=test_entities,
    features=["user_features:transactions_30d"]
)

serve_features = store.get_online_features(
    entity_rows=[{"user_id": "test_user"}],
    features=["user_features:transactions_30d"]
)

# Values should match for same timestamp
assert train_features.iloc[0]['transactions_30d'] == serve_features['transactions_30d'][0]
```

---

### Exercise 2: Detect and Fix Training-Serving Skew

**Difficulty:** Intermediate
**Time:** 4-5 hours
**Objective:** Identify and resolve feature inconsistencies

**Scenario:**
Your model performs well offline (AUC: 0.89) but poorly online (AUC: 0.71). Investigate training-serving skew.

**Given Code (Buggy):**

**Training:**
```python
def compute_training_features(df):
    # Compute features for training
    df['user_avg_purchase'] = df.groupby('user_id')['amount'].transform('mean')
    df['purchase_frequency'] = df.groupby('user_id').size()
    df['days_since_last'] = (df['current_date'] - df.groupby('user_id')['date'].transform('max')).dt.days
    return df
```

**Serving:**
```python
def compute_serving_features(user_id):
    transactions = db.query(f"SELECT * FROM transactions WHERE user_id = '{user_id}'")

    features = {}
    features['user_avg_purchase'] = transactions['amount'].mean()  # Uses .mean() instead of .transform('mean')
    features['purchase_frequency'] = len(transactions)  # Missing .size()
    features['days_since_last'] = (datetime.now() - transactions['date'].max()).days  # Different time reference!

    return features
```

**Tasks:**
1. **Identify all sources of skew** (at least 5 issues)
2. **Create unified feature definitions**
3. **Write unit tests** to ensure parity
4. **Implement monitoring** to detect future skew

**Expected Findings:**
- Different aggregation methods
- Different null handling
- Different time references (batch date vs now())
- Missing features
- Type mismatches

---

### Exercise 3: Implement Point-in-Time Correct Features

**Difficulty:** Advanced
**Time:** 5-6 hours
**Objective:** Build feature pipeline that prevents data leakage

**Scenario:**
Create training dataset for subscription churn model with proper temporal splits.

**Requirements:**
- Labels: Did user churn in 30 days after observation date?
- Features: User behavior in 90 days BEFORE observation date
- NO future data should be used

**Tasks:**

1. **Implement point-in-time feature computation:**
```python
def compute_features_pit(user_id: str, as_of_date: datetime) -> dict:
    """
    Compute features using ONLY data before as_of_date
    """
    # TODO: Implement
    pass
```

2. **Create training dataset:**
   - Generate observations every 7 days for 6 months
   - For each observation, compute features and label
   - Validate temporal correctness

3. **Test for leakage:**
```python
def test_no_leakage():
    """Ensure no future data is used"""
    observation_date = datetime(2025, 9, 1)
    features = compute_features_pit('user_123', observation_date)

    # Features should only use data before Sept 1
    # Check: are any features suspiciously perfect predictors?
    # Check: do feature values change when adding future data?
```

4. **Implement with Feast's historical features API**

---

### Exercise 4: Build Feature Monitoring Dashboard

**Difficulty:** Intermediate
**Time:** 6-7 hours
**Objective:** Monitor feature quality and drift in production

**Requirements:**
- Real-time metrics: freshness, completeness, drift
- Alerting on SLA violations
- Visualization of feature distributions

**Tasks:**

1. **Implement monitoring metrics:**
   - Feature freshness (max age)
   - Feature completeness (% non-null)
   - Distribution drift (KS-test)
   - Correlation drift

2. **Build dashboard:**
   - Use Grafana or Streamlit
   - Display metrics per feature
   - Historical trends (7 days, 30 days)

3. **Set up alerting:**
   - PagerDuty integration
   - Alert conditions:
     - Freshness > 24 hours
     - Completeness < 95%
     - Drift p-value < 0.01

4. **Create runbook for on-call:**
   - How to investigate drift alerts
   - How to roll back features
   - How to trigger re-materialization

**Deliverables:**
- Monitoring code
- Dashboard screenshots
- Sample alert and resolution

---

## üìö Further Reading

### Essential Books

1. **"Feature Engineering for Machine Learning" by Alice Zheng & Amanda Casari**
   - Comprehensive guide to feature engineering techniques
   - Practical examples with code
   - O'Reilly Media, 2018

2. **"Designing Data-Intensive Applications" by Martin Kleppmann**
   - Chapter on derived data and batch/stream processing
   - Essential for understanding feature pipeline architecture

3. **"Machine Learning Design Patterns" by Valliappa Lakshmanan, Sara Robinson, Michael Munn**
   - Pattern: Feature Store
   - Pattern: Windowed Aggregations
   - Pattern: Embeddings

### Key Papers

1. **"Towards ML Engineering: A Brief History Of TensorFlow Extended (TFX)"** (Baylor et al., 2017)
   - Google's production ML pipelines
   - Feature transformation best practices
   - [Paper](https://arxiv.org/abs/2010.02013)

2. **"Feast: Bridging ML Models and Data"** (Feast team, 2020)
   - Architecture of open-source feature store
   - [Documentation](https://docs.feast.dev/)

3. **"Feature Stores for ML"** (Hopsworks, 2021)
   - Comparison of feature store architectures
   - [Blog](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines)

### Blogs and Documentation

1. **Uber Engineering: Michelangelo Feature Store**
   - ["Meet Michelangelo: Uber's Machine Learning Platform"](https://www.uber.com/blog/michelangelo-machine-learning-platform/)
   - Production feature engineering at scale

2. **Airbnb Engineering: Zipline**
   - ["Zipline: Airbnb's Machine Learning Data Management Platform"](https://medium.com/airbnb-engineering/zipline-airbnbs-machine-learning-data-management-platform-ba789e6cfc3)
   - Training-serving consistency solutions

3. **Netflix: Feature Engineering at Scale**
   - ["Human-in-the-Loop for Data Annotation"](https://netflixtechblog.com/notebook-innovation-591ee3221233)

4. **Feast Documentation**
   - [Official Feast Docs](https://docs.feast.dev/)
   - [Feast on AWS](https://docs.feast.dev/tutorials/aws)

5. **Tecton Documentation**
   - [Tecton Feature Platform](https://www.tecton.ai/docs/)
   - [MLOps with Feature Stores](https://www.tecton.ai/blog/what-is-a-feature-store/)

### Video Courses

1. **"Feature Engineering" (Coursera - Deeplearning.AI)**
   - Taught by Andrew Ng
   - Practical techniques and case studies

2. **"Building Production Machine Learning Pipelines" (Coursera - Google Cloud)**
   - TFX and feature transformation
   - Hands-on labs

3. **"Feature Stores in Production" (YouTube - Feast + Tecton)**
   - [ApplyConf 2022 Talks](https://www.youtube.com/playlist?list=PLr)

### Tools and Frameworks

1. **Open Source Feature Stores:**
   - [Feast](https://feast.dev/) - Most popular open-source
   - [Hopsworks](https://www.hopsworks.ai/) - Enterprise-grade open source
   - [Feathr](https://github.com/feathr-ai/feathr) - LinkedIn's feature store

2. **Managed Feature Platforms:**
   - [Tecton](https://www.tecton.ai/) - Feature platform as a service
   - [AWS SageMaker Feature Store](https://aws.amazon.com/sagemaker/feature-store/)
   - [Databricks Feature Store](https://docs.databricks.com/machine-learning/feature-store/index.html)

---

## üìù Key Takeaways

1. **Feature Engineering is the Most Impactful Part of ML**
   - More impact than model selection or hyperparameter tuning
   - Domain knowledge encoded in features drives model performance
   - Requires close collaboration between data engineers and data scientists

2. **Training-Serving Consistency is Critical**
   - Write feature transformation logic once, use everywhere
   - Different implementations ‚Üí training-serving skew ‚Üí model degradation
   - Feature stores solve this with centralized definitions and dual storage

3. **Feature Stores are Central Infrastructure**
   - Not optional for production ML at scale
   - Provide offline (training) and online (serving) storage
   - Enable feature reuse, versioning, monitoring, and governance

4. **Point-in-Time Correctness Prevents Data Leakage**
   - Never use future data when computing training features
   - Use temporal joins that respect label timestamps
   - Feast and other feature stores handle this automatically

5. **Three Types of Feature Transformations**
   - **Batch:** Complex aggregations, high throughput, scheduled
   - **Streaming:** Real-time signals, low latency, continuous
   - **On-Demand:** Contextual features, request-time computation

6. **Monitoring is Essential**
   - Schema validation: Prevent type errors and missing fields
   - Distribution drift: Detect when features change significantly
   - Freshness: Alert on stale features violating SLAs
   - Completeness: Track null rates and data quality

7. **Feature Versioning Prevents Breakage**
   - Version features alongside models
   - Support backward compatibility during transitions
   - Maintain changelog of feature definition changes
   - Enable safe rollbacks

8. **Performance Optimization Varies by Use Case**
   - Batch: Partition by date, use Parquet, leverage Spark
   - Streaming: Minimize state, use windowing, checkpoint frequently
   - Online: Cache in Redis/DynamoDB, precompute expensive features

9. **Reusability Through Feature Libraries**
   - Build feature catalogs/registries
   - Share features across teams and models
   - Comprehensive documentation and discovery tools
   - Ownership and SLA tracking

10. **Testing Feature Pipelines is Non-Negotiable**
    - Unit tests for transformation logic
    - Integration tests for end-to-end flows
    - Data quality tests in production
    - Regression tests for feature value stability

---

## üìù Notes Section

### My Key Insights:
-

### Questions to Explore Further:
-

### How This Applies to My Work:
-

### Tools to Investigate:
-

### Action Items:
-

---

## üîó Related Concepts

- [[01. Data Generation and Collection for ML|Previous: Data Collection]]
- [[02. Data Storage and Versioning Strategies|Previous: Data Versioning]]
- [[04. Model Training and Serving Data Flows|Next: Training and Serving]]
- [[01_Projects/DEforAI/Course/01. Introduction to DE for AI-ML/README|Chapter 1: Introduction]]

---

*Created: October 18, 2025*
*Last Updated: October 18, 2025*
*Status: ‚úÖ Completed - Ready for study*
