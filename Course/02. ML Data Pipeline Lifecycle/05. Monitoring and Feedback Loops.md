# 05. Monitoring and Feedback Loops

**Chapter:** ML Data Pipeline Lifecycle
**Topic:** Observability, drift detection, and continuous learning in production ML systems

---

## üìã Overview

Production ML systems are dynamic entities that require continuous observation and adaptation. Models degrade over time as data distributions shift, user behavior evolves, and business contexts change. This subchapter explores comprehensive monitoring strategies, drift detection techniques, feedback loop design, and continuous learning approaches to maintain model reliability and performance.

**Key Industry Reality (2024-2025):** The focus has shifted from "preventing" drift to rapidly detecting and managing it through automated monitoring and retraining. Research emphasizes that ML observability is now a necessity, not an option, with feedback loops‚Äîboth beneficial and degenerative‚Äîfundamentally altering system behavior over time.

**Critical Challenge:** Hidden feedback loops can cause models to amplify existing biases and degrade silently. Without proper observability, the effects may not appear during testing and manifest only after weeks or months in production.

---

## üéØ Learning Objectives

After completing this subchapter, you will be able to:
- Design comprehensive monitoring strategies for ML systems in production
- Detect and diagnose different types of drift (data drift, concept drift, prediction drift)
- Implement automated alerting and anomaly detection for model degradation
- Identify and manage feedback loops (both beneficial and degenerative)
- Build continuous learning systems with safe retraining workflows
- Use observability tools (Prometheus, Evidently AI, Grafana) for ML systems
- Perform root cause analysis when models fail
- Establish SLAs and success criteria for production models

---

## üìö Core Concepts

### 1. Types of Model Drift

#### A. Data Drift (Covariate Shift)

**Definition:** Changes in the distribution of input features P(X), while the relationship P(Y|X) remains constant.

**Mathematical:** P_train(X) ‚â† P_production(X), but P(Y|X) is stable

**Example:**
- E-commerce: User demographics shift (younger users during holiday season)
- Finance: Economic conditions change (inflation, interest rates)
- Healthcare: Patient population changes (aging demographics)

**Detection Methods:**

```python
from scipy import stats
import numpy as np

class DataDriftDetector:
    """
    Detect distribution shifts in input features
    """

    def __init__(self, reference_data: np.ndarray):
        """
        Args:
            reference_data: Training or baseline data distribution
        """
        self.reference_data = reference_data

    def detect_drift_ks_test(
        self,
        current_data: np.ndarray,
        significance_level: float = 0.05
    ) -> dict:
        """
        Kolmogorov-Smirnov test for distribution shift

        Returns:
            Dict with drift detected (bool) and p-value
        """
        # Two-sample KS test
        ks_statistic, p_value = stats.ks_2samp(
            self.reference_data,
            current_data
        )

        drift_detected = p_value < significance_level

        return {
            "drift_detected": drift_detected,
            "p_value": p_value,
            "ks_statistic": ks_statistic,
            "interpretation": (
                "Significant drift detected" if drift_detected
                else "No significant drift"
            )
        }

    def detect_drift_psi(
        self,
        current_data: np.ndarray,
        num_bins: int = 10,
        psi_threshold: float = 0.2
    ) -> dict:
        """
        Population Stability Index (PSI) for drift detection

        PSI thresholds:
        - < 0.1: No significant change
        - 0.1-0.2: Small change
        - > 0.2: Significant change (retraining recommended)
        """
        # Create bins from reference data
        bins = np.histogram_bin_edges(self.reference_data, bins=num_bins)

        # Calculate distributions
        ref_hist, _ = np.histogram(self.reference_data, bins=bins)
        curr_hist, _ = np.histogram(current_data, bins=bins)

        # Normalize to probabilities
        ref_probs = ref_hist / len(self.reference_data)
        curr_probs = curr_hist / len(current_data)

        # Avoid division by zero
        ref_probs = np.where(ref_probs == 0, 0.0001, ref_probs)
        curr_probs = np.where(curr_probs == 0, 0.0001, curr_probs)

        # Calculate PSI
        psi = np.sum((curr_probs - ref_probs) * np.log(curr_probs / ref_probs))

        drift_detected = psi > psi_threshold

        return {
            "psi": psi,
            "drift_detected": drift_detected,
            "severity": (
                "High" if psi > 0.2
                else "Medium" if psi > 0.1
                else "Low"
            )
        }


# Usage
detector = DataDriftDetector(reference_data=training_features[:, 0])

# Check production data
drift_results = detector.detect_drift_ks_test(production_features[:, 0])

if drift_results['drift_detected']:
    print(f"‚ö†Ô∏è  Data drift detected!")
    print(f"   KS statistic: {drift_results['ks_statistic']:.4f}")
    print(f"   p-value: {drift_results['p_value']:.4f}")
    # Trigger retraining workflow
```

**Impact:** Model may still be accurate if P(Y|X) hasn't changed, but performance may degrade if features move to unseen ranges.

---

#### B. Concept Drift

**Definition:** Changes in the relationship between features and target P(Y|X), even if input distribution P(X) remains stable.

**Mathematical:** P(Y|X) changes over time

**Example:**
- Fraud detection: Fraudsters adapt tactics to evade detection
- Recommendation systems: User preferences evolve (seasonal trends)
- Credit scoring: Economic conditions change default risk patterns

**Detection Methods:**

```python
from sklearn.metrics import roc_auc_score
import pandas as pd

class ConceptDriftDetector:
    """
    Monitor model performance over time to detect concept drift
    """

    def __init__(self, baseline_performance: float, threshold: float = 0.05):
        """
        Args:
            baseline_performance: Expected model performance (e.g., AUC from validation)
            threshold: Acceptable degradation before alerting
        """
        self.baseline_performance = baseline_performance
        self.threshold = threshold
        self.performance_history = []

    def check_performance_drift(
        self,
        y_true: np.ndarray,
        y_pred_proba: np.ndarray,
        window_size: int = 1000
    ) -> dict:
        """
        Monitor rolling window performance

        Args:
            y_true: Ground truth labels (from delayed feedback)
            y_pred_proba: Model predictions
            window_size: Size of rolling window

        Returns:
            Dict with drift status and metrics
        """
        # Compute current performance
        current_auc = roc_auc_score(y_true, y_pred_proba)

        # Store in history
        self.performance_history.append({
            "timestamp": pd.Timestamp.now(),
            "auc": current_auc,
            "num_samples": len(y_true)
        })

        # Calculate degradation
        degradation = self.baseline_performance - current_auc
        drift_detected = degradation > self.threshold

        # Trend analysis (last 7 days)
        if len(self.performance_history) >= 7:
            recent_aucs = [h['auc'] for h in self.performance_history[-7:]]
            trend = "declining" if recent_aucs[-1] < recent_aucs[0] else "stable"
        else:
            trend = "insufficient_data"

        return {
            "drift_detected": drift_detected,
            "current_auc": current_auc,
            "baseline_auc": self.baseline_performance,
            "degradation": degradation,
            "trend": trend,
            "recommendation": (
                "Retrain immediately" if drift_detected
                else "Continue monitoring"
            )
        }


# Usage with delayed feedback
detector = ConceptDriftDetector(baseline_performance=0.89, threshold=0.05)

# Collect predictions and delayed ground truth labels
results = detector.check_performance_drift(
    y_true=ground_truth_labels,
    y_pred_proba=model_predictions
)

if results['drift_detected']:
    print(f"‚ö†Ô∏è  Concept drift detected!")
    print(f"   Performance degraded from {results['baseline_auc']:.3f} to {results['current_auc']:.3f}")
    print(f"   Recommendation: {results['recommendation']}")
```

**Impact:** Model accuracy degrades even if features look "normal." Requires retraining with fresh data.

---

#### C. Label Drift (Prior Probability Shift)

**Definition:** Changes in the distribution of the target variable P(Y), while P(X|Y) remains constant.

**Mathematical:** P_train(Y) ‚â† P_production(Y)

**Example:**
- Spam detection: Proportion of spam vs. legitimate emails changes
- Churn prediction: Economic downturn increases overall churn rate
- Fraud: Black Friday sees spike in fraud attempts

**Detection:**

```python
def detect_label_drift(
    train_labels: np.ndarray,
    production_labels: np.ndarray,
    threshold: float = 0.1
) -> dict:
    """
    Detect changes in class distribution

    Args:
        train_labels: Training set labels
        production_labels: Production labels (requires delayed feedback)
        threshold: Maximum acceptable change in class proportion

    Returns:
        Dict with drift status
    """
    # Calculate class proportions
    train_positive_rate = train_labels.mean()
    prod_positive_rate = production_labels.mean()

    # Calculate change
    change = abs(prod_positive_rate - train_positive_rate)
    drift_detected = change > threshold

    return {
        "drift_detected": drift_detected,
        "train_positive_rate": train_positive_rate,
        "production_positive_rate": prod_positive_rate,
        "change": change,
        "severity": "high" if change > 0.2 else "medium" if change > 0.1 else "low"
    }
```

**Impact:** May need to recalibrate model or retrain with updated class distribution.

---

#### D. Prediction Drift

**Definition:** Changes in model output distribution P(≈∑) without access to ground truth.

**Why Useful:** Can detect issues *before* labels arrive (early warning signal).

**Example:**
- Sudden spike in fraud predictions (0.9 ‚Üí 0.3 average score)
- All users predicted as low churn risk (distribution collapses)

**Detection:**

```python
from scipy import stats

class PredictionDriftDetector:
    """
    Monitor prediction distributions over time
    """

    def __init__(self, baseline_predictions: np.ndarray):
        self.baseline_mean = baseline_predictions.mean()
        self.baseline_std = baseline_predictions.std()
        self.baseline_dist = baseline_predictions

    def detect_drift(
        self,
        current_predictions: np.ndarray,
        method: str = "ks_test"
    ) -> dict:
        """
        Detect if prediction distribution has changed
        """
        if method == "ks_test":
            ks_stat, p_value = stats.ks_2samp(
                self.baseline_dist,
                current_predictions
            )
            drift_detected = p_value < 0.05

            return {
                "drift_detected": drift_detected,
                "p_value": p_value,
                "ks_statistic": ks_stat
            }

        elif method == "mean_shift":
            current_mean = current_predictions.mean()
            z_score = (current_mean - self.baseline_mean) / self.baseline_std

            drift_detected = abs(z_score) > 3  # 3 sigma rule

            return {
                "drift_detected": drift_detected,
                "baseline_mean": self.baseline_mean,
                "current_mean": current_mean,
                "z_score": z_score
            }


# Usage (no labels required!)
baseline_preds = model.predict_proba(validation_set)[:, 1]
detector = PredictionDriftDetector(baseline_predictions=baseline_preds)

# Monitor daily production predictions
daily_preds = model.predict_proba(production_batch)[:, 1]
drift = detector.detect_drift(daily_preds, method="mean_shift")

if drift['drift_detected']:
    print("‚ö†Ô∏è  Prediction drift detected - investigate immediately!")
```

---

### 2. Monitoring Architecture

#### Comprehensive Monitoring Stack

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BUSINESS METRICS                          ‚îÇ
‚îÇ  Revenue, Conversions, CTR, Customer Satisfaction           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üë
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MODEL METRICS                             ‚îÇ
‚îÇ  Accuracy, AUC, Precision, Recall, F1, Calibration          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üë
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PREDICTION METRICS                        ‚îÇ
‚îÇ  Prediction Distribution, Confidence Scores, Outliers       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üë
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATA METRICS                              ‚îÇ
‚îÇ  Feature Distribution, Missing Values, Drift (KS, PSI)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üë
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SYSTEM METRICS                            ‚îÇ
‚îÇ  Latency (p50, p95, p99), Throughput, Errors, Resource Use ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Implementation:**

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# Define Prometheus metrics
prediction_counter = Counter(
    'model_predictions_total',
    'Total number of predictions',
    ['model_name', 'model_version']
)

prediction_latency = Histogram(
    'model_prediction_latency_seconds',
    'Model prediction latency',
    ['model_name'],
    buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
)

prediction_score = Gauge(
    'model_prediction_score',
    'Distribution of prediction scores',
    ['model_name']
)

feature_drift_gauge = Gauge(
    'feature_drift_psi',
    'PSI score for feature drift',
    ['feature_name']
)

class MonitoredModel:
    """
    Wrapper around ML model with comprehensive monitoring
    """

    def __init__(self, model, model_name: str, version: str):
        self.model = model
        self.model_name = model_name
        self.version = version

    def predict(self, features: np.ndarray) -> np.ndarray:
        """
        Predict with monitoring instrumentation
        """
        start_time = time.time()

        try:
            # Make prediction
            predictions = self.model.predict_proba(features)[:, 1]

            # Record metrics
            latency = time.time() - start_time
            prediction_latency.labels(model_name=self.model_name).observe(latency)
            prediction_counter.labels(
                model_name=self.model_name,
                model_version=self.version
            ).inc(len(predictions))

            # Record prediction distribution
            avg_score = predictions.mean()
            prediction_score.labels(model_name=self.model_name).set(avg_score)

            # Log for further analysis
            self._log_predictions(features, predictions)

            return predictions

        except Exception as e:
            # Record error
            error_counter.labels(model_name=self.model_name).inc()
            raise e

    def _log_predictions(self, features, predictions):
        """Log predictions and features for drift monitoring"""
        # Send to data warehouse or feature store
        pass


# Grafana Dashboard Query (PromQL)
"""
# Prediction latency p95
histogram_quantile(0.95, rate(model_prediction_latency_seconds_bucket[5m]))

# Throughput (predictions/sec)
rate(model_predictions_total[1m])

# Average prediction score trend
avg_over_time(model_prediction_score[1h])
"""
```

---

### 3. Feedback Loops

#### A. Beneficial Feedback Loops

**Definition:** Model outputs improve data quality or model performance over time.

**Example: Active Learning**

```python
class ActiveLearningFeedbackLoop:
    """
    Use model uncertainty to prioritize labeling high-value examples
    """

    def __init__(self, model, labeling_budget: int = 100):
        self.model = model
        self.labeling_budget = labeling_budget

    def select_examples_for_labeling(
        self,
        unlabeled_pool: np.ndarray
    ) -> List[int]:
        """
        Select most uncertain examples for human labeling

        Strategy: Query by Committee or Uncertainty Sampling
        """
        # Get predictions
        predictions = self.model.predict_proba(unlabeled_pool)

        # Compute uncertainty (entropy)
        entropy = -np.sum(predictions * np.log(predictions + 1e-10), axis=1)

        # Select top-k most uncertain
        uncertain_indices = np.argsort(entropy)[-self.labeling_budget:]

        return uncertain_indices.tolist()

    def retrain_with_feedback(
        self,
        labeled_examples: np.ndarray,
        labels: np.ndarray
    ):
        """
        Retrain model with newly labeled data
        """
        # Add to training set
        X_train_augmented = np.vstack([self.X_train, labeled_examples])
        y_train_augmented = np.hstack([self.y_train, labels])

        # Retrain
        self.model.fit(X_train_augmented, y_train_augmented)

        print(f"‚úÖ Retrained with {len(labels)} new labels")
        print(f"   Total training set: {len(y_train_augmented)} examples")


# Weekly active learning cycle
loop = ActiveLearningFeedbackLoop(model, labeling_budget=100)

# Select uncertain examples
to_label = loop.select_examples_for_labeling(production_unlabeled_data)

# Send to human labelers
labels = human_labeling_service.label(production_unlabeled_data[to_label])

# Retrain with new labels
loop.retrain_with_feedback(production_unlabeled_data[to_label], labels)
```

**Other Beneficial Loops:**
- User corrections improve training data
- Click-through data refines rankings
- Explicit feedback (thumbs up/down) labels data

---

#### B. Degenerative Feedback Loops

**Definition:** Model outputs create biased future inputs, amplifying errors and biases.

**Example 1: Popularity Bias in Recommendations**

```
1. Model recommends popular items
2. Users click popular items (no exposure to niche items)
3. Popular items get more data, niche items get less
4. Model increasingly recommends only popular items
‚Üí Filter bubble, reduced diversity
```

**Detection:**

```python
class FeedbackLoopDetector:
    """
    Detect degenerative feedback loops
    """

    def detect_popularity_bias(
        self,
        item_impressions: Dict[str, int],
        item_clicks: Dict[str, int],
        time_window_days: int = 30
    ) -> dict:
        """
        Detect if recommendations are becoming increasingly concentrated

        Metrics:
        - Gini coefficient (concentration)
        - Coverage (% of catalog shown)
        """
        # Calculate Gini coefficient
        impressions = np.array(list(item_impressions.values()))
        sorted_impressions = np.sort(impressions)
        n = len(sorted_impressions)
        index = np.arange(1, n + 1)
        gini = (2 * np.sum(index * sorted_impressions)) / (n * np.sum(sorted_impressions)) - (n + 1) / n

        # Calculate coverage
        total_items = len(all_items_catalog)
        shown_items = len([v for v in item_impressions.values() if v > 0])
        coverage = shown_items / total_items

        # Detect loop
        loop_detected = gini > 0.7 or coverage < 0.2

        return {
            "loop_detected": loop_detected,
            "gini_coefficient": gini,  # 0 = equal, 1 = concentrated
            "coverage": coverage,
            "recommendation": (
                "Inject exploration/randomization" if loop_detected
                else "Healthy diversity"
            )
        }


# Mitigation: Epsilon-Greedy Exploration
def recommend_with_exploration(
    user_id: str,
    model: Any,
    epsilon: float = 0.1
) -> List[str]:
    """
    Mix model recommendations with random exploration
    """
    if random.random() < epsilon:
        # Explore: Random items
        recommendations = sample_random_items(n=10)
    else:
        # Exploit: Model recommendations
        recommendations = model.predict_top_n(user_id, n=10)

    return recommendations
```

**Example 2: Automated Hiring Bias Amplification**

```
1. Model trained on historical hires (biased toward certain demographics)
2. Model recommends similar candidates
3. Company hires recommended candidates
4. New training data reinforces bias
‚Üí Discrimination amplifies over time
```

**Mitigation Strategies:**

1. **Randomization/Exploration:**
   - Inject random items in recommendations (epsilon-greedy)
   - Randomized controlled trials

2. **Debiasing:**
   - Monitor protected attributes
   - Fairness constraints in training
   - Counterfactual data augmentation

3. **External Data Injection:**
   - Bring in unbiased external datasets
   - Periodic calibration with human judgment

4. **Transparency:**
   - Explain why recommendations were made
   - Allow users to correct/override

---

### 4. Continuous Learning Systems

#### Automated Retraining Pipeline

```python
from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.utils.trigger_rule import TriggerRule
from datetime import datetime, timedelta

class ContinuousLearningPipeline:
    """
    Automated retraining with safety checks
    """

    def __init__(self, model_name: str, performance_threshold: float = 0.85):
        self.model_name = model_name
        self.performance_threshold = performance_threshold

    def should_retrain(self, **context) -> str:
        """
        Decide if retraining is needed based on drift detection
        """
        # Check for drift
        drift_detected = check_data_drift()
        performance_degraded = check_performance_degradation()

        if drift_detected or performance_degraded:
            print("üîÑ Retraining triggered")
            return "retrain_model"
        else:
            print("‚úÖ Model still healthy, skipping retraining")
            return "skip_retraining"

    def retrain_model(self, **context):
        """
        Retrain model with latest data
        """
        # Get latest training data
        training_data = fetch_latest_training_data(days=90)

        # Train new model
        new_model = train_model(training_data)

        # Save candidate model
        mlflow.sklearn.log_model(new_model, f"candidate_models/{self.model_name}")

        return new_model

    def validate_candidate_model(self, **context):
        """
        Validate candidate model against holdout set and production model
        """
        candidate_model = context['task_instance'].xcom_pull(task_ids='retrain_model')
        production_model = mlflow.sklearn.load_model(f"models:/{self.model_name}/Production")

        # Load holdout test set
        test_data = load_holdout_test_set()

        # Compare performance
        candidate_auc = evaluate_model(candidate_model, test_data)
        production_auc = evaluate_model(production_model, test_data)

        print(f"üìä Candidate AUC: {candidate_auc:.4f}")
        print(f"üìä Production AUC: {production_auc:.4f}")

        # Decide promotion
        if candidate_auc >= production_auc * 0.98:  # Within 2%
            return "promote_to_staging"
        else:
            return "reject_candidate"

    def canary_test(self, **context):
        """
        Test candidate model with 10% of production traffic
        """
        candidate_model = context['task_instance'].xcom_pull(task_ids='retrain_model')

        # Deploy to canary
        deploy_canary(candidate_model, traffic_pct=10)

        # Monitor for 2 hours
        time.sleep(2 * 3600)

        # Check metrics
        canary_metrics = get_canary_metrics()

        if canary_metrics['error_rate'] < 0.01 and canary_metrics['auc'] > self.performance_threshold:
            return "promote_to_production"
        else:
            rollback_canary()
            return "reject_candidate"


# Airflow DAG
default_args = {
    'owner': 'ml-platform',
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'continuous_learning_pipeline',
    default_args=default_args,
    schedule_interval='@daily',  # Check daily
    catchup=False
) as dag:

    pipeline = ContinuousLearningPipeline(model_name="fraud_detector")

    check_retraining = BranchPythonOperator(
        task_id='check_should_retrain',
        python_callable=pipeline.should_retrain
    )

    retrain = PythonOperator(
        task_id='retrain_model',
        python_callable=pipeline.retrain_model
    )

    validate = BranchPythonOperator(
        task_id='validate_candidate',
        python_callable=pipeline.validate_candidate_model
    )

    canary = BranchPythonOperator(
        task_id='canary_test',
        python_callable=pipeline.canary_test
    )

    promote = PythonOperator(
        task_id='promote_to_production',
        python_callable=lambda: promote_model_to_production("fraud_detector")
    )

    skip = PythonOperator(
        task_id='skip_retraining',
        python_callable=lambda: print("Skipping retraining")
    )

    # Define workflow
    check_retraining >> [retrain, skip]
    retrain >> validate
    validate >> [canary, reject]
    canary >> [promote, reject]
```

---

## üí° Practical Examples

### Example 1: Real-Time Fraud Detection Monitoring

```python
class FraudMonitoringSystem:
    """
    Comprehensive monitoring for fraud detection model
    """

    def __init__(self):
        self.model = load_production_model()
        self.baseline_stats = load_baseline_statistics()

    def monitor_transaction(self, transaction: Dict) -> Dict:
        """
        Score transaction and monitor all aspects
        """
        # 1. Extract features
        features = extract_features(transaction)

        # 2. Check for feature drift
        drift_status = self.check_feature_drift(features)

        # 3. Predict
        fraud_score = self.model.predict_proba([features])[0, 1]

        # 4. Monitor prediction distribution
        self.log_prediction(fraud_score)

        # 5. Business logic
        decision = "BLOCK" if fraud_score > 0.9 else "APPROVE"

        # 6. Log for delayed feedback
        self.log_transaction_for_feedback(transaction, fraud_score, decision)

        return {
            "transaction_id": transaction['id'],
            "fraud_score": fraud_score,
            "decision": decision,
            "drift_detected": drift_status['drift_detected'],
            "model_version": self.model.metadata.version
        }

    def process_delayed_feedback(self):
        """
        Collect fraud labels and monitor model performance
        """
        # Get transactions with confirmed fraud labels (24-48 hour delay)
        labeled_transactions = fetch_confirmed_fraud_labels()

        # Calculate actual performance
        y_true = [t['is_fraud'] for t in labeled_transactions]
        y_pred = [t['fraud_score'] for t in labeled_transactions]

        current_auc = roc_auc_score(y_true, y_pred)

        # Check for concept drift
        if current_auc < self.baseline_stats['auc'] - 0.05:
            send_alert(
                severity="HIGH",
                message=f"Model performance degraded: {current_auc:.3f} vs baseline {self.baseline_stats['auc']:.3f}",
                action="Trigger retraining"
            )
```

---

### Example 2: Recommendation System with Feedback Loop Management

```python
class RecommendationMonitoring:
    """
    Monitor recommendations for diversity and feedback loops
    """

    def __init__(self):
        self.model = load_recommendation_model()
        self.item_exposure = {}  # Track item impressions

    def recommend_with_monitoring(
        self,
        user_id: str,
        exploration_rate: float = 0.1
    ) -> List[str]:
        """
        Generate recommendations with exploration and monitoring
        """
        # Get model recommendations
        if random.random() > exploration_rate:
            # Exploit: Use model
            recommendations = self.model.predict(user_id, top_k=10)
        else:
            # Explore: Random sample from underrepresented items
            recommendations = self.sample_underexposed_items(k=10)

        # Track impressions
        for item_id in recommendations:
            self.item_exposure[item_id] = self.item_exposure.get(item_id, 0) + 1

        # Monitor diversity
        self.monitor_diversity()

        return recommendations

    def monitor_diversity(self):
        """
        Detect if recommendations are becoming too concentrated
        """
        # Calculate Gini coefficient
        exposures = np.array(list(self.item_exposure.values()))
        gini = calculate_gini(exposures)

        # Alert if too concentrated
        if gini > 0.7:
            send_alert(
                severity="MEDIUM",
                message=f"Recommendation diversity low (Gini: {gini:.2f})",
                action="Increase exploration rate or inject fresh content"
            )

    def sample_underexposed_items(self, k: int) -> List[str]:
        """
        Sample items with low exposure to break feedback loops
        """
        # Get items sorted by exposure (ascending)
        sorted_items = sorted(
            self.item_exposure.items(),
            key=lambda x: x[1]
        )

        # Sample from bottom 20%
        underexposed = [item_id for item_id, _ in sorted_items[:int(len(sorted_items) * 0.2)]]
        return random.sample(underexposed, min(k, len(underexposed)))
```

---

## üîß Code Examples

### Complete Monitoring Pipeline with Evidently AI

```python
from evidently import ColumnMapping
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset, DataQualityPreset, TargetDriftPreset
from evidently.test_suite import TestSuite
from evidently.tests import *
import pandas as pd

class EvidentlyMonitoringPipeline:
    """
    Production monitoring using Evidently AI
    """

    def __init__(self, reference_data: pd.DataFrame):
        """
        Args:
            reference_data: Training or validation data as baseline
        """
        self.reference_data = reference_data

        # Define column mapping
        self.column_mapping = ColumnMapping(
            target='label',
            prediction='prediction',
            numerical_features=['age', 'income', 'credit_score'],
            categorical_features=['country', 'device_type']
        )

    def generate_data_drift_report(
        self,
        current_data: pd.DataFrame,
        save_path: str = "reports/data_drift.html"
    ):
        """
        Generate comprehensive data drift report
        """
        report = Report(metrics=[
            DataDriftPreset(),
            DataQualityPreset(),
            TargetDriftPreset()
        ])

        report.run(
            reference_data=self.reference_data,
            current_data=current_data,
            column_mapping=self.column_mapping
        )

        # Save HTML report
        report.save_html(save_path)

        # Extract summary
        results = report.as_dict()

        return {
            "data_drift_detected": results['metrics'][0]['result']['dataset_drift'],
            "n_drifted_features": results['metrics'][0]['result']['number_of_drifted_columns'],
            "target_drift_detected": results['metrics'][2]['result']['drift_detected'],
            "report_path": save_path
        }

    def run_test_suite(self, current_data: pd.DataFrame) -> dict:
        """
        Run automated test suite with pass/fail criteria
        """
        tests = TestSuite(tests=[
            TestColumnDrift(column_name='age'),
            TestColumnDrift(column_name='income'),
            TestShareOfMissingValues(column_name='credit_score', lt=0.05),
            TestMeanInNSigmas(column_name='income', n=3),
            TestNumberOfDriftedColumns(lt=3)
        ])

        tests.run(
            reference_data=self.reference_data,
            current_data=current_data,
            column_mapping=self.column_mapping
        )

        results = tests.as_dict()

        all_passed = all([test['status'] == 'SUCCESS' for test in results['tests']])

        return {
            "all_tests_passed": all_passed,
            "failed_tests": [
                test['name'] for test in results['tests']
                if test['status'] == 'FAIL'
            ],
            "summary": results['summary']
        }


# Scheduled monitoring job
pipeline = EvidentlyMonitoringPipeline(reference_data=training_data)

# Daily production data check
production_batch = fetch_production_data(date="2025-10-18")

# Generate report
drift_report = pipeline.generate_data_drift_report(
    current_data=production_batch,
    save_path=f"reports/drift_2025-10-18.html"
)

if drift_report['data_drift_detected']:
    print(f"‚ö†Ô∏è  Data drift detected in {drift_report['n_drifted_features']} features")
    # Trigger alert
    send_slack_alert(f"Data drift detected: {drift_report['report_path']}")

# Run test suite
test_results = pipeline.run_test_suite(production_batch)

if not test_results['all_tests_passed']:
    print(f"‚ùå Tests failed: {test_results['failed_tests']}")
    # Trigger incident
    create_pagerduty_incident(
        title="ML Monitoring Tests Failed",
        details=test_results
    )
```

---

## ‚úÖ Best Practices

### 1. Monitor All Layers
- System metrics (latency, errors, throughput)
- Data metrics (drift, quality, freshness)
- Model metrics (performance, calibration)
- Business metrics (revenue, conversions, UX)

### 2. Establish Baselines Early
- Capture reference distributions from training
- Document expected performance
- Set alert thresholds based on business impact

### 3. Use Multiple Drift Detection Methods
- Statistical tests (KS, Chi-squared)
- Model-based (PSI, KL-divergence)
- Visual inspection (distribution plots)

### 4. Implement Graceful Degradation
- Fallback to simpler models
- Use cached predictions
- Return default values with warnings

### 5. Design Alerts Carefully
- Avoid alert fatigue (too sensitive)
- Prioritize by business impact
- Include actionable context
- Escalation paths

### 6. Monitor Feedback Loops
- Track diversity metrics (Gini, coverage)
- Inject randomization/exploration
- Periodic external data audits

### 7. Automate Retraining with Safety Checks
- Automated triggers (drift, performance)
- Validation against holdout sets
- Canary deployments
- Rollback mechanisms

### 8. Maintain Observability
- Centralized logging
- Distributed tracing
- Dashboards for all stakeholders

---

## ‚ö†Ô∏è Common Pitfalls

### 1. Monitoring Only Model Accuracy

**Problem:** Accuracy stays high but business metrics degrade.

**Solution:** Monitor business KPIs, user engagement, revenue impact.

---

### 2. Not Monitoring Without Labels

**Problem:** Waiting for delayed feedback to detect issues.

**Solution:** Use prediction drift, feature drift as leading indicators.

---

### 3. Ignoring Feedback Loops

**Problem:** Model amplifies biases over time silently.

**Solution:** Monitor diversity, inject exploration, audit for bias.

---

### 4. Alert Fatigue

**Problem:** Too many false alarms, team ignores alerts.

**Solution:**
- Set thresholds based on business impact
- Aggregate alerts (don't alert on every small drift)
- Clear escalation procedures

---

### 5. No Rollback Plan

**Problem:** Bad model deployed, takes days to revert.

**Solution:**
- Canary deployments
- Feature flags for instant rollback
- Version all models with rollback scripts

---

## üèãÔ∏è Hands-On Exercises

### Exercise 1: Build Comprehensive Monitoring Dashboard

**Difficulty:** Advanced
**Time:** 8-10 hours

**Objective:** Create end-to-end monitoring with Prometheus + Grafana

**Tasks:**
1. Instrument model with Prometheus metrics
2. Set up Grafana dashboards
3. Implement alerting (Slack, PagerDuty)
4. Create runbooks for common issues

**Deliverables:**
- Grafana dashboard screenshots
- Alert configurations
- Incident response runbook

---

### Exercise 2: Implement Drift Detection Pipeline

**Difficulty:** Intermediate
**Time:** 6-7 hours

**Objective:** Detect data drift using multiple methods

**Tasks:**
1. Implement KS test for numerical features
2. Implement Chi-squared for categorical features
3. Calculate PSI for all features
4. Create daily drift report with Evidently AI

**Validation:**
- Inject artificial drift and verify detection
- Compare detection methods

---

### Exercise 3: Design Feedback Loop Mitigation

**Difficulty:** Advanced
**Time:** 8-9 hours

**Objective:** Detect and mitigate degenerative feedback loop

**Scenario:** Recommendation system showing declining diversity

**Tasks:**
1. Calculate Gini coefficient for item exposure
2. Implement epsilon-greedy exploration
3. Monitor diversity metrics over time
4. A/B test exploration rates

---

### Exercise 4: Automated Retraining Pipeline

**Difficulty:** Advanced
**Time:** 10-12 hours

**Objective:** Build Airflow DAG for continuous learning

**Requirements:**
- Drift detection triggers retraining
- Validation against holdout set
- Canary deployment
- Auto-rollback on failure

**Deliverables:**
- Airflow DAG code
- Test cases
- Documentation

---

## üìö Further Reading

### Essential Books

1. **"Designing Machine Learning Systems" by Chip Huyen**
   - Chapter 8: Data Distribution Shifts
   - Chapter 9: Continual Learning

2. **"Reliable Machine Learning" by Cathy Chen, Niall Murphy, Kranti Parisa, D. Sculley, Todd Underwood**
   - O'Reilly, 2022
   - Google SRE approach to ML reliability

### Key Papers

1. **"Hidden Technical Debt in Machine Learning Systems"** (Sculley et al., NeurIPS 2015)
   - [Paper](https://papers.nips.cc/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html)
   - Foundational paper on ML system challenges

2. **"Monitoring Machine Learning Models in Production"** (Breck et al., 2019)
   - [Paper](https://research.google/pubs/pub48019/)
   - Google's ML monitoring practices

### Blogs and Documentation

1. **Evidently AI Documentation**
   - [Evidently Docs](https://docs.evidentlyai.com/)
   - Open-source ML monitoring

2. **Netflix: Model Monitoring**
   - ["Lessons from Building Near-Real-Time Feature Store"](https://netflixtechblog.com/)

3. **Uber: ML Platform Monitoring**
   - ["Building Reliable Retraining for Production ML"](https://www.uber.com/blog/)

### Tools

1. **Monitoring Platforms:**
   - [Evidently AI](https://www.evidentlyai.com/) - Open source
   - [Arize AI](https://arize.com/) - Commercial
   - [Fiddler AI](https://www.fiddler.ai/) - Enterprise

2. **Observability:**
   - [Prometheus](https://prometheus.io/) + [Grafana](https://grafana.com/)
   - [Datadog](https://www.datadoghq.com/)

---

## üìù Key Takeaways

1. **Monitoring is Multi-Layered**
   - System ‚Üí Data ‚Üí Predictions ‚Üí Model ‚Üí Business
   - All layers must be monitored

2. **Drift is Inevitable, Not Preventable**
   - Focus on rapid detection and response
   - Automate retraining workflows

3. **Multiple Types of Drift Require Different Strategies**
   - Data drift: Feature distribution changes
   - Concept drift: Relationship changes
   - Label drift: Target distribution changes
   - Prediction drift: Output distribution changes

4. **Feedback Loops Can Be Beneficial or Harmful**
   - Beneficial: Active learning, user corrections
   - Harmful: Popularity bias, discrimination amplification
   - Monitor diversity, inject exploration

5. **Business Metrics Trump Model Metrics**
   - High AUC doesn't guarantee business success
   - Monitor revenue, conversions, user satisfaction

6. **Continuous Learning Requires Safety Guardrails**
   - Automated retraining with validation
   - Canary deployments
   - Rollback mechanisms

7. **Prediction Drift is an Early Warning**
   - Detect issues before labels arrive
   - Investigate immediately

8. **Observability is Essential**
   - Centralized logging and tracing
   - Dashboards for all stakeholders
   - Clear incident response procedures

9. **Alert Design Matters**
   - Balance sensitivity with actionability
   - Include business impact in alerts
   - Avoid alert fatigue

10. **Root Cause Analysis is Iterative**
    - Start with business metrics
    - Drill down through model ‚Üí data ‚Üí system
    - Document lessons learned

---

## üìù Notes Section

### My Key Insights:
-

### Questions to Explore Further:
-

### How This Applies to My Work:
-

### Tools to Investigate:
-

### Action Items:
-

---

## üîó Related Concepts

- [[03. Feature Engineering Workflows|Previous: Feature Engineering]]
- [[04. Model Training and Serving Data Flows|Previous: Training and Serving]]
- [[01_Projects/DEforAI/Course/01. Introduction to DE for AI-ML/README|Chapter 1: Introduction]]

---

*Created: October 18, 2025*
*Last Updated: October 18, 2025*
*Status: ‚úÖ Completed - Ready for study*
