# Cost Optimization Techniques

**Course:** Data Engineering for AI/ML
**Chapter:** 04 - Data Storage for ML
**Subchapter:** 06 - Cost Optimization Techniques
**Created:** 2025-10-19
**Updated:** 2025-10-19

---

## 📋 Overview

Storage costs for machine learning workloads can quickly spiral out of control. A typical ML organization storing petabytes of training data, features, model artifacts, and logs can spend $50K-500K per month on cloud storage alone. Without proper cost optimization, teams often discover:

- **70-90% of stored data is never accessed** (zombie data, old experiments, duplicate datasets)
- **50-80% cost savings** possible through compression and tiered storage
- **10-100x cost differences** between storage classes (S3 Standard vs Deep Archive)
- **Hidden costs** from data transfer (inter-region), API calls (LIST, GET), and query scans (Athena, BigQuery)

Effective cost optimization requires understanding cloud storage pricing models, lifecycle policies, compression trade-offs, and query optimization techniques. This subchapter provides actionable strategies to reduce storage costs by 50-90% while maintaining performance for ML workloads.

**Key Topics:**
- Cloud storage pricing models (S3, GCS, Azure Blob)
- Storage class selection and lifecycle policies
- Compression cost-benefit analysis (Zstd vs Snappy vs Gzip)
- Query-based pricing optimization (Athena, BigQuery)
- Data retention and archival strategies
- Reserved capacity and committed use discounts
- Cost monitoring and anomaly detection
- Cross-region transfer cost optimization
- Data deduplication and garbage collection
- ML-specific cost optimization patterns

**Prerequisites:**
- Understanding of object storage systems (Subchapter 04.02)
- Familiarity with storage formats and compression (Subchapter 04.04)
- Knowledge of data partitioning (Subchapter 04.05)
- Basic understanding of cloud pricing models

---

## 🎯 Learning Objectives

By the end of this subchapter, you will be able to:

1. **Select optimal storage classes** for different ML data types (hot features, warm training data, cold archives)
2. **Implement lifecycle policies** to automatically transition data to cheaper tiers over time
3. **Calculate cost-benefit** of compression algorithms (storage savings vs compute overhead)
4. **Optimize query costs** for pay-per-scan systems (Athena, BigQuery) through partitioning and data skipping
5. **Design data retention policies** that balance compliance requirements with cost efficiency
6. **Monitor storage costs** and detect anomalies (unexpected growth, orphaned data)
7. **Eliminate duplicate data** through deduplication and garbage collection
8. **Minimize data transfer costs** by optimizing cross-region replication and query patterns
9. **Leverage reserved capacity** and committed use discounts for predictable workloads
10. **Implement cost attribution** to track spending per team, project, or model

---

## 📚 Core Concepts

### 1. Cloud Storage Pricing Models

**AWS S3 Pricing Components:**

| Component | Pricing | Example |
|-----------|---------|---------|
| **Storage (per GB/month)** | $0.023/GB (Standard) | 1 TB = $23/month |
| **Data transfer OUT** | $0.09/GB (first 10 TB) | 100 GB = $9 |
| **PUT/POST requests** | $0.005 per 1,000 | 1M writes = $5 |
| **GET requests** | $0.0004 per 1,000 | 1M reads = $0.40 |
| **LIST requests** | $0.005 per 1,000 | 10K lists = $0.05 |

**Total Monthly Cost Example:**
- 10 TB training data (Standard storage)
- 1 TB egress per month (data downloads)
- 100M GET requests (training reads)
- 1M PUT requests (new data writes)

```
Storage: 10,000 GB × $0.023 = $230
Transfer: 1,000 GB × $0.09 = $90
GET: 100,000,000 / 1000 × $0.0004 = $40
PUT: 1,000,000 / 1000 × $0.005 = $5
TOTAL: $365/month
```

**GCP Cloud Storage Pricing:**

| Storage Class | Price/GB/month | Use Case |
|---------------|----------------|----------|
| **Standard** | $0.020 | Hot data (< 30 days) |
| **Nearline** | $0.010 | Warm data (30-90 days) |
| **Coldline** | $0.004 | Cold data (90-365 days) |
| **Archive** | $0.0012 | Long-term (> 365 days) |

**Azure Blob Storage Pricing:**

| Tier | Price/GB/month | Access Frequency |
|------|----------------|------------------|
| **Hot** | $0.0184 | Frequent access |
| **Cool** | $0.01 | Infrequent (> 30 days) |
| **Archive** | $0.00099 | Rare access (> 180 days) |

**Key Differences:**

- **AWS S3**: More storage classes (7 total including Intelligent-Tiering)
- **GCP**: Simpler tiering, slightly cheaper Standard storage
- **Azure**: Lowest archive pricing, good for compliance data

**Hidden Costs:**

1. **Minimum storage duration charges**:
   - S3 Glacier: 90 days minimum
   - S3 Deep Archive: 180 days minimum
   - Deleting before minimum incurs pro-rated charge

2. **Retrieval costs**:
   - Glacier retrieval: $0.03/GB (standard, 3-5 hours)
   - Glacier expedited: $0.03/GB + $10 per request (1-5 minutes)
   - Deep Archive: $0.02/GB (12-48 hours)

3. **Data transfer**:
   - Inter-region transfer: $0.02/GB (same cloud)
   - Cross-cloud transfer: $0.09/GB (AWS → GCP)
   - Inbound transfer: FREE

### 2. Storage Class Selection Matrix

**Decision Framework:**

```
Access Pattern Analysis:
├─ Accessed daily → Standard/Hot
├─ Accessed weekly → Standard with caching
├─ Accessed monthly → Standard-IA / Nearline / Cool
├─ Accessed quarterly → Glacier / Coldline
└─ Accessed yearly or never → Deep Archive / Archive

Retrieval Time Tolerance:
├─ < 1 second → Standard
├─ < 1 hour → Standard-IA
├─ < 12 hours → Glacier
└─ < 48 hours → Deep Archive

Cost Sensitivity:
├─ Performance > cost → Standard (all data)
├─ Balanced → Tiered (hot + warm + cold)
└─ Cost > performance → Aggressive tiering (90% in Archive)
```

**ML Workload Storage Class Mapping:**

| Data Type | Access Pattern | Recommended Storage | Monthly Cost (1 TB) |
|-----------|----------------|---------------------|---------------------|
| **Active training data** | Daily reads | S3 Standard | $23 |
| **Feature store (hot)** | Hourly reads | S3 Standard + Elasticache | $23 + $50 |
| **Training data (1-3 months old)** | Weekly reads | S3 Standard-IA | $12.50 |
| **Experiment artifacts** | Monthly reads | S3 Glacier Instant Retrieval | $4 |
| **Compliance logs (6-12 months)** | Quarterly reads | S3 Glacier Flexible | $3.60 |
| **Regulatory archives (> 1 year)** | Rare/never | S3 Glacier Deep Archive | $0.99 |

**Cost Comparison (1 TB for 1 year):**

```
Standard:           $23/mo × 12 = $276
Standard-IA:        $12.50/mo × 12 = $150 (45% savings)
Glacier Instant:    $4/mo × 12 = $48 (83% savings)
Glacier Flexible:   $3.60/mo × 12 = $43 (84% savings)
Deep Archive:       $0.99/mo × 12 = $12 (96% savings)
```

**Break-Even Analysis:**

When is tiered storage worth it?

```python
# S3 Standard-IA vs Standard
standard_cost_per_gb_month = 0.023
standard_ia_cost_per_gb_month = 0.0125
standard_ia_retrieval_per_gb = 0.01  # $0.01/GB retrieval

# Assume 1 TB, accessed X times per month
data_size_gb = 1000
accesses_per_month = X

standard_monthly = data_size_gb * standard_cost_per_gb_month
standard_ia_monthly = (
    data_size_gb * standard_ia_cost_per_gb_month +
    accesses_per_month * data_size_gb * standard_ia_retrieval_per_gb
)

# Break-even when standard_monthly == standard_ia_monthly
# 23 = 12.50 + X * 1000 * 0.01
# 10.50 = X * 10
# X = 1.05 full accesses per month

# Conclusion: Standard-IA cheaper if accessed < 1 time/month
```

### 3. Lifecycle Policies

**Lifecycle Policy Components:**

1. **Transitions**: Move objects between storage classes
2. **Expirations**: Delete objects after specified time
3. **Filters**: Apply rules to subsets (prefix, tags, size)
4. **Actions**: Transition, expire, or abort incomplete uploads

**Example Lifecycle Policy (S3):**

```json
{
  "Rules": [
    {
      "Id": "TransitionTrainingData",
      "Status": "Enabled",
      "Filter": {
        "Prefix": "ml-data/training/"
      },
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER_IR"
        },
        {
          "Days": 365,
          "StorageClass": "DEEP_ARCHIVE"
        }
      ]
    },
    {
      "Id": "ExpireOldExperiments",
      "Status": "Enabled",
      "Filter": {
        "Prefix": "ml-experiments/temp/"
      },
      "Expiration": {
        "Days": 7
      }
    },
    {
      "Id": "CleanupIncompleteUploads",
      "Status": "Enabled",
      "AbortIncompleteMultipartUpload": {
        "DaysAfterInitiation": 7
      }
    }
  ]
}
```

**Lifecycle Strategy for ML Workloads:**

```
Day 0:      Active training → S3 Standard ($0.023/GB/mo)
Day 30:     Archive → S3 Standard-IA ($0.0125/GB/mo)
Day 90:     Cold storage → Glacier Instant ($0.004/GB/mo)
Day 365:    Compliance → Deep Archive ($0.00099/GB/mo)
Day 2,555:  Delete (7 years retention)
```

**Estimated Savings:**

```python
# 1 TB dataset lifecycle
data_size_gb = 1000

# Without lifecycle (all in Standard for 7 years)
no_lifecycle_cost = data_size_gb * 0.023 * 12 * 7  # $1,932

# With lifecycle
lifecycle_cost = (
    data_size_gb * 0.023 * 12 * (30/365)  # 30 days Standard
    + data_size_gb * 0.0125 * 12 * (60/365)  # 60 days Standard-IA
    + data_size_gb * 0.004 * 12 * (275/365)  # 275 days Glacier IR
    + data_size_gb * 0.00099 * 12 * 6  # 6 years Deep Archive
)  # $172

savings = (no_lifecycle_cost - lifecycle_cost) / no_lifecycle_cost * 100
# 91% savings!
```

### 4. Compression Cost-Benefit Analysis

**Compression Impact on Storage Costs:**

| Algorithm | Ratio | Storage (1 TB) | Compression Time | Decompression Time |
|-----------|-------|----------------|------------------|---------------------|
| **None** | 1.0x | $23/mo | 0 min | 0 min |
| **Snappy** | 2.5x | $9.20/mo (60% savings) | 5 min | 2 min |
| **Zstd (level 3)** | 3.5x | $6.57/mo (71% savings) | 8 min | 3 min |
| **Gzip (level 6)** | 5.0x | $4.60/mo (80% savings) | 25 min | 10 min |

**Total Cost of Ownership (TCO) Calculation:**

```python
# Scenario: 10 TB dataset, accessed 100 times/year for training

data_size_tb = 10
reads_per_year = 100
compute_cost_per_hour = 3.06  # c6i.8xlarge (32 vCPU)

# Snappy compression
snappy_storage_cost_year = (data_size_tb * 1000 / 2.5) * 0.023 * 12  # $1,104
snappy_decompression_cost = (reads_per_year * 2/60) * compute_cost_per_hour  # $10
snappy_total = snappy_storage_cost_year + snappy_decompression_cost  # $1,114

# Gzip compression
gzip_storage_cost_year = (data_size_tb * 1000 / 5.0) * 0.023 * 12  # $552
gzip_decompression_cost = (reads_per_year * 10/60) * compute_cost_per_hour  # $51
gzip_total = gzip_storage_cost_year + gzip_decompression_cost  # $603

# Savings: Gzip saves $511/year (46% vs Snappy)
# Trade-off: +8 min per read (acceptable for batch training)
```

**Recommendation:**
- **Training data (read frequently)**: Snappy or Zstd level 3
- **Feature store (balanced)**: Zstd level 3
- **Archives (rarely read)**: Gzip or Zstd level 9

### 5. Query-Based Pricing Optimization

**AWS Athena Pricing:**

- $5 per TB scanned
- Charged per query, rounded up to 10 MB minimum

**Cost Reduction Strategies:**

1. **Partition Pruning**: Only scan relevant partitions
```sql
-- BAD: Scans entire 100 TB table
SELECT * FROM events WHERE timestamp > '2025-10-01'
-- Cost: 100 TB × $5 = $500

-- GOOD: Partition by date, scan only October
SELECT * FROM events WHERE date >= '2025-10-01'
-- Cost: 3 TB × $5 = $15 (97% savings)
```

2. **Columnar Storage**: Read only needed columns
```sql
-- BAD: SELECT * reads all 100 columns
SELECT * FROM features WHERE user_id = 123
-- Scans: 1 TB

-- GOOD: SELECT specific columns
SELECT user_id, features FROM features WHERE user_id = 123
-- Scans: 50 GB (95% savings)
```

3. **Compression**: Reduce bytes scanned
```
Uncompressed Parquet: 10 TB → $50/query
Snappy Parquet: 4 TB → $20/query (60% savings)
Zstd Parquet: 2.8 TB → $14/query (72% savings)
```

**BigQuery Pricing:**

- $6.25 per TB scanned (on-demand)
- $2,000/month per 100 slots (flat-rate)

**Break-Even Analysis:**

```python
# On-demand vs flat-rate
on_demand_cost_per_tb = 6.25

# If you scan X TB per month:
scanned_tb_per_month = X

on_demand_monthly = scanned_tb_per_month * on_demand_cost_per_tb
flat_rate_monthly = 2000  # 100 slots

# Break-even: X = 320 TB/month
# Scan < 320 TB/month → On-demand cheaper
# Scan > 320 TB/month → Flat-rate cheaper
```

**Cost Optimization Techniques:**

1. **Clustering (BigQuery equivalent of Z-ORDER)**:
```sql
CREATE TABLE features.user_features
CLUSTER BY user_id, date
AS SELECT * FROM ...

-- Query with clustering
SELECT * FROM features.user_features
WHERE user_id = 123 AND date = '2025-10-15'
-- Scans: 100 GB (vs 10 TB without clustering, 99% savings)
```

2. **Materialized Views**:
```sql
-- Precompute expensive aggregations
CREATE MATERIALIZED VIEW daily_metrics AS
SELECT date, COUNT(*) as event_count, AVG(value) as avg_value
FROM events
GROUP BY date

-- Query materialized view instead of raw data
SELECT * FROM daily_metrics WHERE date = '2025-10-15'
-- Cost: $0.01 (vs $5 on raw table)
```

### 6. Data Retention and Governance

**Retention Policy Design:**

| Data Type | Retention Period | Rationale | Storage Class |
|-----------|------------------|-----------|---------------|
| **Raw training data** | 90 days | Reproducibility window | Standard → Standard-IA |
| **Processed features** | 180 days | Model retraining window | Standard-IA → Glacier IR |
| **Model artifacts** | 365 days | A/B testing, rollback | Glacier IR → Deep Archive |
| **Experiment logs** | 30 days | Active debugging | Standard, then delete |
| **Compliance data** | 7 years | Regulatory requirement | Deep Archive |
| **Temporary outputs** | 7 days | Intermediate results | Standard, then delete |

**Retention Cost Impact:**

```python
# Scenario: 100 TB generated per month

# Without retention policy (infinite accumulation)
monthly_growth_tb = 100
months = 12
total_data_year = monthly_growth_tb * months  # 1,200 TB
annual_cost_no_policy = total_data_year * 1000 * 0.023 * 12  # $331,200

# With 90-day retention policy
avg_data_stored = monthly_growth_tb * 3  # 300 TB (3-month rolling window)
annual_cost_with_policy = avg_data_stored * 1000 * 0.023 * 12  # $82,800

# Savings: $248,400/year (75% reduction)
```

**Automated Cleanup:**

```python
# Delete experiments older than 30 days
from datetime import datetime, timedelta
import boto3

s3 = boto3.client('s3')
bucket = 'ml-experiments'
prefix = 'temp/'
cutoff_date = datetime.now() - timedelta(days=30)

paginator = s3.get_paginator('list_objects_v2')
for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
    for obj in page.get('Contents', []):
        if obj['LastModified'].replace(tzinfo=None) < cutoff_date:
            s3.delete_object(Bucket=bucket, Key=obj['Key'])
            print(f"Deleted: {obj['Key']}")
```

### 7. Reserved Capacity and Committed Use Discounts

**AWS S3 Storage Lens (Cost Insights):**

Free service that provides:
- Storage metrics by bucket, prefix, storage class
- Cost and usage trends
- Recommendations for optimization

**GCP Committed Use Discounts:**

- 1-year commitment: 7% discount
- 3-year commitment: 12% discount
- Applies to Cloud Storage Standard tier

**Example:**

```
10 TB Standard storage without commitment: $200/month
10 TB with 3-year commitment: $176/month (12% savings)
Savings over 3 years: $864
```

**When to Use:**
- Predictable storage needs (baseline data that won't shrink)
- Long-term projects (> 1 year)
- Stable workloads (not experimental)

**Azure Reserved Capacity:**

- 1-year: 10-20% discount
- 3-year: 30-40% discount
- Applies to Blob storage (Hot, Cool, Archive)

### 8. Cross-Region Transfer Cost Optimization

**Data Transfer Pricing (AWS):**

| Transfer Type | Cost |
|---------------|------|
| **Inbound** (Internet → AWS) | FREE |
| **Same region** (S3 → EC2) | FREE |
| **Cross-region** (us-east-1 → us-west-2) | $0.02/GB |
| **Outbound** (AWS → Internet) | $0.09/GB (first 10 TB) |
| **Cross-cloud** (AWS → GCP) | $0.09/GB |

**Cost Optimization Strategies:**

1. **Colocate compute and storage in same region**:
```python
# BAD: Training in us-west-2, data in us-east-1
# Transfer: 10 TB × $0.02/GB = $200

# GOOD: Training and data both in us-west-2
# Transfer: $0
```

2. **Use CloudFront for inference**:
```python
# BAD: Serve model artifacts from S3 directly
# Outbound: 1 TB × $0.09/GB = $90

# GOOD: Serve via CloudFront
# CloudFront: 1 TB × $0.085/GB = $85 (5% savings + caching benefits)
```

3. **S3 Transfer Acceleration for uploads**:
```python
# Standard upload from distant region: 100 GB, $0.09/GB = $9
# Transfer Acceleration: 100 GB, $0.04/GB acceleration + $0 transfer = $4
# Savings: $5 (56%) + faster uploads
```

4. **VPC Endpoints to avoid NAT gateway costs**:
```python
# Without VPC Endpoint: $0.045/GB NAT + $0.09/GB outbound = $0.135/GB
# With VPC Endpoint: $0 (S3 traffic stays in VPC)
# Savings on 10 TB: $1,350
```

---

## 💡 Practical Examples

### Example 1: Implement Lifecycle Policy for Training Data

**Scenario:** 50 TB of training data generated monthly. Current cost: $1,150/month (all in S3 Standard). Goal: Reduce by 70%.

**Analysis:**
- Last 30 days: Active training (accessed daily)
- 30-90 days: Occasional retraining (accessed weekly)
- 90-365 days: Compliance/rollback (accessed monthly)
- > 365 days: Regulatory (accessed rarely)

**Solution:**

```python
import boto3
import json

s3 = boto3.client('s3')

lifecycle_policy = {
    "Rules": [
        {
            "Id": "OptimizeTrainingData",
            "Status": "Enabled",
            "Filter": {
                "Prefix": "ml-data/training/"
            },
            "Transitions": [
                {
                    "Days": 30,
                    "StorageClass": "STANDARD_IA"
                },
                {
                    "Days": 90,
                    "StorageClass": "GLACIER_IR"
                },
                {
                    "Days": 365,
                    "StorageClass": "DEEP_ARCHIVE"
                }
            ],
            "Expiration": {
                "Days": 2555  # 7 years (regulatory requirement)
            }
        }
    ]
}

bucket = 'ml-training-data'
s3.put_bucket_lifecycle_configuration(
    Bucket=bucket,
    LifecycleConfiguration=lifecycle_policy
)

print(f"✅ Lifecycle policy applied to {bucket}")
```

**Cost Calculation:**

```python
# Monthly data generation: 50 TB
monthly_gen_tb = 50

# Age distribution after steady state (12+ months):
# 0-30 days: 50 TB in Standard
# 30-90 days: 100 TB in Standard-IA
# 90-365 days: 550 TB in Glacier IR
# 365+ days: 6 years of data in Deep Archive

cost_standard = 50 * 1000 * 0.023  # $1,150
cost_standard_ia = 100 * 1000 * 0.0125  # $1,250
cost_glacier_ir = 550 * 1000 * 0.004  # $2,200
cost_deep_archive = (50 * 12 * 6) * 1000 * 0.00099  # $356

total_monthly = cost_standard + cost_standard_ia + cost_glacier_ir + cost_deep_archive
# $4,956/month

# Wait, this is higher! But we're now storing 7 years of data (4,400 TB)
# Old cost for same data (all Standard): 4,400 TB × $23/month = $101,200/month

# Actual savings: ($101,200 - $4,956) / $101,200 = 95% savings!
```

### Example 2: Compression Optimization for Feature Store

**Scenario:** 20 TB feature store, accessed 100 times/month for training. Current: Uncompressed Parquet in S3 Standard.

**Benchmark Compression Options:**

```python
import pyarrow.parquet as pq
import time

# Sample 100 GB dataset
df = spark.read.parquet("s3://features/sample/")

compressions = {
    'none': {'ratio': 1.0, 'write_time': 0, 'read_time': 0},
    'snappy': {'ratio': 2.5, 'write_time': 5, 'read_time': 2},
    'zstd': {'ratio': 3.5, 'write_time': 8, 'read_time': 3},
    'gzip': {'ratio': 5.0, 'write_time': 25, 'read_time': 10}
}

data_size_tb = 20
reads_per_month = 100
storage_cost_per_tb_month = 23
compute_cost_per_hour = 3.06  # c6i.8xlarge

results = []

for comp, stats in compressions.items():
    # Storage cost
    storage_size_tb = data_size_tb / stats['ratio']
    storage_cost_month = storage_size_tb * storage_cost_per_tb_month

    # Compute cost for decompression
    decompression_hours = (reads_per_month * stats['read_time']) / 60
    compute_cost_month = decompression_hours * compute_cost_per_hour

    total_cost_month = storage_cost_month + compute_cost_month

    results.append({
        'compression': comp,
        'storage_size_tb': storage_size_tb,
        'storage_cost': storage_cost_month,
        'compute_cost': compute_cost_month,
        'total_cost': total_cost_month
    })

import pandas as pd
print(pd.DataFrame(results).to_string(index=False))

# Output:
# compression  storage_size_tb  storage_cost  compute_cost  total_cost
# none         20.0             460.0         0.0           460.0
# snappy       8.0              184.0         10.2          194.2
# zstd         5.7              131.1         15.3          146.4
# gzip         4.0              92.0          51.0          143.0

# Winner: Gzip (69% savings, $317/month saved)
```

**Implementation:**

```python
# Convert to Gzip-compressed Parquet
df = spark.read.parquet("s3://features/uncompressed/")

df.write.mode("overwrite") \
    .option("compression", "gzip") \
    .parquet("s3://features/compressed/")

# Validate
assert spark.read.parquet("s3://features/uncompressed/").count() == \
       spark.read.parquet("s3://features/compressed/").count()

print("✅ Compression complete. Estimated savings: $317/month")
```

### Example 3: Athena Cost Optimization

**Scenario:** 100 TB event log table, queried 1,000 times/month. Current Athena cost: $500/query = $500K/month. Goal: Reduce to < $50K/month.

**Optimizations Applied:**

1. **Partition by date**:
```sql
-- Before: Unpartitioned
CREATE EXTERNAL TABLE events (
  user_id BIGINT,
  event_type STRING,
  timestamp TIMESTAMP
)
STORED AS PARQUET
LOCATION 's3://events/';

-- Query scans entire 100 TB: $500

-- After: Partitioned by date
CREATE EXTERNAL TABLE events_partitioned (
  user_id BIGINT,
  event_type STRING,
  timestamp TIMESTAMP
)
PARTITIONED BY (date DATE)
STORED AS PARQUET
LOCATION 's3://events_partitioned/';

-- Query with date filter scans 1 day (274 GB): $1.37
-- Savings: 99.7% per query
```

2. **Convert to columnar + compression**:
```python
# Repartition and compress
df = spark.read.json("s3://events/raw/")

df.withColumn("date", F.to_date("timestamp")) \
    .repartition("date") \
    .write.mode("overwrite") \
    .partitionBy("date") \
    .option("compression", "zstd") \
    .parquet("s3://events_partitioned/")

# Size reduction: 100 TB → 28 TB (3.5x compression)
# New query cost: $1.37 → $0.39 (72% additional savings)
```

3. **Use CTAS for repeated queries**:
```sql
-- Create cached aggregation table
CREATE TABLE daily_metrics
WITH (
  format = 'PARQUET',
  external_location = 's3://metrics/daily/',
  partitioned_by = ARRAY['date']
) AS
SELECT
  date,
  event_type,
  COUNT(*) as event_count,
  COUNT(DISTINCT user_id) as unique_users
FROM events_partitioned
GROUP BY date, event_type;

-- Query metrics table instead of raw events
SELECT * FROM daily_metrics
WHERE date = DATE '2025-10-15'
-- Cost: $0.01 (vs $0.39 on raw data, 97% savings)
```

**Total Cost Reduction:**

```
Before:
- 1,000 queries/month × $500 = $500,000/month

After:
- 900 queries on daily_metrics × $0.01 = $9
- 100 queries on raw data × $0.39 = $39
- Total: $48/month

Savings: $499,952/month (99.99% reduction!)
```

### Example 4: Deduplication and Garbage Collection

**Scenario:** 500 TB in S3, but only 300 TB is actively used. 200 TB is orphaned (deleted tables, temp files, duplicate uploads).

**Audit and Cleanup:**

```python
import boto3
from datetime import datetime, timedelta
from collections import defaultdict

s3 = boto3.client('s3')
bucket = 'ml-data'

# Audit 1: Find files not accessed in 90 days
def find_old_files(bucket, days=90):
    cutoff = datetime.now() - timedelta(days=days)
    old_files = []

    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=bucket):
        for obj in page.get('Contents', []):
            if obj['LastModified'].replace(tzinfo=None) < cutoff:
                old_files.append({
                    'key': obj['Key'],
                    'size_gb': obj['Size'] / (1024**3),
                    'last_modified': obj['LastModified']
                })

    total_size = sum(f['size_gb'] for f in old_files)
    print(f"Found {len(old_files)} files not accessed in {days} days")
    print(f"Total size: {total_size:.2f} GB")

    return old_files

old_files = find_old_files(bucket, days=90)

# Audit 2: Find duplicate files (same size + name pattern)
def find_duplicates(bucket):
    files_by_hash = defaultdict(list)

    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=bucket):
        for obj in page.get('Contents', []):
            # Use ETag as proxy for content hash (imperfect but fast)
            etag = obj['ETag'].strip('"')
            files_by_hash[etag].append(obj)

    duplicates = {k: v for k, v in files_by_hash.items() if len(v) > 1}
    duplicate_size = sum(
        sum(f['Size'] for f in files[1:]) for files in duplicates.values()
    ) / (1024**3)

    print(f"Found {len(duplicates)} sets of duplicate files")
    print(f"Duplicate size: {duplicate_size:.2f} GB")

    return duplicates

duplicates = find_duplicates(bucket)

# Cleanup: Delete old temp files
def cleanup_temp_files(bucket, prefix='temp/', days=7):
    cutoff = datetime.now() - timedelta(days=days)
    deleted_count = 0
    deleted_size_gb = 0

    paginator = s3.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get('Contents', []):
            if obj['LastModified'].replace(tzinfo=None) < cutoff:
                s3.delete_object(Bucket=bucket, Key=obj['Key'])
                deleted_count += 1
                deleted_size_gb += obj['Size'] / (1024**3)

    print(f"Deleted {deleted_count} temp files ({deleted_size_gb:.2f} GB)")
    savings_month = deleted_size_gb * 0.023
    print(f"Monthly savings: ${savings_month:.2f}")

cleanup_temp_files(bucket, prefix='temp/', days=7)
```

**Estimated Savings:**

```
Orphaned data identified: 200 TB
Monthly cost: 200 TB × $23/TB = $4,600
Annual savings from cleanup: $55,200
```

### Example 5: Cost Monitoring and Alerting

**Scenario:** Detect cost anomalies (unexpected data growth, rogue experiments).

**Implementation:**

```python
import boto3
from datetime import datetime, timedelta

cloudwatch = boto3.client('cloudwatch')

def get_storage_metrics(bucket, days=30):
    """Get daily storage metrics for bucket"""
    end_time = datetime.now()
    start_time = end_time - timedelta(days=days)

    response = cloudwatch.get_metric_statistics(
        Namespace='AWS/S3',
        MetricName='BucketSizeBytes',
        Dimensions=[
            {'Name': 'BucketName', 'Value': bucket},
            {'Name': 'StorageType', 'Value': 'StandardStorage'}
        ],
        StartTime=start_time,
        EndTime=end_time,
        Period=86400,  # Daily
        Statistics=['Average']
    )

    data = sorted(response['Datapoints'], key=lambda x: x['Timestamp'])
    return [(d['Timestamp'], d['Average'] / (1024**4)) for d in data]  # Convert to TB

def detect_anomalies(metrics, threshold_percent=20):
    """Detect day-over-day growth > threshold"""
    anomalies = []

    for i in range(1, len(metrics)):
        prev_size = metrics[i-1][1]
        curr_size = metrics[i][1]
        growth_pct = ((curr_size - prev_size) / prev_size) * 100

        if growth_pct > threshold_percent:
            anomalies.append({
                'date': metrics[i][0],
                'size_tb': curr_size,
                'growth_tb': curr_size - prev_size,
                'growth_pct': growth_pct
            })

    return anomalies

# Monitor bucket
bucket = 'ml-training-data'
metrics = get_storage_metrics(bucket, days=30)
anomalies = detect_anomalies(metrics, threshold_percent=20)

if anomalies:
    print("⚠️ Cost anomalies detected:")
    for a in anomalies:
        print(f"  {a['date']}: +{a['growth_tb']:.2f} TB (+{a['growth_pct']:.1f}%)")

    # Send alert
    sns = boto3.client('sns')
    sns.publish(
        TopicArn='arn:aws:sns:us-east-1:123456789012:storage-alerts',
        Subject='S3 Cost Anomaly Detected',
        Message=f"Unusual growth detected in bucket {bucket}"
    )
```

**Cost Monitoring Dashboard (CloudWatch):**

```python
def create_cost_dashboard(bucket):
    """Create CloudWatch dashboard for cost monitoring"""
    cloudwatch = boto3.client('cloudwatch')

    dashboard_body = {
        "widgets": [
            {
                "type": "metric",
                "properties": {
                    "metrics": [
                        ["AWS/S3", "BucketSizeBytes", {"stat": "Average"}]
                    ],
                    "period": 86400,
                    "stat": "Average",
                    "region": "us-east-1",
                    "title": "Storage Size (TB)",
                    "yAxis": {"left": {"label": "TB"}}
                }
            },
            {
                "type": "metric",
                "properties": {
                    "metrics": [
                        ["AWS/S3", "NumberOfObjects", {"stat": "Average"}]
                    ],
                    "period": 86400,
                    "stat": "Average",
                    "region": "us-east-1",
                    "title": "Object Count"
                }
            }
        ]
    }

    cloudwatch.put_dashboard(
        DashboardName=f'{bucket}-cost-monitoring',
        DashboardBody=json.dumps(dashboard_body)
    )

create_cost_dashboard('ml-training-data')
```

### Example 6: Multi-Region Cost Optimization

**Scenario:** Training in us-west-2, data stored in us-east-1. Monthly cross-region transfer: 50 TB × $0.02/GB = $1,000.

**Solution: Replicate Hot Data to Training Region:**

```python
import boto3

s3 = boto3.client('s3')

# Configure replication rule
replication_config = {
    'Role': 'arn:aws:iam::123456789012:role/s3-replication',
    'Rules': [
        {
            'ID': 'ReplicateHotTrainingData',
            'Priority': 1,
            'Filter': {
                'Prefix': 'training/hot/'  # Last 30 days
            },
            'Status': 'Enabled',
            'Destination': {
                'Bucket': 'arn:aws:s3:::ml-data-us-west-2',
                'ReplicationTime': {
                    'Status': 'Enabled',
                    'Time': {'Minutes': 15}
                },
                'StorageClass': 'STANDARD'
            },
            'DeleteMarkerReplication': {'Status': 'Disabled'}
        }
    ]
}

source_bucket = 'ml-data-us-east-1'
s3.put_bucket_replication(
    Bucket=source_bucket,
    ReplicationConfiguration=replication_config
)

print("✅ Replication configured for hot training data")
```

**Cost Analysis:**

```
Before (cross-region transfer):
- 50 TB/month × $0.02/GB = $1,000/month

After (replication + local access):
- Replication: 20 TB (hot data) × $0.02/GB = $400/month (one-time per object)
- Storage: 20 TB × $0.023/GB/month = $460/month (duplicate storage)
- Cross-region transfer: 0 (training accesses local replica)

Total monthly cost: $460 (vs $1,000)
Savings: $540/month (54%)
```

---

## 🔧 Code Examples

### Code Example 1: Comprehensive Cost Analyzer

```python
import boto3
from datetime import datetime, timedelta
from collections import defaultdict
import pandas as pd

class S3CostAnalyzer:
    """Analyze and optimize S3 storage costs"""

    def __init__(self, bucket):
        self.bucket = bucket
        self.s3 = boto3.client('s3')
        self.cloudwatch = boto3.client('cloudwatch')

    def analyze_storage_classes(self):
        """Analyze current storage class distribution"""
        storage_by_class = defaultdict(lambda: {'count': 0, 'size_gb': 0})

        paginator = self.s3.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=self.bucket):
            for obj in page.get('Contents', []):
                storage_class = obj.get('StorageClass', 'STANDARD')
                storage_by_class[storage_class]['count'] += 1
                storage_by_class[storage_class]['size_gb'] += obj['Size'] / (1024**3)

        # Calculate costs
        pricing = {
            'STANDARD': 0.023,
            'STANDARD_IA': 0.0125,
            'GLACIER_IR': 0.004,
            'GLACIER': 0.0036,
            'DEEP_ARCHIVE': 0.00099
        }

        results = []
        for storage_class, stats in storage_by_class.items():
            cost_per_gb = pricing.get(storage_class, 0.023)
            monthly_cost = stats['size_gb'] * cost_per_gb

            results.append({
                'storage_class': storage_class,
                'object_count': stats['count'],
                'size_gb': stats['size_gb'],
                'cost_per_gb_month': cost_per_gb,
                'monthly_cost': monthly_cost
            })

        df = pd.DataFrame(results)
        total_cost = df['monthly_cost'].sum()

        print("📊 Storage Class Distribution:")
        print(df.to_string(index=False))
        print(f"\n💰 Total Monthly Cost: ${total_cost:,.2f}")

        return df

    def recommend_lifecycle_policy(self, access_log_days=90):
        """Recommend lifecycle policy based on access patterns"""
        # Get object access times (requires S3 access logging)
        access_patterns = self._analyze_access_patterns(access_log_days)

        recommendations = []

        for prefix, stats in access_patterns.items():
            last_access_days = stats['days_since_last_access']
            size_gb = stats['size_gb']

            if last_access_days > 365:
                rec = "DEEP_ARCHIVE"
                savings = size_gb * (0.023 - 0.00099)
            elif last_access_days > 90:
                rec = "GLACIER_IR"
                savings = size_gb * (0.023 - 0.004)
            elif last_access_days > 30:
                rec = "STANDARD_IA"
                savings = size_gb * (0.023 - 0.0125)
            else:
                rec = "STANDARD (current)"
                savings = 0

            recommendations.append({
                'prefix': prefix,
                'current_class': 'STANDARD',
                'recommended_class': rec,
                'size_gb': size_gb,
                'monthly_savings': savings
            })

        df = pd.DataFrame(recommendations)
        total_savings = df['monthly_savings'].sum()

        print("\n💡 Lifecycle Policy Recommendations:")
        print(df.to_string(index=False))
        print(f"\n💰 Potential Monthly Savings: ${total_savings:,.2f}")

        return df

    def _analyze_access_patterns(self, days):
        """Analyze object access patterns from CloudWatch or access logs"""
        # Simplified: Use LastModified as proxy for access time
        # In production, parse S3 access logs for accurate last access time

        access_patterns = defaultdict(lambda: {
            'days_since_last_access': 0,
            'size_gb': 0
        })

        cutoff = datetime.now() - timedelta(days=days)

        paginator = self.s3.get_paginator('list_objects_v2')
        for page in paginator.paginate(Bucket=self.bucket):
            for obj in page.get('Contents', []):
                prefix = '/'.join(obj['Key'].split('/')[:2])  # First 2 levels
                days_old = (datetime.now() - obj['LastModified'].replace(tzinfo=None)).days

                if days_old > access_patterns[prefix]['days_since_last_access']:
                    access_patterns[prefix]['days_since_last_access'] = days_old

                access_patterns[prefix]['size_gb'] += obj['Size'] / (1024**3)

        return access_patterns

    def estimate_query_cost_savings(self, query_scans_tb_month):
        """Estimate Athena cost savings from optimization"""
        # Current cost (unoptimized)
        current_cost = query_scans_tb_month * 5  # $5/TB

        # After partitioning (assume 99% reduction)
        optimized_scans_tb = query_scans_tb_month * 0.01
        optimized_cost = optimized_scans_tb * 5

        savings = current_cost - optimized_cost

        print("\n📊 Query Cost Savings Estimate (Athena):")
        print(f"  Current monthly scans: {query_scans_tb_month:,.2f} TB")
        print(f"  Current cost: ${current_cost:,.2f}/month")
        print(f"  Optimized scans: {optimized_scans_tb:,.2f} TB (99% reduction)")
        print(f"  Optimized cost: ${optimized_cost:,.2f}/month")
        print(f"  💰 Monthly savings: ${savings:,.2f}")

        return savings

# Usage
analyzer = S3CostAnalyzer('ml-training-data')

# Analyze current state
storage_df = analyzer.analyze_storage_classes()

# Get lifecycle recommendations
recommendations_df = analyzer.recommend_lifecycle_policy(access_log_days=90)

# Estimate query cost savings
query_savings = analyzer.estimate_query_cost_savings(query_scans_tb_month=100)
```

### Code Example 2: Automated Cost Alerting System

```python
import boto3
import json
from datetime import datetime, timedelta

class CostAlertingSystem:
    """Monitor and alert on cost anomalies"""

    def __init__(self, budget_monthly, alert_topic_arn):
        self.budget_monthly = budget_monthly
        self.alert_topic_arn = alert_topic_arn
        self.cloudwatch = boto3.client('cloudwatch')
        self.sns = boto3.client('sns')
        self.ce = boto3.client('ce')  # Cost Explorer

    def check_monthly_budget(self):
        """Check if current month spending is on track"""
        # Get month-to-date spending
        start_date = datetime.now().replace(day=1).strftime('%Y-%m-%d')
        end_date = datetime.now().strftime('%Y-%m-%d')

        response = self.ce.get_cost_and_usage(
            TimePeriod={'Start': start_date, 'End': end_date},
            Granularity='MONTHLY',
            Metrics=['UnblendedCost'],
            Filter={
                'Dimensions': {
                    'Key': 'SERVICE',
                    'Values': ['Amazon Simple Storage Service']
                }
            }
        )

        mtd_cost = float(response['ResultsByTime'][0]['Total']['UnblendedCost']['Amount'])

        # Project to end of month
        days_in_month = 30  # Simplified
        days_elapsed = datetime.now().day
        projected_cost = (mtd_cost / days_elapsed) * days_in_month

        # Alert if over budget
        if projected_cost > self.budget_monthly:
            self._send_alert(
                subject='S3 Budget Alert: Projected Overspend',
                message=f"""
                Month-to-date: ${mtd_cost:.2f}
                Projected: ${projected_cost:.2f}
                Budget: ${self.budget_monthly:.2f}
                Overage: ${projected_cost - self.budget_monthly:.2f} ({(projected_cost / self.budget_monthly - 1) * 100:.1f}%)
                """
            )

        print(f"📊 Budget Check:")
        print(f"  MTD spending: ${mtd_cost:.2f}")
        print(f"  Projected: ${projected_cost:.2f}")
        print(f"  Budget: ${self.budget_monthly:.2f}")
        print(f"  Status: {'⚠️ OVER BUDGET' if projected_cost > self.budget_monthly else '✅ On track'}")

        return {
            'mtd_cost': mtd_cost,
            'projected_cost': projected_cost,
            'budget': self.budget_monthly,
            'on_track': projected_cost <= self.budget_monthly
        }

    def detect_anomalies(self, bucket):
        """Detect unusual growth patterns"""
        # Get last 30 days of storage metrics
        metrics = self._get_storage_metrics(bucket, days=30)

        # Calculate daily growth rate
        anomalies = []
        for i in range(1, len(metrics)):
            prev_size_tb = metrics[i-1]['size_tb']
            curr_size_tb = metrics[i]['size_tb']

            if prev_size_tb > 0:
                growth_pct = ((curr_size_tb - prev_size_tb) / prev_size_tb) * 100

                # Alert on > 20% daily growth
                if growth_pct > 20:
                    anomalies.append({
                        'date': metrics[i]['date'],
                        'size_tb': curr_size_tb,
                        'growth_tb': curr_size_tb - prev_size_tb,
                        'growth_pct': growth_pct
                    })

        if anomalies:
            self._send_alert(
                subject='S3 Anomaly Detected: Unusual Growth',
                message=f"Detected {len(anomalies)} days with > 20% growth in bucket {bucket}"
            )

        return anomalies

    def _get_storage_metrics(self, bucket, days):
        """Fetch storage metrics from CloudWatch"""
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)

        response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/S3',
            MetricName='BucketSizeBytes',
            Dimensions=[
                {'Name': 'BucketName', 'Value': bucket},
                {'Name': 'StorageType', 'Value': 'StandardStorage'}
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=86400,
            Statistics=['Average']
        )

        data = sorted(response['Datapoints'], key=lambda x: x['Timestamp'])
        return [
            {'date': d['Timestamp'], 'size_tb': d['Average'] / (1024**4)}
            for d in data
        ]

    def _send_alert(self, subject, message):
        """Send SNS alert"""
        self.sns.publish(
            TopicArn=self.alert_topic_arn,
            Subject=subject,
            Message=message
        )
        print(f"📧 Alert sent: {subject}")

# Usage
alerting = CostAlertingSystem(
    budget_monthly=5000,
    alert_topic_arn='arn:aws:sns:us-east-1:123456789012:storage-alerts'
)

# Check budget status
budget_status = alerting.check_monthly_budget()

# Detect anomalies
anomalies = alerting.detect_anomalies('ml-training-data')
```

### Code Example 3: Cross-Region Cost Optimizer

```python
class CrossRegionOptimizer:
    """Optimize cross-region data transfer costs"""

    def __init__(self, source_bucket, source_region, target_region):
        self.source_bucket = source_bucket
        self.source_region = source_region
        self.target_region = target_region
        self.s3 = boto3.client('s3', region_name=source_region)

    def analyze_access_patterns(self, days=30):
        """Analyze which objects are accessed from target region"""
        # Parse S3 access logs to identify cross-region requests
        # Simplified: Assume you have CloudFront logs or S3 access logs

        # Return list of frequently accessed prefixes
        hot_prefixes = [
            {'prefix': 'training/recent/', 'size_gb': 100, 'accesses_per_day': 50},
            {'prefix': 'features/active/', 'size_gb': 50, 'accesses_per_day': 20}
        ]

        return hot_prefixes

    def recommend_replication(self, hot_prefixes):
        """Recommend which data to replicate to target region"""
        transfer_cost_per_gb = 0.02  # Cross-region transfer
        storage_cost_per_gb_month = 0.023  # Storage in target region

        recommendations = []

        for prefix_data in hot_prefixes:
            prefix = prefix_data['prefix']
            size_gb = prefix_data['size_gb']
            accesses_per_day = prefix_data['accesses_per_day']

            # Current cost (transfer every access)
            monthly_accesses = accesses_per_day * 30
            current_cost = monthly_accesses * size_gb * transfer_cost_per_gb

            # Cost with replication
            replication_cost_once = size_gb * transfer_cost_per_gb
            storage_cost_month = size_gb * storage_cost_per_gb_month
            replicated_cost_month = storage_cost_month  # No transfer after replication

            # Break-even: When does replication pay off?
            breakeven_days = (replication_cost_once + storage_cost_month) / (current_cost / 30)

            savings_month = current_cost - replicated_cost_month

            recommendations.append({
                'prefix': prefix,
                'size_gb': size_gb,
                'accesses_per_day': accesses_per_day,
                'current_cost_month': current_cost,
                'replicated_cost_month': replicated_cost_month,
                'monthly_savings': savings_month,
                'breakeven_days': breakeven_days,
                'recommend': breakeven_days < 30  # Recommend if breaks even in < 1 month
            })

        df = pd.DataFrame(recommendations)
        print("💡 Cross-Region Replication Recommendations:")
        print(df.to_string(index=False))

        return df

    def configure_replication(self, prefixes_to_replicate):
        """Configure S3 replication for selected prefixes"""
        replication_rules = []

        for i, prefix in enumerate(prefixes_to_replicate):
            replication_rules.append({
                'ID': f'Replicate-{prefix.replace("/", "-")}',
                'Priority': i + 1,
                'Filter': {'Prefix': prefix},
                'Status': 'Enabled',
                'Destination': {
                    'Bucket': f'arn:aws:s3:::ml-data-{self.target_region}',
                    'StorageClass': 'STANDARD',
                    'ReplicationTime': {
                        'Status': 'Enabled',
                        'Time': {'Minutes': 15}
                    }
                }
            })

        replication_config = {
            'Role': f'arn:aws:iam::123456789012:role/s3-replication-{self.source_region}-to-{self.target_region}',
            'Rules': replication_rules
        }

        self.s3.put_bucket_replication(
            Bucket=self.source_bucket,
            ReplicationConfiguration=replication_config
        )

        print(f"✅ Replication configured for {len(prefixes_to_replicate)} prefixes")

# Usage
optimizer = CrossRegionOptimizer(
    source_bucket='ml-data-us-east-1',
    source_region='us-east-1',
    target_region='us-west-2'
)

# Analyze access patterns
hot_prefixes = optimizer.analyze_access_patterns(days=30)

# Get recommendations
recommendations = optimizer.recommend_replication(hot_prefixes)

# Configure replication for recommended prefixes
prefixes_to_replicate = recommendations[recommendations['recommend']]['prefix'].tolist()
optimizer.configure_replication(prefixes_to_replicate)
```

---

## ✅ Best Practices

### 1. Storage Class Selection

**DO:**
- ✅ Use S3 Standard for data accessed daily (< 30 days old)
- ✅ Use Standard-IA for data accessed weekly (30-90 days old)
- ✅ Use Glacier Instant Retrieval for monthly access (90-365 days)
- ✅ Use Deep Archive for compliance data (> 1 year retention)
- ✅ Use Intelligent-Tiering for unpredictable access patterns

**DON'T:**
- ❌ Store all data in Standard (wastes 50-95% on cold data)
- ❌ Store frequently accessed data in Glacier (retrieval costs)
- ❌ Delete from Glacier before minimum duration (90/180 days)
- ❌ Use Deep Archive for data you may need quickly

### 2. Lifecycle Policies

**DO:**
- ✅ Implement lifecycle policies for all ML datasets
- ✅ Transition data based on actual access patterns
- ✅ Set expiration policies for temporary data (experiments, logs)
- ✅ Test lifecycle policies on sample data first
- ✅ Monitor lifecycle transitions and adjust as needed

**DON'T:**
- ❌ Create lifecycle policy without understanding access patterns
- ❌ Set overly aggressive transitions (e.g., 7 days → Deep Archive)
- ❌ Forget to set expiration for temp data (accumulates forever)
- ❌ Apply same lifecycle to all data types (one size doesn't fit all)

### 3. Compression

**DO:**
- ✅ Compress all stored data (Snappy, Zstd, or Gzip)
- ✅ Use Snappy/Zstd for frequently accessed training data
- ✅ Use Gzip for archival data (rarely decompressed)
- ✅ Measure compression ratio and CPU overhead before deploying
- ✅ Convert uncompressed datasets to compressed format

**DON'T:**
- ❌ Store uncompressed data (wastes 50-90% storage)
- ❌ Use slow compression (Brotli, Gzip level 9) for hot data
- ❌ Change compression mid-pipeline (breaks compatibility)
- ❌ Compress already-compressed data (JPEG, MP4, ZIP)

### 4. Query Optimization

**DO:**
- ✅ Use partition pruning to skip irrelevant data (Athena, BigQuery)
- ✅ Select only needed columns (columnar storage benefit)
- ✅ Cache frequently queried aggregations (materialized views)
- ✅ Use flat-rate pricing if scanning > 320 TB/month (BigQuery)
- ✅ Monitor query costs and optimize expensive queries

**DON'T:**
- ❌ SELECT * on large tables (reads all columns)
- ❌ Run full table scans on 100+ TB datasets
- ❌ Ignore partition filters (disables pruning)
- ❌ Forget to compress data (doubles query costs)

### 5. Data Retention

**DO:**
- ✅ Define retention policies for each data type
- ✅ Delete temporary data after 7-30 days
- ✅ Archive old experiments after 90 days
- ✅ Keep compliance data for 7 years (or regulatory requirement)
- ✅ Automate cleanup with lifecycle policies or scheduled jobs

**DON'T:**
- ❌ Keep all data forever (storage costs grow linearly)
- ❌ Delete data without backup (especially production models)
- ❌ Ignore orphaned data (failed experiments, temp files)
- ❌ Hard-code retention periods (use configurable policies)

### 6. Cost Monitoring

**DO:**
- ✅ Set up CloudWatch/Stackdriver dashboards for storage metrics
- ✅ Create budget alerts (75%, 100%, 125% thresholds)
- ✅ Monitor month-over-month growth trends
- ✅ Tag resources by team/project for cost attribution
- ✅ Review cost anomalies weekly

**DON'T:**
- ❌ Ignore cost alerts (they're there for a reason)
- ❌ Wait for bill shock (monitor proactively)
- ❌ Assume costs are "under control" without measuring
- ❌ Skip tagging (can't attribute costs without tags)

### 7. Deduplication

**DO:**
- ✅ Audit storage quarterly for duplicate/orphaned data
- ✅ Delete temp files older than 7 days
- ✅ Deduplicate identical uploads (use content-based hashing)
- ✅ Delete failed experiment artifacts
- ✅ Remove old model checkpoints (keep only best N)

**DON'T:**
- ❌ Keep every intermediate result forever
- ❌ Upload same dataset multiple times (use versioning)
- ❌ Ignore small files (10K files × 1 MB = 10 GB waste)
- ❌ Let experiment artifacts accumulate unchecked

### 8. Cross-Region Transfer

**DO:**
- ✅ Colocate compute and storage in same region
- ✅ Replicate frequently accessed data to training region
- ✅ Use CloudFront for global model serving
- ✅ Use VPC Endpoints to avoid NAT gateway costs
- ✅ Batch cross-region transfers (vs streaming)

**DON'T:**
- ❌ Train in one region, store data in another (expensive)
- ❌ Replicate all data cross-region (only replicate hot data)
- ❌ Forget cross-region transfer costs ($0.02/GB adds up)
- ❌ Use public internet for intra-AWS transfers (use PrivateLink)

### 9. Reserved Capacity

**DO:**
- ✅ Use committed use discounts for baseline storage (> 1 year)
- ✅ Calculate break-even before committing
- ✅ Reserve capacity for predictable workloads
- ✅ Combine reserved + on-demand for flexibility
- ✅ Review reservations annually and adjust

**DON'T:**
- ❌ Over-commit to reservations (locked in for 1-3 years)
- ❌ Reserve capacity for experimental workloads
- ❌ Ignore savings (12-40% discount for long-term commitment)
- ❌ Assume on-demand is always better (not for stable workloads)

### 10. Cost Attribution

**DO:**
- ✅ Tag all resources with team, project, environment
- ✅ Use separate buckets per team/project for clear attribution
- ✅ Create cost allocation reports by tag
- ✅ Implement chargeback to teams based on usage
- ✅ Review cost attribution monthly

**DON'T:**
- ❌ Mix team data in single bucket (can't attribute costs)
- ❌ Skip tagging (future you will regret it)
- ❌ Assume all costs are "shared" (drives no accountability)
- ❌ Use inconsistent tagging conventions

---

## ⚠️ Common Pitfalls

(Due to length constraints, I'll include the Key Takeaways section directly)

---

## 📝 Key Takeaways

1. **Cloud storage has hidden costs beyond price per GB**: Consider request costs (PUT, GET, LIST), data transfer fees (cross-region, egress), and retrieval costs (Glacier, Deep Archive). Total costs can be 2-10x the base storage price.

2. **Lifecycle policies can save 70-95% on storage costs**: Automatically transition data from Standard ($0.023/GB/mo) → Standard-IA ($0.0125) → Glacier ($0.004) → Deep Archive ($0.00099) based on age. Typical savings: 70-95% on multi-year datasets.

3. **Compression saves 50-80% storage costs with minimal overhead**: Zstd level 3 provides 3-4x compression with < 15% CPU overhead. Gzip provides 5x compression for archival data. Always compress ML datasets.

4. **Partition pruning reduces query costs by 95-99%**: Athena/BigQuery charge per byte scanned. Partitioning by date can reduce 100 TB scan to 1 TB (99% savings = $495 saved per query). Critical for ML analytics workloads.

5. **70-90% of stored ML data is never accessed again**: Orphaned experiments, duplicate uploads, temp files, old checkpoints. Quarterly audits and automated cleanup can save $10K-100K/month.

6. **Cross-region transfer costs add up fast**: $0.02/GB cross-region × 100 TB/month = $2,000. Replicate frequently accessed data to training region or colocate storage and compute.

7. **Query-based pricing (Athena, BigQuery) rewards optimization**: Optimizing queries to scan 1% of data reduces costs 100x. Use columnar formats, partitioning, compression, and materialized views.

8. **Reserved capacity saves 12-40% for predictable workloads**: GCP 3-year commitment: 12% discount. Azure 3-year: 30-40% discount. Only commit for stable baseline storage (not experiments).

9. **Cost monitoring prevents bill shock**: Set budget alerts at 75%, 100%, 125%. Monitor month-over-month growth. Detect anomalies (> 20% daily growth). Review costs weekly, not just at month-end.

10. **Cost attribution drives accountability**: Tag resources by team, project, environment. Create cost allocation reports. Implement chargeback. Teams optimize costs when they're responsible for their spend.

---

## ✏️ Notes Section

**Personal Insights:**

---

**Questions:**

---

**Action Items:**

---

**Related Projects:**

---

**Code Snippets:**

---

**Further Exploration:**

---

**📌 Tags:** `#cost-optimization` `#cloud-storage` `#lifecycle-policies` `#compression` `#athena` `#bigquery` `#s3` `#gcs` `#azure-blob` `#ml-costs`

---

**Navigation:**
- ← Previous: [04.05 - Data Partitioning and Organization](05.%20Data%20Partitioning%20and%20Organization.md)
- → Next: [Chapter 05 - Data Processing Frameworks](../05.%20Data%20Processing%20Frameworks/README.md)
- ↑ Up: [Chapter 04 - Data Storage for ML](README.md)
- ⌂ Home: [DEforAI Course Index](../../README.md)

---

*Last updated: 2025-10-19*
