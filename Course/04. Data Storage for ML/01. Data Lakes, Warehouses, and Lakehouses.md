# 01. Data Lakes, Warehouses, and Lakehouses

**Date:** 2025-10-19
**Status:** #research
**Tags:** #data-engineering #ml-systems #data-lakes #data-warehouses #lakehouses #delta-lake #iceberg #storage

---

## 📋 Overview

Choosing the right storage architecture is one of the most critical decisions for ML systems. The architecture you select fundamentally shapes data access patterns, query performance, cost structure, and operational complexity.

**Data Lakes**, **Data Warehouses**, and **Lakehouses** represent three distinct architectural approaches to storing and managing ML data, each with specific strengths:

- **Data Lakes** store raw, unstructured data at massive scale with low cost, ideal for exploratory ML and storing diverse data formats (images, text, logs, events)
- **Data Warehouses** provide structured, optimized storage for analytics with strong ACID guarantees, excellent for feature serving and SQL-based ML
- **Lakehouses** combine the flexibility of data lakes with the performance and reliability of warehouses, representing the modern best practice for ML workloads (2024-2025)

Understanding these architectures enables you to design storage systems that balance cost, performance, and operational simplicity. Modern ML platforms increasingly adopt lakehouse architectures (Delta Lake, Apache Iceberg, Apache Hudi) to gain the benefits of both worlds: cheap object storage with warehouse-like features (ACID transactions, schema enforcement, time travel).

**Key Decision Factors**:
- **Data Types**: Structured (tables) vs unstructured (images, text)
- **Query Patterns**: SQL analytics vs full table scans for training
- **Update Frequency**: Append-only vs frequent updates/deletes
- **Cost Sensitivity**: Storage cost vs query performance trade-offs
- **ML Use Cases**: Batch training, real-time serving, exploratory analysis

---

## 🎯 Learning Objectives

By the end of this subchapter, you will:

1. **Differentiate between data lakes, warehouses, and lakehouses** and understand the trade-offs of each architecture
2. **Design data lake architectures** with medallion pattern (bronze, silver, gold layers)
3. **Select appropriate data warehouse solutions** (Snowflake, BigQuery, Redshift) based on ML requirements
4. **Implement lakehouse patterns** using Delta Lake, Apache Iceberg, or Apache Hudi
5. **Compare ACID transaction support** across architectures for ML data consistency
6. **Optimize storage formats** (Parquet, ORC, Avro) for ML training and serving
7. **Implement schema evolution strategies** to handle changing data structures
8. **Design partitioning schemes** for efficient data access in each architecture
9. **Calculate total cost of ownership** (TCO) for different storage architectures
10. **Migrate between architectures** (lake → warehouse → lakehouse) with minimal disruption

---

## 📚 Core Concepts

### 1. Data Lake Architecture

**Definition**: Centralized repository storing vast amounts of raw data in native format (structured, semi-structured, unstructured) at low cost.

**Core Components**:
```
Data Lake Architecture:
├── Object Storage (S3, GCS, Azure Blob)
│   ├── Raw/Bronze Layer (landing zone)
│   ├── Curated/Silver Layer (cleaned, validated)
│   └── Aggregated/Gold Layer (feature tables)
├── Metadata Catalog (Glue, Hive Metastore)
├── Processing Engines (Spark, Presto, Athena)
└── Access Control (IAM, Lake Formation)
```

**Characteristics**:
- **Schema-on-Read**: No schema enforcement at write time; schema applied when reading
- **Low-Cost Storage**: Leverages cheap object storage (~$0.023/GB/month for S3 Standard)
- **Flexible Data Formats**: Parquet, Avro, JSON, CSV, images, videos
- **Scalable**: Store petabytes of data without performance degradation
- **ELT Pattern**: Extract, Load, then Transform (transform on read)

**When to Use Data Lakes**:
- **Exploratory ML**: Experimenting with diverse data sources
- **Unstructured Data**: Images, text, audio, video for deep learning
- **Data Science Workloads**: Ad-hoc analysis, notebook-based exploration
- **Long-Term Archival**: Historical data for model retraining
- **Cost-Sensitive Workloads**: Storing large volumes cheaply

**Limitations**:
- **No ACID Transactions**: Risk of data inconsistency during concurrent writes
- **Schema Drift**: No enforcement leads to inconsistent data quality
- **Slow Queries**: Full table scans for unpartitioned data
- **Data Swamp Risk**: Without governance, becomes unusable

**Example: AWS Data Lake**:
```python
# Data Lake structure on S3
s3://ml-data-lake/
├── raw/                          # Bronze layer: raw data
│   ├── events/
│   │   ├── year=2025/
│   │   │   ├── month=10/
│   │   │   │   ├── day=19/
│   │   │   │   │   └── events.parquet
│   ├── transactions/
│   └── user_profiles/
├── curated/                      # Silver layer: cleaned
│   ├── events_validated/
│   ├── transactions_deduped/
│   └── user_profiles_enriched/
└── features/                     # Gold layer: ML features
    ├── user_features/
    ├── product_features/
    └── transaction_features/
```

**Querying Data Lake with Athena**:
```sql
-- Create external table pointing to S3
CREATE EXTERNAL TABLE IF NOT EXISTS ml_data_lake.events (
    user_id STRING,
    event_type STRING,
    timestamp TIMESTAMP,
    properties MAP<STRING, STRING>
)
PARTITIONED BY (year INT, month INT, day INT)
STORED AS PARQUET
LOCATION 's3://ml-data-lake/raw/events/';

-- Add partitions (necessary for performance)
ALTER TABLE ml_data_lake.events ADD IF NOT EXISTS
PARTITION (year=2025, month=10, day=19)
LOCATION 's3://ml-data-lake/raw/events/year=2025/month=10/day=19/';

-- Query for ML feature extraction
SELECT
    user_id,
    COUNT(*) as event_count,
    COUNT(DISTINCT event_type) as unique_event_types
FROM ml_data_lake.events
WHERE year = 2025 AND month = 10
GROUP BY user_id;
```

---

### 2. Data Warehouse Architecture

**Definition**: Centralized, structured repository optimized for analytics with strong consistency guarantees and performance optimization.

**Characteristics**:
- **Schema-on-Write**: Schema enforced at write time (data validation)
- **ACID Transactions**: Atomicity, Consistency, Isolation, Durability
- **Optimized for SQL**: Columnar storage, query optimization, materialized views
- **Managed Service**: Auto-scaling, automatic optimization (Snowflake, BigQuery)
- **Separation of Compute/Storage**: Independent scaling
- **ETL Pattern**: Extract, Transform, then Load (transform before storage)

**Popular Data Warehouses for ML**:

| **Warehouse** | **Best For** | **Pricing** | **ML Features** |
|--------------|-------------|------------|-----------------|
| **Snowflake** | Multi-cloud, SQL-heavy ML | $2-3/TB/month storage + compute | Snowpark (Python/Java UDFs), ML functions |
| **Google BigQuery** | GCP ecosystem, serverless | $20/TB query + $0.02/GB storage | BigQuery ML (in-SQL ML), AutoML integration |
| **Amazon Redshift** | AWS ecosystem, cost-sensitive | $0.024/GB/month + compute | Redshift ML (SageMaker integration) |
| **Databricks SQL** | Spark-based, lakehouse hybrid | $0.75/DBU | Delta Lake, Unity Catalog, MLflow |

**When to Use Data Warehouses**:
- **Structured ML Features**: Tabular data, aggregated features
- **SQL-Based ML**: In-database ML training (BigQuery ML, Snowpark ML)
- **BI + ML Integration**: Dashboards alongside ML features
- **Strong Consistency**: ACID guarantees for critical data
- **High-Performance Queries**: Sub-second SQL queries

**Limitations**:
- **Higher Cost**: 10-100x more expensive than data lakes for storage
- **Structured Data Only**: Not ideal for images, text, unstructured data
- **Vendor Lock-In**: Hard to migrate between warehouses
- **ETL Complexity**: Transform before load adds latency

**Example: Snowflake for ML Features**:
```sql
-- Create feature table with schema enforcement
CREATE OR REPLACE TABLE ml_features.user_features (
    user_id STRING NOT NULL,
    feature_date DATE NOT NULL,
    purchase_count_30d INT,
    avg_purchase_amount FLOAT,
    category_preferences ARRAY,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    PRIMARY KEY (user_id, feature_date)
);

-- Insert features with validation
INSERT INTO ml_features.user_features
SELECT
    user_id,
    CURRENT_DATE() as feature_date,
    COUNT(*) as purchase_count_30d,
    AVG(amount) as avg_purchase_amount,
    ARRAY_AGG(DISTINCT category) as category_preferences,
    CURRENT_TIMESTAMP() as created_at
FROM transactions
WHERE transaction_date >= DATEADD(day, -30, CURRENT_DATE())
GROUP BY user_id;

-- Time travel (Snowflake)
SELECT * FROM ml_features.user_features
AT(TIMESTAMP => DATEADD(hour, -24, CURRENT_TIMESTAMP()));  -- Data as of 24 hours ago

-- Zero-copy clone for experimentation
CREATE TABLE ml_features.user_features_experiment
CLONE ml_features.user_features;
```

**BigQuery ML Example** (Train model in SQL):
```sql
-- Create logistic regression model directly in BigQuery
CREATE OR REPLACE MODEL ml_models.churn_predictor
OPTIONS(
  model_type='LOGISTIC_REG',
  input_label_cols=['churned']
) AS
SELECT
    purchase_count_30d,
    avg_purchase_amount,
    days_since_last_purchase,
    churned
FROM ml_features.user_features;

-- Predict churn
SELECT
    user_id,
    predicted_churned,
    predicted_churned_probs[OFFSET(1)].prob as churn_probability
FROM ML.PREDICT(MODEL ml_models.churn_predictor,
    (SELECT * FROM ml_features.user_features WHERE feature_date = CURRENT_DATE())
);
```

---

### 3. Lakehouse Architecture

**Definition**: Unified architecture combining the low-cost, flexible storage of data lakes with the ACID transactions and performance of data warehouses.

**Key Technologies**:

**Delta Lake** (Databricks):
- ACID transactions on S3/GCS/Azure Blob
- Time travel and versioning
- Schema enforcement and evolution
- Merge, Update, Delete operations
- Z-ordering for performance

**Apache Iceberg** (Netflix):
- Hidden partitioning (automatic partition pruning)
- Partition evolution (change partitioning without rewriting)
- Schema evolution (add/remove columns)
- Snapshot isolation and time travel

**Apache Hudi** (Uber):
- Upserts on data lakes
- Incremental processing
- Copy-on-write (COW) and merge-on-read (MOR) tables
- Record-level updates

**Lakehouse Benefits**:
```
✅ Low-cost object storage (same as data lake)
✅ ACID transactions (same as warehouse)
✅ Schema enforcement (same as warehouse)
✅ Time travel (version control for data)
✅ Efficient upserts/deletes (slowly changing dimensions)
✅ Unified batch + streaming (single architecture)
✅ Open formats (no vendor lock-in)
```

**When to Use Lakehouses**:
- **Modern ML Platforms**: Best practice for 2024-2025
- **Streaming + Batch**: Unified architecture for both
- **Data Versioning**: Time travel for reproducible ML
- **CDC Workloads**: Change Data Capture with upserts
- **Cost + Performance**: Need both low storage cost and fast queries

**Lakehouse Architecture**:
```
Lakehouse (Delta Lake Example):
├── Object Storage (S3/GCS/Azure Blob)
│   ├── _delta_log/ (transaction log)
│   │   ├── 00000000000000000000.json
│   │   ├── 00000000000000000001.json
│   │   └── 00000000000000000002.json
│   ├── part-00000.parquet
│   ├── part-00001.parquet
│   └── part-00002.parquet
├── Compute Engines
│   ├── Apache Spark (batch processing)
│   ├── Presto/Trino (SQL queries)
│   └── Flink (streaming ingestion)
└── Catalog (Unity Catalog, Glue)
```

---

### 4. Comparison Matrix

| **Feature** | **Data Lake** | **Data Warehouse** | **Lakehouse** |
|------------|--------------|-------------------|---------------|
| **Storage Cost** | Low ($0.023/GB) | High ($0.10-$0.50/GB) | Low ($0.023/GB) |
| **Query Performance** | Slow (full scans) | Fast (optimized) | Fast (with optimization) |
| **ACID Transactions** | ❌ No | ✅ Yes | ✅ Yes |
| **Schema Enforcement** | ❌ No | ✅ Yes | ✅ Yes |
| **Data Types** | All (images, text, tables) | Structured only | All |
| **Update/Delete** | ❌ Difficult | ✅ Easy | ✅ Easy |
| **Time Travel** | ❌ No | ✅ Limited (Snowflake) | ✅ Yes |
| **Vendor Lock-In** | Low (open formats) | High | Low (open formats) |
| **ETL Complexity** | Low (ELT) | High (ETL) | Low (ELT) |
| **Best For ML** | Exploration, raw data | SQL-based ML, features | Modern ML platforms |

**Cost Comparison** (1 TB data, 1 year):
```
Data Lake (S3 Standard):
- Storage: 1000 GB × $0.023/GB/month × 12 = $276/year
- Queries (Athena): 100 TB scanned × $5/TB = $500/year
- Total: ~$776/year

Data Warehouse (Snowflake):
- Storage: 1000 GB × $0.04/GB/month × 12 = $480/year
- Compute: 100 queries × 5 sec × $2/credit ÷ 3600 = ~$2,000/year
- Total: ~$2,480/year

Lakehouse (Delta Lake on S3):
- Storage: 1000 GB × $0.023/GB/month × 12 = $276/year
- Compute (Spark): 100 hours × $0.20/DBU × 2 DBUs = $40/year
- Total: ~$316/year
```

**Lakehouse is most cost-effective for ML workloads.**

---

### 5. Medallion Architecture (Bronze-Silver-Gold)

**Pattern**: Organize data lake/lakehouse into layers based on quality and transformation level.

**Bronze Layer** (Raw):
- **Purpose**: Landing zone for raw data
- **Format**: Native format (JSON, CSV, Avro) or Parquet
- **Schema**: Schema-on-read, minimal validation
- **Retention**: Long-term (years)
- **Example**: Raw clickstream events, API responses, database dumps

**Silver Layer** (Curated):
- **Purpose**: Cleaned, validated, deduplicated data
- **Format**: Parquet with compression
- **Schema**: Enforced schema, type validation
- **Retention**: Medium-term (months to years)
- **Example**: Validated user events, deduplicated transactions

**Gold Layer** (Aggregated):
- **Purpose**: Business-level aggregations, ML features
- **Format**: Parquet, Delta Lake tables
- **Schema**: Strictly enforced with versioning
- **Retention**: As needed for ML serving
- **Example**: Daily user features, product embeddings, aggregated metrics

**Benefits**:
- **Incremental Processing**: Only reprocess from bronze to silver if needed
- **Data Quality Zones**: Separate raw from clean data
- **Multiple Gold Tables**: Create different views for different ML models
- **Auditability**: Trace feature values back to raw data

---

### 6. ACID Transactions for ML

**Why ACID Matters for ML**:
- **Training-Serving Consistency**: Ensure same data seen during training and serving
- **Feature Consistency**: Prevent partial updates (half old, half new features)
- **Concurrent Writes**: Multiple jobs updating same table without corruption
- **Rollback Capability**: Revert bad feature updates

**ACID in Different Architectures**:

**Data Lake (S3 + Parquet)**: ❌ No ACID
```python
# Problem: Concurrent writes can corrupt data
# Job 1: Writing user_features for users 1-1000
# Job 2: Writing user_features for users 1001-2000
# Risk: Partial reads, overwritten files, inconsistent data
```

**Data Warehouse (Snowflake)**: ✅ Full ACID
```sql
-- Atomic update: all or nothing
BEGIN TRANSACTION;
    DELETE FROM user_features WHERE feature_date = CURRENT_DATE();
    INSERT INTO user_features SELECT * FROM staging_user_features;
COMMIT;
```

**Lakehouse (Delta Lake)**: ✅ Full ACID
```python
from delta.tables import DeltaTable

# Atomic merge (upsert)
delta_table = DeltaTable.forPath(spark, "s3://ml-features/user_features")

delta_table.alias("target").merge(
    new_features.alias("source"),
    "target.user_id = source.user_id AND target.feature_date = source.feature_date"
).whenMatchedUpdateAll() \
 .whenNotMatchedInsertAll() \
 .execute()
```

---

### 7. Schema Evolution

**Challenge**: ML features change over time (new features added, old features deprecated).

**Schema Evolution Strategies**:

**1. Additive Changes** (Safe):
```python
# v1: Original schema
user_features_v1 = {
    "user_id": "string",
    "purchase_count_30d": "int"
}

# v2: Add new feature (backward compatible)
user_features_v2 = {
    "user_id": "string",
    "purchase_count_30d": "int",
    "avg_purchase_amount": "float"  # New field
}
```

**2. Breaking Changes** (Requires versioning):
```python
# Rename field: purchase_count_30d → transaction_count_30d
# Solution: Create new table version
s3://ml-features/user_features/v1/  # Old schema
s3://ml-features/user_features/v2/  # New schema
```

**Delta Lake Schema Evolution**:
```python
# Enable automatic schema merging
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")

# New data with additional column
new_features = spark.createDataFrame([
    ("user_123", 10, 120.5, "2025-10-19")  # Added avg_purchase_amount
], ["user_id", "purchase_count_30d", "avg_purchase_amount", "feature_date"])

# Delta Lake automatically merges schema
new_features.write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save("s3://ml-features/user_features")
```

**Iceberg Schema Evolution**:
```python
from pyspark.sql.types import StructType, StructField, FloatType

# Add column to existing table
spark.sql("""
    ALTER TABLE ml_features.user_features
    ADD COLUMN category_diversity FLOAT
""")

# Rename column (Iceberg supports this natively)
spark.sql("""
    ALTER TABLE ml_features.user_features
    RENAME COLUMN purchase_count_30d TO transaction_count_30d
""")
```

---

## 💡 Practical Examples

### Example 1: Building a Data Lake for ML

**Scenario**: E-commerce company storing diverse data (events, transactions, images) for ML.

**Implementation**:

```python
import boto3
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Initialize Spark
spark = SparkSession.builder \
    .appName("MLDataLake") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# 1. Bronze Layer: Ingest raw events from Kafka to S3
raw_events = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "user_events") \
    .load()

# Write to bronze layer (raw JSON)
raw_events.select(F.col("value").cast("string").alias("event_data")) \
    .writeStream \
    .format("parquet") \
    .option("path", "s3://ml-data-lake/bronze/events/") \
    .option("checkpointLocation", "s3://ml-data-lake/checkpoints/bronze_events/") \
    .partitionBy("year", "month", "day") \
    .start()

# 2. Silver Layer: Clean and validate
from pyspark.sql.types import StructType, StructField, StringType, TimestampType

event_schema = StructType([
    StructField("user_id", StringType(), False),
    StructField("event_type", StringType(), False),
    StructField("timestamp", TimestampType(), False),
    StructField("product_id", StringType(), True),
    StructField("amount", FloatType(), True)
])

# Read from bronze, parse JSON, validate
bronze_events = spark.read.parquet("s3://ml-data-lake/bronze/events/")

silver_events = bronze_events \
    .select(F.from_json(F.col("event_data"), event_schema).alias("event")) \
    .select("event.*") \
    .filter(F.col("user_id").isNotNull()) \
    .filter(F.col("timestamp").isNotNull()) \
    .dropDuplicates(["user_id", "timestamp", "event_type"])

# Write to silver layer with Delta Lake (ACID)
silver_events.write.format("delta") \
    .mode("overwrite") \
    .partitionBy("event_type") \
    .save("s3://ml-data-lake/silver/events/")

# 3. Gold Layer: Aggregate features for ML
user_features = silver_events \
    .groupBy("user_id") \
    .agg(
        F.count("*").alias("total_events"),
        F.countDistinct("event_type").alias("unique_event_types"),
        F.sum(F.when(F.col("event_type") == "purchase", 1).otherwise(0)).alias("purchase_count"),
        F.avg("amount").alias("avg_purchase_amount"),
        F.max("timestamp").alias("last_event_timestamp")
    )

# Write to gold layer (ML features)
user_features.write.format("delta") \
    .mode("overwrite") \
    .save("s3://ml-data-lake/gold/user_features/")

# 4. Query gold layer for ML training
training_data = spark.read.format("delta").load("s3://ml-data-lake/gold/user_features/")

# Join with labels for supervised learning
labels = spark.read.parquet("s3://ml-data-lake/gold/user_labels/")
ml_dataset = training_data.join(labels, "user_id")

# Save as training dataset
ml_dataset.write.format("parquet") \
    .mode("overwrite") \
    .save("s3://ml-training-data/churn_prediction/dataset_v1/")
```

---

### Example 2: Migrating from Data Lake to Lakehouse (Delta Lake)

**Scenario**: Existing Parquet data lake, want ACID transactions and schema enforcement.

**Migration Steps**:

```python
from delta.tables import DeltaTable

# Step 1: Convert existing Parquet table to Delta Lake
parquet_path = "s3://ml-data-lake/features/user_features/"
delta_path = "s3://ml-lakehouse/features/user_features/"

# Read existing Parquet data
df = spark.read.parquet(parquet_path)

# Write as Delta Lake table
df.write.format("delta") \
    .mode("overwrite") \
    .partitionBy("feature_date") \
    .save(delta_path)

# Step 2: Create Delta table in catalog
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS ml_lakehouse.user_features
    USING DELTA
    LOCATION '{delta_path}'
""")

# Step 3: Enable features (auto-optimize, auto-compact)
spark.sql("""
    ALTER TABLE ml_lakehouse.user_features
    SET TBLPROPERTIES (
        'delta.autoOptimize.optimizeWrite' = 'true',
        'delta.autoOptimize.autoCompact' = 'true'
    )
""")

# Step 4: Implement CDC (Change Data Capture) with merge
def upsert_user_features(new_features_df):
    """
    Merge new features into existing Delta table (upsert pattern)
    """
    delta_table = DeltaTable.forPath(spark, delta_path)

    delta_table.alias("target").merge(
        new_features_df.alias("source"),
        "target.user_id = source.user_id AND target.feature_date = source.feature_date"
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()

# Usage
new_features = spark.read.parquet("s3://ml-data-lake/staging/new_user_features/")
upsert_user_features(new_features)

# Step 5: Time travel (restore to previous version if needed)
# Query data as of yesterday
df_yesterday = spark.read.format("delta") \
    .option("timestampAsOf", "2025-10-18") \
    .load(delta_path)

# Restore table to previous version
delta_table = DeltaTable.forPath(spark, delta_path)
delta_table.restoreToVersion(5)  # Restore to version 5

# Step 6: Optimize table (compaction, Z-ordering)
spark.sql("""
    OPTIMIZE ml_lakehouse.user_features
    ZORDER BY (user_id)
""")

# Step 7: Vacuum old files (delete files older than retention period)
spark.sql("""
    VACUUM ml_lakehouse.user_features RETAIN 168 HOURS
""")  # Retain 7 days of history
```

---

### Example 3: Snowflake Data Warehouse for ML Features

**Scenario**: Feature store in Snowflake with time travel and zero-copy cloning.

**Implementation**:

```sql
-- 1. Create database and schema for ML
CREATE DATABASE IF NOT EXISTS ml_platform;
CREATE SCHEMA IF NOT EXISTS ml_platform.features;

-- 2. Create feature table with time travel
CREATE OR REPLACE TABLE ml_platform.features.user_features (
    user_id STRING NOT NULL,
    feature_date DATE NOT NULL,
    purchase_count_7d INT,
    purchase_count_30d INT,
    avg_purchase_amount FLOAT,
    category_preferences ARRAY,
    last_purchase_date DATE,
    days_since_last_purchase INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    CONSTRAINT pk_user_features PRIMARY KEY (user_id, feature_date)
) CHANGE_TRACKING = TRUE;  -- Enable CDC

-- 3. Create stream for CDC (capture changes)
CREATE OR REPLACE STREAM user_features_changes
ON TABLE ml_platform.features.user_features;

-- 4. Populate features from transactions
INSERT INTO ml_platform.features.user_features
SELECT
    user_id,
    CURRENT_DATE() as feature_date,
    COUNT(CASE WHEN transaction_date >= DATEADD(day, -7, CURRENT_DATE()) THEN 1 END) as purchase_count_7d,
    COUNT(CASE WHEN transaction_date >= DATEADD(day, -30, CURRENT_DATE()) THEN 1 END) as purchase_count_30d,
    AVG(amount) as avg_purchase_amount,
    ARRAY_AGG(DISTINCT category) WITHIN GROUP (ORDER BY category) as category_preferences,
    MAX(transaction_date) as last_purchase_date,
    DATEDIFF(day, MAX(transaction_date), CURRENT_DATE()) as days_since_last_purchase,
    CURRENT_TIMESTAMP() as created_at,
    CURRENT_TIMESTAMP() as updated_at
FROM transactions
WHERE transaction_date >= DATEADD(day, -30, CURRENT_DATE())
GROUP BY user_id;

-- 5. Update features (merge pattern for upsert)
MERGE INTO ml_platform.features.user_features AS target
USING (
    -- New features computed from staging
    SELECT * FROM staging.user_features_new
) AS source
ON target.user_id = source.user_id AND target.feature_date = source.feature_date
WHEN MATCHED THEN
    UPDATE SET
        target.purchase_count_30d = source.purchase_count_30d,
        target.avg_purchase_amount = source.avg_purchase_amount,
        target.updated_at = CURRENT_TIMESTAMP()
WHEN NOT MATCHED THEN
    INSERT (user_id, feature_date, purchase_count_30d, avg_purchase_amount, created_at, updated_at)
    VALUES (source.user_id, source.feature_date, source.purchase_count_30d, source.avg_purchase_amount, CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP());

-- 6. Time travel: Query features as of yesterday
SELECT * FROM ml_platform.features.user_features
AT(TIMESTAMP => DATEADD(hour, -24, CURRENT_TIMESTAMP()));

-- 7. Zero-copy clone for experimentation (instant, no storage cost)
CREATE TABLE ml_platform.features.user_features_experiment
CLONE ml_platform.features.user_features;

-- Experiment with new feature without affecting production
ALTER TABLE ml_platform.features.user_features_experiment
ADD COLUMN new_experimental_feature FLOAT;

-- 8. Snowpark Python UDF for ML feature computation
CREATE OR REPLACE FUNCTION ml_platform.features.calculate_user_embedding(user_id STRING)
RETURNS ARRAY
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('numpy', 'pandas')
HANDLER = 'calculate_embedding'
AS
$$
import numpy as np

def calculate_embedding(user_id):
    # Placeholder: In practice, call ML model or compute from user behavior
    embedding = np.random.rand(128).tolist()
    return embedding
$$;

-- Use UDF in query
SELECT
    user_id,
    ml_platform.features.calculate_user_embedding(user_id) as user_embedding
FROM ml_platform.features.user_features
LIMIT 10;

-- 9. Export features for external ML training
COPY INTO @ml_platform.exports/user_features/
FROM ml_platform.features.user_features
FILE_FORMAT = (TYPE = PARQUET COMPRESSION = SNAPPY)
PARTITION BY (feature_date);

-- 10. Monitor table changes with stream
SELECT
    user_id,
    feature_date,
    METADATA$ACTION as change_type,  -- INSERT, UPDATE, DELETE
    METADATA$ISUPDATE as is_update,
    METADATA$ROW_ID as row_id
FROM user_features_changes
WHERE METADATA$ACTION = 'INSERT';
```

---

## 🔧 Code Examples

### Code Example 1: Apache Iceberg Table Creation and Schema Evolution

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType

# Initialize Spark with Iceberg
spark = SparkSession.builder \
    .appName("IcebergML") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.spark_catalog.type", "hadoop") \
    .config("spark.sql.catalog.spark_catalog.warehouse", "s3://ml-lakehouse/iceberg/") \
    .getOrCreate()

# 1. Create Iceberg table with partitioning
spark.sql("""
    CREATE TABLE IF NOT EXISTS ml_features.user_features (
        user_id STRING,
        feature_date DATE,
        purchase_count_30d INT,
        avg_purchase_amount FLOAT
    )
    USING iceberg
    PARTITIONED BY (days(feature_date))
""")

# 2. Insert data
from datetime import date

data = [
    ("user_123", date(2025, 10, 19), 15, 120.5),
    ("user_456", date(2025, 10, 19), 8, 95.3)
]

df = spark.createDataFrame(data, ["user_id", "feature_date", "purchase_count_30d", "avg_purchase_amount"])

df.writeTo("ml_features.user_features").append()

# 3. Schema Evolution: Add new column
spark.sql("""
    ALTER TABLE ml_features.user_features
    ADD COLUMN category_preferences ARRAY<STRING>
""")

# 4. Partition Evolution: Change partitioning strategy (Iceberg unique feature!)
spark.sql("""
    ALTER TABLE ml_features.user_features
    SET PARTITION spec (bucket(10, user_id), days(feature_date))
""")
# Old data remains in old partitioning, new data uses new partitioning

# 5. Hidden Partitioning: Query without specifying partitions
result = spark.sql("""
    SELECT * FROM ml_features.user_features
    WHERE feature_date = '2025-10-19'
""")
# Iceberg automatically prunes partitions (no need to specify partition columns)

# 6. Time Travel
# Query data as of specific snapshot
spark.sql("""
    SELECT * FROM ml_features.user_features
    VERSION AS OF 3
""")

# Query data as of timestamp
spark.sql("""
    SELECT * FROM ml_features.user_features
    TIMESTAMP AS OF '2025-10-18 10:00:00'
""")

# 7. Snapshot Management
# List all snapshots
snapshots = spark.sql("SELECT * FROM ml_features.user_features.snapshots")
snapshots.show()

# Rollback to previous snapshot
spark.sql("""
    CALL spark_catalog.system.rollback_to_snapshot('ml_features.user_features', 3)
""")

# 8. Compaction (optimize small files)
spark.sql("""
    CALL spark_catalog.system.rewrite_data_files('ml_features.user_features')
""")

# 9. Expire old snapshots (cleanup)
spark.sql("""
    CALL spark_catalog.system.expire_snapshots('ml_features.user_features', TIMESTAMP '2025-10-01 00:00:00')
""")

# 10. Metadata tables (inspect table internals)
# View files
spark.sql("SELECT * FROM ml_features.user_features.files").show()

# View history
spark.sql("SELECT * FROM ml_features.user_features.history").show()

# View manifests
spark.sql("SELECT * FROM ml_features.user_features.manifests").show()
```

---

### Code Example 2: Apache Hudi for Streaming Upserts

```python
from pyspark.sql import SparkSession

# Initialize Spark with Hudi
spark = SparkSession.builder \
    .appName("HudiML") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension") \
    .getOrCreate()

# Hudi configuration
hudi_options = {
    'hoodie.table.name': 'user_features',
    'hoodie.datasource.write.recordkey.field': 'user_id',
    'hoodie.datasource.write.partitionpath.field': 'feature_date',
    'hoodie.datasource.write.table.name': 'user_features',
    'hoodie.datasource.write.operation': 'upsert',
    'hoodie.datasource.write.precombine.field': 'updated_at',
    'hoodie.upsert.shuffle.parallelism': 2,
    'hoodie.insert.shuffle.parallelism': 2
}

# 1. Initial write (insert)
from pyspark.sql import functions as F

initial_data = spark.createDataFrame([
    ("user_123", "2025-10-19", 10, 100.0, "2025-10-19 10:00:00"),
    ("user_456", "2025-10-19", 5, 75.0, "2025-10-19 10:00:00")
], ["user_id", "feature_date", "purchase_count_30d", "avg_purchase_amount", "updated_at"])

initial_data.write.format("hudi") \
    .options(**hudi_options) \
    .mode("overwrite") \
    .save("s3://ml-lakehouse/hudi/user_features/")

# 2. Streaming upserts (update existing records)
updated_data = spark.createDataFrame([
    ("user_123", "2025-10-19", 12, 110.0, "2025-10-19 11:00:00"),  # Update user_123
    ("user_789", "2025-10-19", 8, 90.0, "2025-10-19 11:00:00")      # Insert user_789
], ["user_id", "feature_date", "purchase_count_30d", "avg_purchase_amount", "updated_at"])

updated_data.write.format("hudi") \
    .options(**hudi_options) \
    .mode("append") \
    .save("s3://ml-lakehouse/hudi/user_features/")

# 3. Read Hudi table
user_features = spark.read.format("hudi") \
    .load("s3://ml-lakehouse/hudi/user_features/")

user_features.show()

# 4. Time Travel Query (read as of specific commit time)
time_travel_df = spark.read.format("hudi") \
    .option("as.of.instant", "2025-10-19 10:00:00") \
    .load("s3://ml-lakehouse/hudi/user_features/")

# 5. Incremental Query (read only changes since last commit)
incremental_df = spark.read.format("hudi") \
    .option("hoodie.datasource.query.type", "incremental") \
    .option("hoodie.datasource.read.begin.instanttime", "2025-10-19 10:00:00") \
    .load("s3://ml-lakehouse/hudi/user_features/")

incremental_df.show()

# 6. Delete records
delete_keys = spark.createDataFrame([
    ("user_456", "2025-10-19")
], ["user_id", "feature_date"])

hudi_delete_options = hudi_options.copy()
hudi_delete_options['hoodie.datasource.write.operation'] = 'delete'

delete_keys.write.format("hudi") \
    .options(**hudi_delete_options) \
    .mode("append") \
    .save("s3://ml-lakehouse/hudi/user_features/")

# 7. Compaction (optimize small files for MOR tables)
from pyspark.sql import Row

spark._jvm.org.apache.hudi.utilities.HoodieCompactor.main(
    [
        "--base-path", "s3://ml-lakehouse/hudi/user_features/",
        "--table-name", "user_features",
        "--schema-file", "/path/to/schema.avsc",
        "--instant-time", ""  # Empty to compact all pending compactions
    ]
)

# 8. Streaming read (continuously read new data)
streaming_df = spark.readStream.format("hudi") \
    .load("s3://ml-lakehouse/hudi/user_features/")

streaming_df.writeStream \
    .format("console") \
    .start() \
    .awaitTermination()
```

---

### Code Example 3: Cost Comparison Calculator

```python
class StorageCostCalculator:
    """
    Calculate total cost of ownership for different storage architectures
    """

    def __init__(self, data_size_tb: float, queries_per_month: int, query_scan_tb: float):
        self.data_size_tb = data_size_tb
        self.queries_per_month = queries_per_month
        self.query_scan_tb = query_scan_tb

    def data_lake_cost(self) -> dict:
        """
        Data Lake (S3 + Athena) cost calculation
        """
        # Storage cost
        storage_cost_monthly = self.data_size_tb * 1000 * 0.023  # $0.023/GB

        # Query cost (Athena: $5/TB scanned)
        query_cost_monthly = self.queries_per_month * self.query_scan_tb * 5

        # Glue catalog cost (assume 1M API calls)
        catalog_cost_monthly = 1 * 1  # $1 per million requests

        total_monthly = storage_cost_monthly + query_cost_monthly + catalog_cost_monthly
        total_annual = total_monthly * 12

        return {
            "architecture": "Data Lake (S3 + Athena)",
            "storage_cost_monthly": round(storage_cost_monthly, 2),
            "query_cost_monthly": round(query_cost_monthly, 2),
            "catalog_cost_monthly": catalog_cost_monthly,
            "total_monthly": round(total_monthly, 2),
            "total_annual": round(total_annual, 2)
        }

    def data_warehouse_cost(self, warehouse: str = "snowflake") -> dict:
        """
        Data Warehouse cost calculation
        """
        if warehouse == "snowflake":
            # Storage: $40/TB/month
            storage_cost_monthly = self.data_size_tb * 40

            # Compute: Assume 5 seconds per query, $2/credit, 1 credit = 1 hour
            compute_hours = (self.queries_per_month * 5) / 3600
            compute_cost_monthly = compute_hours * 2

        elif warehouse == "bigquery":
            # Storage: $20/TB/month (active), $10/TB/month (long-term)
            storage_cost_monthly = self.data_size_tb * 20

            # Query: $6.25/TB scanned (on-demand)
            compute_cost_monthly = self.queries_per_month * self.query_scan_tb * 6.25

        elif warehouse == "redshift":
            # Assume ra3.xlplus node: $1.086/hour, 32 TB storage
            nodes_needed = max(1, self.data_size_tb / 32)
            storage_cost_monthly = nodes_needed * 1.086 * 24 * 30
            compute_cost_monthly = 0  # Included in node cost

        else:
            raise ValueError(f"Unknown warehouse: {warehouse}")

        total_monthly = storage_cost_monthly + compute_cost_monthly
        total_annual = total_monthly * 12

        return {
            "architecture": f"Data Warehouse ({warehouse})",
            "storage_cost_monthly": round(storage_cost_monthly, 2),
            "query_cost_monthly": round(compute_cost_monthly, 2),
            "total_monthly": round(total_monthly, 2),
            "total_annual": round(total_annual, 2)
        }

    def lakehouse_cost(self, technology: str = "delta_lake") -> dict:
        """
        Lakehouse cost calculation
        """
        # Storage: Same as data lake (S3 Standard)
        storage_cost_monthly = self.data_size_tb * 1000 * 0.023

        if technology == "delta_lake":
            # Compute: Databricks Photon runtime
            # Assume 1 DBU = $0.55 (all-purpose), 2 DBUs per query, 0.1 hours per query
            query_hours = self.queries_per_month * 0.1
            compute_cost_monthly = query_hours * 2 * 0.55

        elif technology == "iceberg":
            # Use Spark on EMR: $0.27/hour for m5.xlarge
            # Assume 0.05 hours per query, 2 instances
            query_hours = self.queries_per_month * 0.05
            compute_cost_monthly = query_hours * 2 * 0.27

        elif technology == "hudi":
            # Similar to Iceberg (Spark-based)
            query_hours = self.queries_per_month * 0.05
            compute_cost_monthly = query_hours * 2 * 0.27

        else:
            raise ValueError(f"Unknown technology: {technology}")

        total_monthly = storage_cost_monthly + compute_cost_monthly
        total_annual = total_monthly * 12

        return {
            "architecture": f"Lakehouse ({technology})",
            "storage_cost_monthly": round(storage_cost_monthly, 2),
            "query_cost_monthly": round(compute_cost_monthly, 2),
            "total_monthly": round(total_monthly, 2),
            "total_annual": round(total_annual, 2)
        }

    def compare_all(self):
        """
        Compare costs across all architectures
        """
        results = [
            self.data_lake_cost(),
            self.data_warehouse_cost("snowflake"),
            self.data_warehouse_cost("bigquery"),
            self.lakehouse_cost("delta_lake"),
            self.lakehouse_cost("iceberg")
        ]

        # Sort by total annual cost
        results.sort(key=lambda x: x["total_annual"])

        return results

# Usage example
calculator = StorageCostCalculator(
    data_size_tb=10,           # 10 TB of data
    queries_per_month=1000,    # 1000 queries per month
    query_scan_tb=0.5          # Average query scans 500 GB
)

comparison = calculator.compare_all()

for result in comparison:
    print(f"\n{result['architecture']}")
    print(f"  Storage Cost: ${result['storage_cost_monthly']}/month")
    print(f"  Query Cost: ${result['query_cost_monthly']}/month")
    print(f"  Total Monthly: ${result['total_monthly']}")
    print(f"  Total Annual: ${result['total_annual']}")

# Expected output:
# Lakehouse (iceberg)
#   Storage Cost: $230.0/month
#   Query Cost: $27.0/month
#   Total Monthly: $257.0
#   Total Annual: $3084.0
#
# Lakehouse (delta_lake)
#   Storage Cost: $230.0/month
#   Query Cost: $110.0/month
#   Total Monthly: $340.0
#   Total Annual: $4080.0
#
# Data Lake (S3 + Athena)
#   Storage Cost: $230.0/month
#   Query Cost: $2500.0/month
#   Total Monthly: $2731.0
#   Total Annual: $32772.0
```

---

## ✅ Best Practices

### 1. Use Lakehouse for Modern ML Platforms

**Principle**: Lakehouse (Delta Lake, Iceberg, Hudi) combines best of lakes and warehouses.

**Recommendation**:
- Start with Delta Lake (easiest, Databricks ecosystem)
- Use Iceberg for multi-engine access (Spark, Trino, Flink)
- Use Hudi for high-frequency upserts

---

### 2. Implement Medallion Architecture

**Principle**: Separate raw, curated, and aggregated data into bronze/silver/gold layers.

**Benefits**:
- Incremental processing (only transform what changed)
- Data quality zones
- Auditability (trace features to raw data)

---

### 3. Enable ACID Transactions

**Principle**: Use technologies with ACID guarantees to prevent data corruption.

**Avoid**:
- Direct Parquet writes with concurrent jobs
- Manual partition management

**Use**:
- Delta Lake/Iceberg/Hudi for ACID
- Warehouse MERGE statements for upserts

---

### 4. Partition Data Strategically

**Principle**: Partition by columns frequently used in filters.

**Good Partitioning**:
```python
# Partition by date for time-series queries
.partitionBy("year", "month", "day")

# Partition by entity for lookups
.partitionBy("user_id")
```

**Bad Partitioning**:
```python
# Too granular (millions of small files)
.partitionBy("user_id", "event_type", "hour")

# Never filtered (adds overhead without benefit)
.partitionBy("random_id")
```

---

### 5. Use Columnar Formats

**Principle**: Parquet/ORC for analytics, Avro for streaming.

**Compression**:
- Parquet with Snappy (good balance)
- Parquet with Gzip (max compression, slower)
- ORC with Zlib (good for Hive/Presto)

---

### 6. Implement Data Versioning

**Principle**: Track data versions for reproducible ML.

**Approaches**:
- Delta Lake/Iceberg time travel
- DVC (Data Version Control)
- Timestamped paths: `s3://data/v1/`, `s3://data/v2/`

---

### 7. Enable Schema Enforcement

**Principle**: Validate schema at write time to prevent bad data.

**Delta Lake**:
```python
.option("mergeSchema", "false")  # Reject schema changes
```

**Snowflake**:
```sql
CREATE TABLE ... ENABLE_SCHEMA_EVOLUTION = FALSE
```

---

### 8. Optimize for Query Patterns

**Principle**: Design storage based on how data is accessed.

**Point Queries** (lookup by ID):
- Use Z-ordering (Delta Lake)
- Partition by ID (if cardinality low)

**Range Queries** (date ranges):
- Partition by date
- Sort by timestamp

**Full Table Scans** (ML training):
- Larger files (128-512 MB)
- Fewer partitions

---

### 9. Monitor Storage Costs

**Principle**: Track costs and optimize regularly.

**Cost Optimization**:
- Use lifecycle policies (S3 → Glacier after 90 days)
- Vacuum old versions (Delta Lake)
- Compress with appropriate codec

---

### 10. Test at Scale

**Principle**: Validate architecture with production-scale data.

**Testing**:
- Load test with 10x expected data volume
- Measure query latency at scale
- Test concurrent writes (ACID validation)

---

## ⚠️ Common Pitfalls

### 1. Data Swamp (Ungoverned Data Lake)

**Pitfall**: Data lake becomes unusable due to lack of governance.

**Symptoms**:
- Unknown data quality
- Duplicate datasets
- No documentation
- Orphaned files

**Solution**:
- Implement medallion architecture
- Use catalog (Glue, Unity Catalog)
- Enforce naming conventions
- Regular audits and cleanup

---

### 2. Small Files Problem

**Pitfall**: Millions of small files degrade query performance.

**Cause**:
- Streaming ingestion writing per-record files
- Over-partitioning

**Solution**:
- Compact small files (Delta OPTIMIZE, Iceberg rewrite_data_files)
- Use larger micro-batches for streaming
- Reduce partition granularity

---

### 3. Schema Evolution Without Versioning

**Pitfall**: Breaking schema changes break downstream consumers.

**Solution**:
- Version datasets explicitly (v1/, v2/)
- Use additive changes only (add columns, don't remove/rename)
- Test schema changes in staging environment

---

### 4. Ignoring Partition Pruning

**Pitfall**: Queries scan all partitions instead of filtering.

**Example**:
```python
# Bad: Doesn't filter partitions
df = spark.read.parquet("s3://data/events/")
df.filter(F.col("date") == "2025-10-19")  # Scans all partitions!

# Good: Partition pruning
df = spark.read.parquet("s3://data/events/date=2025-10-19/")
```

**Solution**: Ensure filters on partition columns

---

### 5. Not Using Columnar Formats

**Pitfall**: Storing data as CSV/JSON wastes storage and query time.

**Impact**:
- 10x larger files (no compression)
- 100x slower queries (row-by-row parsing)

**Solution**: Always use Parquet or ORC for analytics

---

### 6. Underestimating Data Warehouse Costs

**Pitfall**: Snowflake/BigQuery costs spiral due to inefficient queries.

**Solution**:
- Use materialized views for repeated queries
- Implement result caching
- Monitor query costs and optimize expensive queries
- Consider lakehouse for archival/exploration

---

### 7. No Time Travel / Backup Strategy

**Pitfall**: Cannot recover from bad writes or accidental deletes.

**Solution**:
- Enable Delta Lake time travel
- Configure S3 versioning
- Regular backups to separate bucket
- Test restore procedures

---

### 8. Mixing Lakehouse Technologies

**Pitfall**: Using Delta Lake and Iceberg on same data causes corruption.

**Solution**: Pick one lakehouse format per table, stick with it

---

### 9. Not Monitoring File Sizes

**Pitfall**: Files too small (slow queries) or too large (OOM errors).

**Optimal**:
- 128-512 MB per file for Parquet
- Monitor with Spark UI or table metadata

---

### 10. Ignoring Data Locality

**Pitfall**: Compute in us-east-1, data in eu-west-1 → cross-region transfer costs and latency.

**Solution**: Co-locate compute and storage in same region

---

## 🏋️ Hands-On Exercises

### Exercise 1: Build a Three-Layer Data Lake (Intermediate, 8-10 hours)

**Objective**: Implement medallion architecture (bronze, silver, gold) for ML data.

**Tasks**:

1. **Bronze Layer**: Ingest raw clickstream data from Kafka to S3
   - Set up Kafka producer (generate synthetic clickstream events)
   - Stream to S3 with Spark Structured Streaming
   - Partition by date

2. **Silver Layer**: Clean and validate data
   - Parse JSON events
   - Remove duplicates
   - Validate schema (user_id not null, timestamp valid)
   - Write as Delta Lake table

3. **Gold Layer**: Aggregate ML features
   - Compute user-level features (event_count, unique_event_types)
   - Daily aggregations
   - Write as Delta Lake table

4. **Query Gold for ML**:
   - Load features for training
   - Join with labels
   - Export as Parquet for training

**Validation**:
- Bronze: 1M+ events ingested
- Silver: Deduplication removes ~5% duplicates
- Gold: Features computed for all users
- Query latency <5 seconds

**Deliverables**:
- Spark code for all three layers
- Data quality report (# duplicates, null values)
- Query performance benchmarks

---

### Exercise 2: Migrate Parquet Lake to Delta Lake (Intermediate, 6-8 hours)

**Objective**: Convert existing Parquet data lake to Delta Lake lakehouse.

**Tasks**:

1. **Setup**: Create Parquet dataset (simulate existing lake)
   - 10 GB of user transaction data
   - Partitioned by date

2. **Convert to Delta**:
   - Read Parquet, write as Delta Lake
   - Enable auto-optimize and auto-compact

3. **Implement Upserts**:
   - Merge new transactions into existing data
   - Handle late-arriving data

4. **Test Time Travel**:
   - Query data as of yesterday
   - Restore to previous version after bad write

5. **Optimize**:
   - OPTIMIZE table with Z-ordering on user_id
   - VACUUM old files

**Validation**:
- Upserts correctly update existing records
- Time travel returns correct historical data
- Optimization reduces query time by 50%

**Deliverables**:
- Migration script (Parquet → Delta)
- Upsert implementation
- Performance comparison (before/after optimization)

---

### Exercise 3: Compare Lakehouse Technologies (Advanced, 12-15 hours)

**Objective**: Implement same use case with Delta Lake, Iceberg, and Hudi, compare.

**Use Case**: User features table with frequent upserts.

**Tasks**:

1. **Implement with Delta Lake**:
   - Create table, populate with 1M users
   - Implement upsert workflow
   - Measure write/read performance

2. **Implement with Iceberg**:
   - Same dataset, same operations
   - Test schema evolution (add column)
   - Test partition evolution

3. **Implement with Hudi**:
   - Same dataset
   - Test COW vs MOR table types
   - Measure compaction time

4. **Benchmark**:
   - Write throughput (records/sec)
   - Query latency
   - Storage size
   - Operational complexity

**Validation**:
- All three technologies produce identical results
- Benchmarks show trade-offs clearly

**Deliverables**:
- Implementation code for all three
- Benchmark report (table with metrics)
- Recommendation based on use case

---

### Exercise 4: Cost Optimization Challenge (Advanced, 10-12 hours)

**Objective**: Reduce storage costs by 50% without sacrificing query performance.

**Starting Point**: 100 TB data lake, $3,000/month cost.

**Tasks**:

1. **Analyze Current Costs**:
   - Storage by layer (bronze, silver, gold)
   - Query costs (Athena scans)
   - Identify hot vs cold data

2. **Implement Lifecycle Policies**:
   - Move bronze data >90 days to S3 Glacier
   - Delete silver data >1 year
   - Keep gold data indefinitely

3. **Optimize File Formats**:
   - Convert JSON to Parquet (compression)
   - Tune compression codec

4. **Reduce Query Scans**:
   - Improve partitioning
   - Create summary tables for common queries
   - Cache frequent queries

5. **Calculate Savings**:
   - New monthly cost
   - Percentage reduction

**Validation**:
- Cost reduction ≥50%
- Query latency <10% increase

**Deliverables**:
- Cost analysis spreadsheet (before/after)
- Lifecycle policy configuration
- Query optimization report

---

## 🔗 Related Concepts

### Within This Course:
- [[02. Object Storage (S3, GCS, Azure Blob)]] - Foundational storage layer
- [[03. Distributed File Systems]] - HDFS, Alluxio alternatives
- [[04. Storage Optimization for ML Workloads]] - Performance tuning
- [[05. Data Partitioning and Organization]] - Structuring for efficient access

### External Connections:
- **Data Processing**: [[07. Batch Processing for ML Data]], [[08. Stream Processing & Real-Time ML]]
- **Feature Engineering**: [[09. Feature Engineering Pipelines]], [[10. Feature Stores]]
- **Data Quality**: [[14. Data Quality & Validation for ML]]
- **MLOps**: [[13. MLOps & Data Pipeline Integration]]

---

## 📚 Further Reading

### Essential Books:

1. **"Designing Data-Intensive Applications" by Martin Kleppmann (O'Reilly, 2017)**
   - Chapter 3: Storage and Retrieval
   - Foundational storage concepts

2. **"The Enterprise Big Data Lake" by Alex Gorelik (O'Reilly, 2019)**
   - Data lake architecture patterns
   - Governance and metadata management

3. **"Delta Lake: The Definitive Guide" by Denny Lee et al. (O'Reilly, 2022)**
   - Comprehensive Delta Lake guide
   - Lakehouse patterns

### Key Papers:

1. **"Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores" (Databricks, 2020)**
   - https://databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf
   - Academic foundation for Delta Lake

2. **"Apache Iceberg: The Definitive Guide" (Netflix, 2021)**
   - https://iceberg.apache.org/
   - Hidden partitioning, schema evolution

3. **"Apache Hudi: Upserts on Cloud Storage" (Uber, 2019)**
   - https://hudi.apache.org/
   - Incremental processing patterns

### Blogs and Articles:

1. **Databricks Blog - "Lakehouse: A New Generation of Open Platforms"**
   - https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html
   - Lakehouse architecture overview

2. **Netflix Tech Blog - "Apache Iceberg - Tables Designed for Object Stores"**
   - https://netflixtechblog.com/
   - Production Iceberg at Netflix scale

3. **Uber Engineering - "Hoodie: Incremental Processing on Hadoop"**
   - https://eng.uber.com/hoodie/
   - Hudi architecture and use cases

4. **AWS Big Data Blog - "Build a Modern Data Architecture on AWS"**
   - https://aws.amazon.com/blogs/big-data/
   - Lake formation, Glue, Athena integration

### Documentation:

1. **Delta Lake Documentation**
   - https://docs.delta.io/
   - Comprehensive guides and APIs

2. **Apache Iceberg Documentation**
   - https://iceberg.apache.org/docs/latest/
   - Table format specification

3. **Apache Hudi Documentation**
   - https://hudi.apache.org/docs/overview
   - Table types and configurations

4. **Snowflake Data Warehouse Guide**
   - https://docs.snowflake.com/
   - Time travel, zero-copy cloning

### Video Courses:

1. **"Data Engineering on AWS" (Coursera)**
   - Data lakes, Glue, Athena, Redshift

2. **"Delta Lake Fundamentals" (Databricks Academy)**
   - https://academy.databricks.com/
   - Hands-on Delta Lake training

3. **"Apache Iceberg: Modern Table Format" (Confluent)**
   - Streaming to Iceberg with Flink

---

## 📝 Key Takeaways

1. **Lakehouses are the modern best practice for ML**: Combine low-cost object storage with ACID transactions, schema enforcement, and time travel.

2. **Data lakes excel at flexibility and cost**: Ideal for exploratory ML and storing diverse data types (images, text, raw logs).

3. **Data warehouses optimize for SQL analytics**: Best for structured ML features with strong consistency requirements, but 10-100x more expensive.

4. **Medallion architecture (bronze-silver-gold) separates concerns**: Raw data → cleaned data → aggregated features, enabling incremental processing.

5. **ACID transactions prevent data corruption**: Essential for concurrent writes and consistent feature serving; use Delta Lake, Iceberg, or Hudi.

6. **Schema evolution requires planning**: Use additive changes (add columns) or version datasets explicitly to avoid breaking downstream consumers.

7. **Partitioning dramatically improves query performance**: Partition by frequently-filtered columns (date, user_id) but avoid over-partitioning.

8. **Columnar formats (Parquet, ORC) reduce costs and improve performance**: 10x smaller storage, 100x faster queries vs JSON/CSV.

9. **Time travel enables reproducible ML**: Query historical data for debugging, retraining, and auditing with Delta Lake, Iceberg, or Snowflake.

10. **Cost optimization is critical at scale**: Use lifecycle policies (hot → cold → archive), compression, and lakehouse architecture to reduce costs by 50-90%.

---

## ✏️ Notes Section

**Personal Insights**:

**Questions to Explore**:

**Related Projects**:

---

*Last updated: 2025-10-19*
*Part of: [[DEforAI - Data Engineering for AI/ML]]*
*Chapter: [[04. Data Storage for ML]]*
