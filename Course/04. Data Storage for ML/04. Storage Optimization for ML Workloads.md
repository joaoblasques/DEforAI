# Storage Optimization for ML Workloads

**Course:** Data Engineering for AI/ML
**Chapter:** 04 - Data Storage for ML
**Subchapter:** 04 - Storage Optimization for ML Workloads
**Created:** 2025-10-19
**Updated:** 2025-10-19

---

## 📋 Overview

Storage optimization is critical for machine learning workloads due to the massive volumes of data involved in training, feature engineering, and inference. Unlike traditional OLTP or OLAP workloads, ML pipelines have unique access patterns: sequential reads of large datasets, columnar analytics for feature extraction, and frequent random access for model serving.

Poor storage optimization can result in:
- **10-100x slower training times** due to I/O bottlenecks
- **5-20x higher cloud storage costs** from inefficient formats
- **Memory pressure** from uncompressed data structures
- **Network saturation** from unnecessary data transfer
- **Failed experiments** due to out-of-memory errors

This subchapter covers storage-level optimizations specifically tailored for ML workloads, including file format selection, compression strategies, partitioning schemes, and caching mechanisms. We'll focus on practical techniques that deliver measurable improvements in both performance and cost.

**Key Topics:**
- Columnar storage formats (Parquet, ORC) and their ML-specific optimizations
- Compression algorithms and their trade-offs for different data types
- File sizing strategies to avoid small files problem
- Storage format selection based on ML workload patterns
- Row groups, column chunks, and statistics for query pushdown
- Delta encoding, dictionary encoding, and run-length encoding
- Caching strategies for feature stores and model artifacts

**Prerequisites:**
- Understanding of data lakes, warehouses, and lakehouses (Subchapter 04.01)
- Familiarity with object storage systems (Subchapter 04.02)
- Basic knowledge of distributed file systems (Subchapter 04.03)
- Experience with Spark, Pandas, or similar data processing frameworks

---

## 🎯 Learning Objectives

By the end of this subchapter, you will be able to:

1. **Select optimal storage formats** for different ML workloads (training, feature engineering, batch inference, real-time serving)
2. **Configure Parquet and ORC** files with appropriate row group sizes, compression, and encoding schemes
3. **Avoid the small files problem** by implementing file consolidation strategies (bin-packing, compaction, coalescing)
4. **Optimize compression** by choosing the right algorithm (Snappy, Zstandard, LZ4) based on CPU/I/O trade-offs
5. **Leverage storage statistics** (min/max, bloom filters, dictionary encoding) for query pushdown and data skipping
6. **Design partition schemes** that align with ML access patterns (time-based, feature-based, model-version-based)
7. **Implement multi-tier caching** for frequently accessed features and embeddings
8. **Measure storage efficiency** using metrics like compression ratio, scan throughput, and query latency
9. **Profile I/O bottlenecks** in ML pipelines and apply targeted optimizations
10. **Balance cost vs performance** by selecting appropriate storage classes and retention policies

---

## 📚 Core Concepts

### 1. Columnar Storage Fundamentals

**Row-Oriented vs Column-Oriented Storage:**

Traditional row-oriented storage (CSV, JSON, Avro) stores entire records contiguously:
```
Record 1: [user_id=123, age=25, city="NYC", spend=$500, features=[0.1, 0.2, ...]]
Record 2: [user_id=456, age=30, city="SF", spend=$800, features=[0.3, 0.4, ...]]
```

Columnar storage (Parquet, ORC) stores each column separately:
```
user_id column: [123, 456, ...]
age column: [25, 30, ...]
city column: ["NYC", "SF", ...]
spend column: [$500, $800, ...]
features column: [[0.1, 0.2, ...], [0.3, 0.4, ...], ...]
```

**Why Columnar for ML:**
1. **Column pruning**: Read only the features needed for a model (e.g., 20 out of 500 columns)
2. **Better compression**: Similar values cluster together (e.g., all ages, all cities)
3. **Vectorized processing**: Modern CPUs can process columns in SIMD batches
4. **Predicate pushdown**: Skip row groups where column statistics indicate no matches
5. **Schema evolution**: Add new features without rewriting existing columns

**Performance Impact:**
- **Feature selection**: Columnar reduces I/O by 10-50x when selecting 10% of columns
- **Aggregations**: 5-20x faster for operations like `AVG(age)` or `COUNT(DISTINCT city)`
- **Compression**: 3-10x better compression ratios than row formats

### 2. Parquet File Format

**Internal Structure:**

```
┌─────────────────────────────────────┐
│         Parquet File                │
├─────────────────────────────────────┤
│ Magic Number: PAR1                  │
├─────────────────────────────────────┤
│ Row Group 1 (128 MB default)        │
│  ├─ Column Chunk: user_id           │
│  │   ├─ Data Pages (1 MB each)      │
│  │   ├─ Dictionary Page             │
│  │   └─ Statistics (min, max, null) │
│  ├─ Column Chunk: features          │
│  │   └─ Data Pages                  │
│  └─ ...                              │
├─────────────────────────────────────┤
│ Row Group 2                          │
│  └─ ...                              │
├─────────────────────────────────────┤
│ Footer (schema, metadata, offsets)  │
├─────────────────────────────────────┤
│ Magic Number: PAR1                  │
└─────────────────────────────────────┘
```

**Key Parameters:**

1. **Row Group Size** (`parquet.block.size`):
   - Default: 128 MB
   - ML recommendation: 256 MB - 1 GB for large datasets
   - Trade-off: Larger = better compression, but coarser predicate pushdown

2. **Page Size** (`parquet.page.size`):
   - Default: 1 MB
   - ML recommendation: 1-8 MB for sequential scans
   - Trade-off: Larger = fewer seeks, but more data read per column access

3. **Dictionary Encoding**:
   - Enabled by default for low-cardinality columns
   - Automatically disabled if dictionary exceeds 1 MB
   - ML use: Excellent for categorical features (city, product_category)

4. **Column Statistics**:
   - Min/max values, null count, distinct count
   - Used for predicate pushdown (`WHERE age > 25`)
   - Critical for data skipping in large-scale training

**Parquet Encoding Schemes:**

| Encoding | Best For | Example |
|----------|----------|---------|
| PLAIN | High-cardinality numerics | Embeddings, continuous features |
| DICTIONARY | Categorical features | City names, product IDs (< 40K unique) |
| RLE (Run-Length) | Repeated values | Boolean flags, sparse features |
| DELTA_BINARY_PACKED | Sorted integers | Timestamps, user IDs |
| DELTA_LENGTH_BYTE_ARRAY | Variable-length strings | Text descriptions |
| DELTA_BYTE_ARRAY | Sorted strings | Alphabetically sorted categories |

### 3. ORC File Format

**ORC (Optimized Row Columnar):**

ORC is similar to Parquet but optimized for Hive/Presto workloads:

```
┌─────────────────────────────────────┐
│ ORC File                             │
├─────────────────────────────────────┤
│ Stripe 1 (256 MB default)            │
│  ├─ Index Data (row group stats)    │
│  ├─ Row Data                         │
│  │   ├─ Stream: user_id (PRESENT)   │
│  │   ├─ Stream: user_id (DATA)      │
│  │   ├─ Stream: features (PRESENT)  │
│  │   └─ Stream: features (DATA)     │
│  └─ Stripe Footer                    │
├─────────────────────────────────────┤
│ File Footer (schema, statistics)    │
│  ├─ Column Statistics                │
│  ├─ Stripe Information               │
│  └─ Bloom Filters (optional)         │
└─────────────────────────────────────┘
```

**ORC vs Parquet for ML:**

| Feature | Parquet | ORC |
|---------|---------|-----|
| Ecosystem | Spark, Pandas, TensorFlow IO | Hive, Presto, Trino |
| Compression | Snappy, Gzip, Zstd, LZ4 | Zlib, Snappy, Zstd, LZ4 |
| Bloom Filters | No (use Delta Lake) | Yes (built-in) |
| ACID Support | No (use Delta/Iceberg) | Yes (Hive 3.0+) |
| Nested Types | Excellent (Dremel encoding) | Good |
| Default Row Group | 128 MB | 256 MB (stripe) |
| ML Integration | Better (TensorFlow, PyTorch) | Limited |

**Recommendation:** Use **Parquet** for ML workloads due to better ecosystem support (TensorFlow I/O, PyTorch, Petastorm) and nested type handling for embeddings.

### 4. Compression Algorithms

**Compression Trade-offs:**

```
┌──────────────────────────────────────────────────┐
│ Compression Spectrum for ML Workloads           │
├──────────────────────────────────────────────────┤
│                                                  │
│  Fast Decompression ←───────────→ High Ratio   │
│                                                  │
│  LZ4 ─── Snappy ─── Zstd ─── Gzip ─── Brotli   │
│                                                  │
│  Read-Heavy        Balanced       Write-Heavy   │
│  (Training)        (Feature Eng)  (Archival)    │
└──────────────────────────────────────────────────┘
```

**Detailed Comparison:**

| Algorithm | Ratio | Compress Speed | Decompress Speed | ML Use Case |
|-----------|-------|----------------|------------------|-------------|
| **LZ4** | 2-3x | 500 MB/s | 2000 MB/s | Real-time inference, frequent access |
| **Snappy** | 2-4x | 250 MB/s | 500 MB/s | Default for training (balanced) |
| **Zstandard** | 3-5x | 200 MB/s | 600 MB/s | Feature stores, cold storage |
| **Gzip** | 4-6x | 50 MB/s | 250 MB/s | Archival, regulatory compliance |
| **Brotli** | 5-8x | 10 MB/s | 300 MB/s | Long-term archival only |

**Rule of Thumb:**
- **Training (read-heavy)**: Use **Snappy** or **LZ4** to minimize CPU overhead
- **Feature engineering (balanced)**: Use **Zstandard (level 3)** for 30-40% better compression than Snappy
- **Archival (write-once)**: Use **Gzip** or **Zstd (level 9)** to maximize storage savings
- **Real-time serving**: Use **LZ4** or no compression if latency < 10ms is critical

**Compression Efficiency by Data Type:**

| Data Type | Best Compression | Typical Ratio | Notes |
|-----------|------------------|---------------|-------|
| Timestamps | Delta + Zstd | 10-50x | Sorted time series compress extremely well |
| Categorical (low cardinality) | Dictionary + Snappy | 5-20x | Dictionary encoding is key |
| Categorical (high cardinality) | Zstd | 2-4x | User IDs, session IDs |
| Numerical (integers) | Delta + Zstd | 3-8x | Sorted or sequential IDs |
| Numerical (floats) | Snappy | 1.5-3x | Embeddings, features (low redundancy) |
| Text (natural language) | Zstd or Brotli | 3-6x | Good for descriptions, reviews |
| Boolean flags | RLE + LZ4 | 10-100x | Sparse binary features |

### 5. File Sizing and Small Files Problem

**The Small Files Problem:**

HDFS, S3, and cloud storage have per-file overhead:
- **Metadata overhead**: Each file requires directory entry, inode, or S3 object metadata
- **List operation cost**: `spark.read.parquet("s3://bucket/path")` must list all files
- **Connection overhead**: Opening 10,000 files requires 10,000 HTTP connections
- **Parallelism limits**: Spark creates 1 task per file (max 10K tasks in practice)

**Impact on ML:**
- Reading **10,000 x 1 MB files** takes 10-50x longer than reading **10 x 1 GB files**
- S3 LIST operations cost $0.005 per 1,000 requests
- Driver OOM errors from maintaining file metadata

**Optimal File Sizes:**

| Storage System | Recommended File Size | Rationale |
|----------------|----------------------|-----------|
| **HDFS** | 128 MB - 1 GB | Matches HDFS block size (128 MB default) |
| **S3 / GCS / Azure Blob** | 128 MB - 1 GB | Balances parallelism and overhead |
| **Local SSD (training)** | 64 MB - 256 MB | Smaller = more parallel I/O threads |
| **NFS / Lustre (HPC)** | 256 MB - 2 GB | Reduces metadata operations |

**File Consolidation Strategies:**

1. **Coalesce After ETL**:
```python
# Bad: Writes 10,000 small files
df.write.parquet("s3://bucket/features/")

# Good: Coalesce to ~1 GB files
df.coalesce(num_partitions=100).write.parquet("s3://bucket/features/")
```

2. **Bin-Packing Compaction**:
```python
# Combine small files into larger ones
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "s3://bucket/features/")
delta_table.optimize().executeCompaction()  # Combines small files
```

3. **Auto-Optimize (Databricks)**:
```sql
ALTER TABLE features
SET TBLPROPERTIES (
  delta.autoOptimize.optimizeWrite = true,  -- Combines small files on write
  delta.autoOptimize.autoCompact = true     -- Background compaction
);
```

### 6. Query Pushdown and Data Skipping

**Predicate Pushdown:**

When executing `df.filter("age > 25").select("user_id", "features")`, modern query engines push predicates down to storage:

1. **Partition pruning**: Skip entire partitions (e.g., `date < 2025-01-01`)
2. **Row group skipping**: Use min/max statistics to skip row groups
3. **Column pruning**: Read only `user_id` and `features` columns
4. **Projection pushdown**: Apply `SELECT` before `WHERE` if beneficial

**Storage Statistics for Skipping:**

```
Row Group 1: age column stats
  - min: 18
  - max: 24
  - null_count: 0
  → SKIP (all values < 25)

Row Group 2: age column stats
  - min: 22
  - max: 45
  - null_count: 12
  → READ (some values may be > 25)
```

**Bloom Filters (ORC / Delta Lake):**

Probabilistic data structure to test set membership:
- **False positives**: Possible (may read unnecessary data)
- **False negatives**: Never (won't skip matching data)
- **Size**: Typically 1-10 KB per row group

Example: Check if `user_id = 12345` exists in row group:
```python
# Delta Lake with bloom filters
spark.conf.set("spark.databricks.io.skipping.bloomFilter.enabled", "true")

df.write.format("delta") \
    .option("delta.bloomFilter.user_id", "10000000")  # 10M expected items \
    .save("s3://bucket/users/")
```

### 7. Caching Strategies for ML

**Multi-Tier Caching:**

```
┌─────────────────────────────────────────────┐
│ L1: In-Memory Cache (Spark, Redis)         │
│ - Size: 10-100 GB                           │
│ - Latency: 1-10 ms                          │
│ - Use: Hot features, embeddings             │
├─────────────────────────────────────────────┤
│ L2: Local SSD Cache (Alluxio, RocksDB)     │
│ - Size: 100 GB - 10 TB                      │
│ - Latency: 10-100 ms                        │
│ - Use: Training data, feature stores        │
├─────────────────────────────────────────────┤
│ L3: Object Storage (S3, GCS)                │
│ - Size: Unlimited                           │
│ - Latency: 100-500 ms                       │
│ - Use: Cold data, archives                  │
└─────────────────────────────────────────────┘
```

**When to Cache:**
- **Training**: Cache preprocessed datasets to avoid repeated ETL
- **Hyperparameter tuning**: Cache validation sets for fast evaluation
- **Real-time inference**: Cache embeddings and frequent feature lookups
- **Feature engineering**: Cache intermediate transformations

**Eviction Policies:**
- **LRU (Least Recently Used)**: Best for general-purpose ML workloads
- **LFU (Least Frequently Used)**: Good for feature stores with Zipfian access
- **TTL (Time to Live)**: Useful for time-decaying features (e.g., daily aggregates)

### 8. Storage Format Selection Matrix

**Decision Tree:**

```
Q1: Is your data structured with a schema?
  ├─ No → Use JSON/JSONL with Zstd compression (exploratory ML)
  └─ Yes → Continue to Q2

Q2: Do you need ACID transactions or schema evolution?
  ├─ Yes → Use Delta Lake (Parquet) or Iceberg
  └─ No → Continue to Q3

Q3: What is your primary access pattern?
  ├─ Sequential scans (training) → Parquet with Snappy, 1 GB files
  ├─ Random access (serving) → Feather/Arrow in-memory format
  ├─ Streaming (real-time) → Avro or Protobuf
  └─ Archival (compliance) → Parquet with Gzip, Delta Lake for versioning

Q4: What is your data size?
  ├─ < 1 GB → CSV or Feather (simplicity)
  ├─ 1 GB - 1 TB → Parquet with appropriate partitioning
  └─ > 1 TB → Parquet/ORC with lakehouse (Delta/Iceberg)
```

**Format Comparison for ML:**

| Format | Pros | Cons | ML Use Case |
|--------|------|------|-------------|
| **Parquet** | Columnar, excellent compression, wide support | No ACID, schema evolution limited | Default for batch training |
| **ORC** | Built-in bloom filters, ACID (Hive) | Less ML ecosystem support | Hive/Presto feature stores |
| **Avro** | Schema evolution, streaming-friendly | Row-oriented (slower analytics) | Kafka → ML pipeline |
| **Feather/Arrow** | Zero-copy reads, 10-100x faster than Parquet | No compression, larger files | In-memory training data |
| **Delta Lake** | ACID, time travel, schema enforcement | Requires Spark/Databricks | Production feature stores |
| **TFRecord** | TensorFlow native, sharded | TensorFlow-only, not human-readable | TensorFlow training |
| **Petastorm** | Parquet for PyTorch/TF, multi-GPU | Additional dependency | Multi-framework training |

---

## 💡 Practical Examples

### Example 1: Parquet Optimization for Training Data

**Scenario:** Convert a 500 GB CSV dataset to optimized Parquet for distributed training.

**Original Problem:**
```python
# Slow: CSV with 100,000 small files (5 MB each)
df = spark.read.csv("s3://ml-data/raw/features/")
# Takes 45 minutes to load
```

**Optimized Solution:**
```python
# Step 1: Read CSV and repartition
df = spark.read.csv("s3://ml-data/raw/features/", header=True, inferSchema=True)

# Step 2: Coalesce to 512 files (~1 GB each)
num_files = 512
df_optimized = df.repartition(num_files)

# Step 3: Write as Parquet with optimal settings
df_optimized.write.mode("overwrite") \
    .option("compression", "snappy") \
    .option("parquet.block.size", 1024 * 1024 * 1024)  # 1 GB row groups \
    .option("parquet.page.size", 8 * 1024 * 1024)      # 8 MB pages \
    .parquet("s3://ml-data/optimized/features/")

# Result: Load time reduced to 2 minutes (22.5x improvement)
```

**Measured Impact:**
- **Storage**: 500 GB → 180 GB (2.8x compression with Snappy)
- **Load time**: 45 min → 2 min (22.5x faster)
- **Training throughput**: 2.5 GB/s (network-saturated on 10 Gbps)

### Example 2: Zstandard Compression for Feature Store

**Scenario:** Feature store with 2 TB of historical features, read occasionally for model retraining.

**Problem:**
```python
# Default Snappy compression
df.write.format("delta") \
    .option("compression", "snappy") \
    .save("s3://feature-store/user_features/")

# Storage cost: $46/month (S3 Standard, 2 TB at $0.023/GB)
```

**Optimized with Zstandard:**
```python
# Zstandard level 3 (balanced speed/ratio)
df.write.format("delta") \
    .option("compression", "zstd") \
    .option("zstd.level", 3) \
    .save("s3://feature-store/user_features/")

# Storage: 2 TB → 800 GB (2.5x better than Snappy)
# Cost: $18.40/month (60% savings)

# Decompression overhead: +15% CPU, acceptable for monthly retraining
```

**Cost-Benefit Analysis:**
- **Storage savings**: $27.60/month ($331/year)
- **Decompression cost**: +$2/month in compute (15% more CPU)
- **Net savings**: $25.60/month ($307/year) per 2 TB dataset

### Example 3: Small Files Compaction

**Scenario:** Streaming feature pipeline writes 10,000 small files per day.

**Problem:**
```python
# Streaming writes create many small files
spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .load() \
    .writeStream \
    .format("delta") \
    .option("checkpointLocation", "s3://checkpoints/") \
    .start("s3://feature-store/realtime_features/")

# After 30 days: 300,000 files (avg 500 KB each)
# Query time: 5 minutes to read 1 day of data
```

**Solution: Auto-Optimize + Manual Compaction:**
```python
from delta.tables import DeltaTable

# Enable auto-optimize for new writes
spark.sql("""
ALTER TABLE feature_store.realtime_features
SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true'
)
""")

# Compact existing small files
delta_table = DeltaTable.forPath(spark, "s3://feature-store/realtime_features/")
delta_table.optimize().executeCompaction()

# Result: 300,000 files → 150 files (~1 GB each)
# Query time: 5 minutes → 10 seconds (30x improvement)
```

### Example 4: Bloom Filters for Point Lookups

**Scenario:** Feature store with 10 billion user records, need fast lookups by `user_id`.

**Without Bloom Filters:**
```python
# Full scan of all row groups
df = spark.read.format("delta").load("s3://feature-store/users/")
result = df.filter("user_id = 987654321").select("features")

# Query time: 45 seconds (scans 500 GB across 500 files)
```

**With Bloom Filters:**
```python
# Create table with bloom filter on user_id
spark.sql("""
CREATE TABLE feature_store.users (
  user_id BIGINT,
  features ARRAY<DOUBLE>,
  updated_at TIMESTAMP
)
USING DELTA
TBLPROPERTIES (
  'delta.bloomFilter.user_id' = '10000000000'  -- 10B expected items
)
""")

# Same query now uses bloom filter
result = spark.read.format("delta") \
    .load("s3://feature-store/users/") \
    .filter("user_id = 987654321") \
    .select("features")

# Query time: 1.2 seconds (99.7% of data skipped)
```

**Bloom Filter Sizing:**
- **Expected items**: 10 billion users
- **False positive rate**: 1% (default)
- **Bloom filter size**: ~14.4 GB total (~30 MB per file)
- **Storage overhead**: 0.3% of 5 TB dataset

### Example 5: Multi-Tier Caching for Training

**Scenario:** Training a transformer model, repeatedly reading the same 50 GB dataset.

**Without Caching:**
```python
# Epoch 1: Read from S3 (3 minutes)
# Epoch 2: Read from S3 again (3 minutes)
# Total time for 10 epochs: 30 minutes of I/O
```

**With Spark Caching:**
```python
# Load and cache in memory
df = spark.read.parquet("s3://ml-data/train/")
df.cache()  # Stores in executor memory
df.count()  # Triggers caching

# Epoch 1: Read from S3 + cache (3 minutes)
# Epoch 2-10: Read from memory cache (5 seconds each)
# Total time for 10 epochs: 3 min 45 sec (8x improvement)
```

**With Alluxio (Local SSD Cache):**
```python
# Configure Spark to use Alluxio
spark.conf.set("spark.hadoop.fs.s3a.impl", "alluxio.hadoop.FileSystem")
spark.conf.set("spark.hadoop.fs.s3a.bucket.ml-data.alluxio.underfs.address", "s3://ml-data/")

# First read: S3 → Alluxio → Spark (3 minutes)
# Subsequent reads: Alluxio → Spark (30 seconds)
# Benefit: Cache persists across Spark sessions
```

### Example 6: Dictionary Encoding for Categorical Features

**Scenario:** 1 TB dataset with categorical column `product_category` (10,000 unique values).

**Without Dictionary Encoding (PLAIN):**
```python
# Average category length: 20 bytes
# 1 TB / 1000 bytes per row → 1 billion rows
# Category column size: 1B rows × 20 bytes = 20 GB uncompressed
# With Snappy: 20 GB → 8 GB
```

**With Dictionary Encoding:**
```python
# Dictionary: 10,000 categories × 20 bytes = 200 KB
# Encoded values: 1B rows × 2 bytes (16-bit IDs) = 2 GB
# Total: 2 GB + 200 KB ≈ 2 GB
# Compression: 4x better than PLAIN + Snappy
```

**How to Enable:**
```python
# Parquet automatically uses dictionary encoding for low-cardinality columns
# To force it:
df.write.parquet("s3://bucket/data/", compression="snappy")
# Parquet writer detects cardinality and chooses encoding

# Verify encoding used:
import pyarrow.parquet as pq
parquet_file = pq.ParquetFile("s3://bucket/data/part-00000.parquet")
print(parquet_file.metadata.row_group(0).column(0).encodings)
# Output: ['RLE_DICTIONARY', 'PLAIN']
```

---

## 🔧 Code Examples

### Code Example 1: Comprehensive Parquet Writer Configuration

```python
from pyspark.sql import SparkSession
from pyspark.sql.types import *

# Initialize Spark with optimal configurations
spark = SparkSession.builder \
    .appName("Optimized Parquet Writer") \
    .config("spark.sql.parquet.compression.codec", "snappy") \
    .config("spark.sql.parquet.block.size", 1024 * 1024 * 1024)  # 1 GB \
    .config("spark.sql.parquet.page.size", 8 * 1024 * 1024)      # 8 MB \
    .config("spark.sql.parquet.enableVectorizedReader", "true") \
    .config("spark.sql.parquet.mergeSchema", "false") \
    .config("spark.sql.files.maxPartitionBytes", 1024 * 1024 * 1024) \
    .config("spark.sql.shuffle.partitions", 200) \
    .getOrCreate()

# Define schema for ML features
schema = StructType([
    StructField("user_id", LongType(), nullable=False),
    StructField("timestamp", TimestampType(), nullable=False),
    StructField("category", StringType(), nullable=False),
    StructField("features", ArrayType(DoubleType()), nullable=False),
    StructField("label", IntegerType(), nullable=True)
])

# Read raw data
df = spark.read.schema(schema).json("s3://ml-data/raw/")

# Optimize partitioning for ~1 GB files
num_partitions = int(df.count() * 1000 / (1024 ** 3))  # Estimate based on row count
df_optimized = df.repartition(num_partitions, "timestamp")  # Co-locate by time

# Write with optimal settings
df_optimized.write.mode("overwrite") \
    .partitionBy("category")  # Partition for predicate pushdown \
    .option("compression", "snappy") \
    .option("parquet.block.size", 1024 * 1024 * 1024) \
    .option("parquet.page.size", 8 * 1024 * 1024) \
    .option("parquet.enable.dictionary", "true") \
    .parquet("s3://ml-data/optimized/features/")

print("✅ Parquet optimization complete")
```

### Code Example 2: Zstandard Compression Comparison

```python
import time
import pandas as pd

# Generate sample data
data = {
    'user_id': range(10_000_000),
    'age': [25, 30, 35, 40, 45] * 2_000_000,  # Low cardinality
    'city': ['NYC', 'SF', 'LA', 'Chicago', 'Boston'] * 2_000_000,
    'features': [[0.1] * 128] * 10_000_000  # 128-dim embeddings
}
df = pd.DataFrame(data)

# Test different compression algorithms
compressions = ['snappy', 'gzip', 'zstd', 'lz4', 'brotli']
results = []

for compression in compressions:
    start = time.time()

    # Write
    write_start = time.time()
    df.to_parquet(f'/tmp/test_{compression}.parquet', compression=compression)
    write_time = time.time() - write_start

    # Get file size
    import os
    file_size = os.path.getsize(f'/tmp/test_{compression}.parquet') / (1024 ** 2)  # MB

    # Read
    read_start = time.time()
    df_read = pd.read_parquet(f'/tmp/test_{compression}.parquet')
    read_time = time.time() - read_start

    results.append({
        'compression': compression,
        'file_size_mb': file_size,
        'write_time_s': write_time,
        'read_time_s': read_time,
        'compression_ratio': 1000 / file_size  # Assuming ~1000 MB uncompressed
    })

# Display results
results_df = pd.DataFrame(results)
print(results_df.to_string(index=False))

# Example output:
# compression  file_size_mb  write_time_s  read_time_s  compression_ratio
# snappy       320.5         2.3           0.8          3.1x
# gzip         180.2         8.1           2.4          5.5x
# zstd         210.7         3.2           1.1          4.7x
# lz4          350.1         1.8           0.6          2.9x
# brotli       165.3         45.2          3.1          6.0x
```

### Code Example 3: Small Files Consolidation Pipeline

```python
from delta.tables import DeltaTable
from pyspark.sql import functions as F

class SmallFilesOptimizer:
    def __init__(self, spark, table_path, target_file_size_mb=1024):
        self.spark = spark
        self.table_path = table_path
        self.target_file_size_bytes = target_file_size_mb * 1024 * 1024
        self.delta_table = DeltaTable.forPath(spark, table_path)

    def analyze_files(self):
        """Analyze file size distribution"""
        files_df = self.spark.sql(f"""
            DESCRIBE DETAIL delta.`{self.table_path}`
        """)

        num_files = files_df.select("numFiles").first()[0]
        size_in_bytes = files_df.select("sizeInBytes").first()[0]
        avg_file_size = size_in_bytes / num_files if num_files > 0 else 0

        print(f"📊 File Analysis:")
        print(f"  Total files: {num_files:,}")
        print(f"  Total size: {size_in_bytes / (1024**3):.2f} GB")
        print(f"  Average file size: {avg_file_size / (1024**2):.2f} MB")

        return num_files, avg_file_size

    def compact_small_files(self):
        """Compact files smaller than target size"""
        num_files, avg_file_size = self.analyze_files()

        if avg_file_size < self.target_file_size_bytes / 2:
            print(f"🔧 Compacting files (avg {avg_file_size / (1024**2):.1f} MB < target {self.target_file_size_bytes / (1024**2)} MB)")

            # Run OPTIMIZE with bin-packing
            self.delta_table.optimize().executeCompaction()

            # Re-analyze
            new_num_files, new_avg_file_size = self.analyze_files()
            improvement = (num_files - new_num_files) / num_files * 100

            print(f"✅ Compaction complete:")
            print(f"  Files reduced: {num_files:,} → {new_num_files:,} ({improvement:.1f}% reduction)")
            print(f"  Avg file size: {avg_file_size / (1024**2):.1f} MB → {new_avg_file_size / (1024**2):.1f} MB")
        else:
            print(f"✅ Files already optimized (avg {avg_file_size / (1024**2):.1f} MB)")

    def z_order_optimize(self, z_order_cols):
        """Apply Z-ORDER clustering for better data skipping"""
        print(f"🔧 Applying Z-ORDER on columns: {z_order_cols}")

        self.delta_table.optimize().executeZOrderBy(z_order_cols)

        print(f"✅ Z-ORDER complete")

# Usage
optimizer = SmallFilesOptimizer(
    spark=spark,
    table_path="s3://feature-store/user_features/",
    target_file_size_mb=1024
)

optimizer.analyze_files()
optimizer.compact_small_files()
optimizer.z_order_optimize(["user_id", "timestamp"])
```

### Code Example 4: Bloom Filter Implementation

```python
from pyspark.sql import functions as F
from delta.tables import DeltaTable

class BloomFilterManager:
    def __init__(self, spark, table_path):
        self.spark = spark
        self.table_path = table_path

    def create_bloom_filter_table(self, schema, bloom_columns, expected_items=10_000_000):
        """Create Delta table with bloom filters on specified columns"""

        # Create table
        self.spark.sql(f"""
            CREATE TABLE IF NOT EXISTS delta.`{self.table_path}` (
                {self._schema_to_ddl(schema)}
            )
            USING DELTA
        """)

        # Configure bloom filters
        for col in bloom_columns:
            self.spark.sql(f"""
                ALTER TABLE delta.`{self.table_path}`
                SET TBLPROPERTIES (
                    'delta.bloomFilter.{col}' = '{expected_items}'
                )
            """)

        print(f"✅ Created table with bloom filters on: {bloom_columns}")

    def _schema_to_ddl(self, schema):
        """Convert StructType to DDL string"""
        fields = []
        for field in schema.fields:
            nullable = "NULL" if field.nullable else "NOT NULL"
            fields.append(f"{field.name} {field.dataType.simpleString()} {nullable}")
        return ",\n    ".join(fields)

    def benchmark_lookup(self, filter_condition, num_iterations=10):
        """Benchmark lookup performance with/without bloom filters"""
        import time

        times = []
        for i in range(num_iterations):
            start = time.time()
            result = self.spark.read.format("delta") \
                .load(self.table_path) \
                .filter(filter_condition) \
                .count()
            elapsed = time.time() - start
            times.append(elapsed)

        avg_time = sum(times) / len(times)
        print(f"📊 Average lookup time: {avg_time:.2f}s (result: {result} rows)")
        return avg_time

# Usage
from pyspark.sql.types import *

schema = StructType([
    StructField("user_id", LongType(), False),
    StructField("session_id", StringType(), False),
    StructField("features", ArrayType(DoubleType()), False),
    StructField("timestamp", TimestampType(), False)
])

bloom_mgr = BloomFilterManager(
    spark=spark,
    table_path="s3://feature-store/sessions/"
)

bloom_mgr.create_bloom_filter_table(
    schema=schema,
    bloom_columns=["user_id", "session_id"],
    expected_items=10_000_000_000  # 10 billion records
)

# Benchmark
bloom_mgr.benchmark_lookup("user_id = 987654321")
```

### Code Example 5: Storage Format Converter

```python
class StorageFormatConverter:
    """Convert between storage formats with optimization"""

    def __init__(self, spark):
        self.spark = spark

    def csv_to_parquet(self, input_path, output_path, schema=None,
                       compression="snappy", target_file_size_mb=1024):
        """Convert CSV to optimized Parquet"""
        print(f"🔄 Converting CSV to Parquet...")

        # Read CSV
        df = self.spark.read.csv(input_path, header=True, inferSchema=(schema is None), schema=schema)

        # Estimate partitions for target file size
        size_estimate = df.count() * 1000  # Rough estimate: 1KB per row
        num_partitions = max(1, int(size_estimate / (target_file_size_mb * 1024 * 1024)))

        # Write optimized Parquet
        df.repartition(num_partitions).write.mode("overwrite") \
            .option("compression", compression) \
            .option("parquet.block.size", target_file_size_mb * 1024 * 1024) \
            .parquet(output_path)

        print(f"✅ Conversion complete: {output_path}")

    def parquet_to_delta(self, input_path, output_path, partition_cols=None):
        """Convert Parquet to Delta Lake with optimization"""
        print(f"🔄 Converting Parquet to Delta Lake...")

        df = self.spark.read.parquet(input_path)

        writer = df.write.format("delta").mode("overwrite")

        if partition_cols:
            writer = writer.partitionBy(*partition_cols)

        writer.save(output_path)

        # Enable auto-optimize
        self.spark.sql(f"""
            ALTER TABLE delta.`{output_path}`
            SET TBLPROPERTIES (
                'delta.autoOptimize.optimizeWrite' = 'true',
                'delta.autoOptimize.autoCompact' = 'true'
            )
        """)

        print(f"✅ Delta Lake table created: {output_path}")

    def json_to_parquet(self, input_path, output_path, compression="zstd"):
        """Convert JSON to Parquet with Zstandard compression"""
        print(f"🔄 Converting JSON to Parquet...")

        df = self.spark.read.json(input_path)

        # Infer optimal partitions
        size_estimate = df.count() * 2000  # JSON typically 2KB per row
        num_partitions = max(1, int(size_estimate / (1024 ** 3)))  # 1 GB files

        df.repartition(num_partitions).write.mode("overwrite") \
            .option("compression", compression) \
            .parquet(output_path)

        print(f"✅ Conversion complete: {output_path}")

    def compare_formats(self, df, output_dir="/tmp/format_comparison"):
        """Compare storage efficiency across formats"""
        import os

        formats = [
            ("csv", "gzip"),
            ("parquet", "snappy"),
            ("parquet", "zstd"),
            ("orc", "zlib"),
            ("avro", "snappy")
        ]

        results = []
        for fmt, compression in formats:
            path = f"{output_dir}/{fmt}_{compression}"

            # Write
            import time
            start = time.time()

            if fmt == "csv":
                df.write.mode("overwrite").option("compression", compression).csv(path)
            elif fmt == "parquet":
                df.write.mode("overwrite").option("compression", compression).parquet(path)
            elif fmt == "orc":
                df.write.mode("overwrite").option("compression", compression).orc(path)
            elif fmt == "avro":
                df.write.mode("overwrite").option("compression", compression).format("avro").save(path)

            write_time = time.time() - start

            # Get size
            size = sum(os.path.getsize(os.path.join(path, f)) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)))
            size_mb = size / (1024 ** 2)

            # Read time
            start = time.time()
            if fmt == "csv":
                self.spark.read.csv(path).count()
            elif fmt == "parquet":
                self.spark.read.parquet(path).count()
            elif fmt == "orc":
                self.spark.read.orc(path).count()
            elif fmt == "avro":
                self.spark.read.format("avro").load(path).count()
            read_time = time.time() - start

            results.append({
                'format': f"{fmt}+{compression}",
                'size_mb': size_mb,
                'write_time': write_time,
                'read_time': read_time
            })

        import pandas as pd
        print(pd.DataFrame(results).to_string(index=False))

# Usage
converter = StorageFormatConverter(spark)

# Convert CSV to Parquet
converter.csv_to_parquet(
    input_path="s3://ml-data/raw/features.csv",
    output_path="s3://ml-data/parquet/features/",
    compression="snappy",
    target_file_size_mb=1024
)

# Convert Parquet to Delta Lake
converter.parquet_to_delta(
    input_path="s3://ml-data/parquet/features/",
    output_path="s3://ml-data/delta/features/",
    partition_cols=["date", "model_version"]
)

# Compare formats
sample_df = spark.read.parquet("s3://ml-data/parquet/features/").limit(1_000_000)
converter.compare_formats(sample_df)
```

### Code Example 6: I/O Profiler for ML Pipelines

```python
import time
from functools import wraps
from pyspark.sql import DataFrame

class IOProfiler:
    """Profile I/O operations in ML pipelines"""

    def __init__(self):
        self.metrics = []

    def profile_read(self, func):
        """Decorator to profile read operations"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            start = time.time()

            # Execute read
            result = func(*args, **kwargs)

            # Force evaluation
            if isinstance(result, DataFrame):
                count = result.count()
            else:
                count = None

            elapsed = time.time() - start

            # Collect metrics
            self.metrics.append({
                'operation': func.__name__,
                'type': 'read',
                'duration_s': elapsed,
                'row_count': count
            })

            print(f"📖 {func.__name__}: {elapsed:.2f}s ({count:,} rows)" if count else f"📖 {func.__name__}: {elapsed:.2f}s")

            return result
        return wrapper

    def profile_write(self, func):
        """Decorator to profile write operations"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            start = time.time()

            result = func(*args, **kwargs)

            elapsed = time.time() - start

            self.metrics.append({
                'operation': func.__name__,
                'type': 'write',
                'duration_s': elapsed
            })

            print(f"✍️ {func.__name__}: {elapsed:.2f}s")

            return result
        return wrapper

    def get_summary(self):
        """Get summary of all profiled operations"""
        import pandas as pd

        df = pd.DataFrame(self.metrics)

        summary = df.groupby('type').agg({
            'duration_s': ['sum', 'mean', 'count']
        }).round(2)

        print("\n📊 I/O Profile Summary:")
        print(summary)

        return summary

# Usage
profiler = IOProfiler()

@profiler.profile_read
def load_training_data(spark, path):
    return spark.read.parquet(path)

@profiler.profile_write
def save_features(df, path):
    df.write.mode("overwrite").parquet(path)

# Execute pipeline
df = load_training_data(spark, "s3://ml-data/train/")
df_features = df.select("user_id", "features")
save_features(df_features, "s3://ml-data/features/")

# Get summary
profiler.get_summary()
```

---

## ✅ Best Practices

### 1. File Format Selection

**DO:**
- ✅ Use **Parquet** as the default format for ML training data
- ✅ Use **Delta Lake** (Parquet + transaction log) for production feature stores
- ✅ Use **Avro** for streaming data from Kafka before landing in Parquet
- ✅ Use **Feather/Arrow** for in-memory datasets loaded into RAM/GPU
- ✅ Use **TFRecord** only if you're exclusively using TensorFlow and need native integration

**DON'T:**
- ❌ Use CSV for large-scale ML pipelines (10-50x slower than Parquet)
- ❌ Use JSON for training data (use JSONL + Zstd only for exploratory data)
- ❌ Mix formats within a single dataset (e.g., some partitions in Parquet, others in ORC)
- ❌ Use row-oriented formats (Avro, JSON) for analytics-heavy feature engineering

### 2. Compression Configuration

**DO:**
- ✅ Use **Snappy** for training datasets that are read frequently (balanced speed/ratio)
- ✅ Use **Zstandard (level 3)** for feature stores with infrequent access (30-40% better compression)
- ✅ Use **LZ4** for real-time inference where decompression latency < 10ms is critical
- ✅ Use **Gzip** for archival data with regulatory retention requirements
- ✅ Test compression on a sample dataset before applying to production

**DON'T:**
- ❌ Use Brotli for ML workloads (10-50x slower compression, minimal benefit)
- ❌ Use no compression unless you have infinite storage budget
- ❌ Use Gzip for training data (3-5x slower decompression than Snappy)
- ❌ Change compression mid-pipeline (maintain consistency per dataset)

### 3. File Sizing

**DO:**
- ✅ Target **128 MB - 1 GB per file** for cloud object storage (S3, GCS, Azure Blob)
- ✅ Use `df.repartition(N)` or `df.coalesce(N)` to control file count
- ✅ Run **weekly compaction** on tables with frequent small writes (streaming, real-time features)
- ✅ Enable **auto-optimize** in Delta Lake for automatic file management
- ✅ Monitor file count and average size using `DESCRIBE DETAIL` (Delta) or `ls -lh` (raw files)

**DON'T:**
- ❌ Write millions of small files (< 10 MB) - causes driver OOM and slow queries
- ❌ Write files larger than 2 GB - limits parallelism and increases failure risk
- ❌ Ignore file size distribution (some 10 MB, some 2 GB) - causes unbalanced partitions
- ❌ Use default Spark partitioning without tuning (often creates too many small files)

### 4. Row Group and Page Sizing

**DO:**
- ✅ Use **256 MB - 1 GB row groups** for large sequential scans (training)
- ✅ Use **1-8 MB page size** to balance seek overhead and column pruning
- ✅ Align row group size with file size (1 row group per file for simplicity)
- ✅ Use larger row groups (1 GB) when predicate pushdown is not critical
- ✅ Use smaller row groups (128 MB) when you frequently filter by time ranges

**DON'T:**
- ❌ Use tiny row groups (< 64 MB) - reduces compression efficiency
- ❌ Use huge row groups (> 2 GB) - may hit memory limits during reads
- ❌ Use page sizes < 512 KB (too many seeks) or > 16 MB (defeats column pruning)

### 5. Partitioning and Data Skipping

**DO:**
- ✅ Partition by **low-cardinality, frequently filtered columns** (date, model_version, category)
- ✅ Limit partitions to **< 10,000 total directories** (more causes metadata overhead)
- ✅ Use **Z-ORDER** (Delta Lake) or **CLUSTER BY** (BigQuery) for multi-column filtering
- ✅ Add **bloom filters** for high-cardinality point lookups (user_id, session_id)
- ✅ Collect statistics on all columns to enable predicate pushdown

**DON'T:**
- ❌ Partition by high-cardinality columns (user_id, timestamp_ms) - creates millions of directories
- ❌ Over-partition (> 10K partitions) - slows down metadata operations
- ❌ Under-partition (< 10 partitions) - limits parallelism
- ❌ Partition by columns you never filter on (wastes I/O and metadata space)

### 6. Caching Strategies

**DO:**
- ✅ Cache **preprocessed training datasets** to avoid repeated ETL (use `df.cache()`)
- ✅ Use **Alluxio** or similar for multi-session caching across Spark clusters
- ✅ Cache **validation sets** during hyperparameter tuning to speed up evaluation loops
- ✅ Implement **LRU eviction** for general-purpose ML caching
- ✅ Monitor cache hit rates and eviction rates to validate effectiveness

**DON'T:**
- ❌ Cache raw data before preprocessing (cache after transformations)
- ❌ Cache datasets larger than available cluster memory (causes spilling to disk)
- ❌ Cache data you only read once (no benefit, wastes memory)
- ❌ Forget to unpersist datasets when no longer needed (`df.unpersist()`)

### 7. Encoding and Dictionary Compression

**DO:**
- ✅ Let Parquet **automatically choose encodings** based on data characteristics
- ✅ Use **dictionary encoding** for categorical features with < 40K unique values
- ✅ Use **delta encoding** for sorted numerical columns (timestamps, incrementing IDs)
- ✅ Use **RLE (run-length encoding)** for boolean flags and sparse features
- ✅ Verify encoding usage with `pyarrow.parquet.read_metadata()`

**DON'T:**
- ❌ Force PLAIN encoding for low-cardinality columns (loses 5-10x compression)
- ❌ Use dictionary encoding for high-cardinality columns (> 100K unique values) - wastes memory
- ❌ Assume all columns use the same encoding (Parquet chooses per-column)

### 8. Monitoring and Profiling

**DO:**
- ✅ Profile I/O operations using **Spark UI** (look for I/O wait time in task metrics)
- ✅ Monitor **bytes read vs bytes scanned** to measure predicate pushdown effectiveness
- ✅ Track **file count, average size, compression ratio** in production tables
- ✅ Set up alerts for **> 10K files per table** or **average file size < 64 MB**
- ✅ Benchmark different formats/compressions on sample data before production rollout

**DON'T:**
- ❌ Ignore I/O bottlenecks (often account for 50-80% of training time)
- ❌ Assume storage is "fast enough" without measuring actual throughput
- ❌ Optimize CPU/memory without first profiling I/O (premature optimization)
- ❌ Deploy storage changes without A/B testing on real workloads

### 9. Cost Optimization

**DO:**
- ✅ Use **S3 Intelligent-Tiering** for datasets with unpredictable access patterns
- ✅ Transition **cold training data** (> 90 days old) to Glacier or Deep Archive
- ✅ Implement **lifecycle policies** to automatically delete old checkpoints and logs
- ✅ Compress with **Zstandard** for long-term storage (30-50% savings vs Snappy)
- ✅ Monitor **storage costs per TB per month** and set budgets

**DON'T:**
- ❌ Store uncompressed data in S3 (3-10x higher costs)
- ❌ Keep duplicate datasets (raw + processed) without clear versioning strategy
- ❌ Retain infinite training checkpoints (delete all but best N models)
- ❌ Store interim results in Standard storage (use Standard-IA or Glacier for > 30 days)

### 10. Schema Evolution and Versioning

**DO:**
- ✅ Use **Delta Lake** or **Iceberg** for schema evolution and time travel
- ✅ Add new columns at the end of schema (Parquet handles this gracefully)
- ✅ Version feature store schemas with semantic versioning (v1.0.0, v1.1.0)
- ✅ Test schema changes on a sample dataset before applying to production
- ✅ Document breaking changes in `CHANGELOG.md`

**DON'T:**
- ❌ Change column data types (INT → STRING) without migrating existing data
- ❌ Rename columns without backward compatibility layer
- ❌ Delete columns still used by production models
- ❌ Mix schema versions in the same table without explicit versioning

---

## ⚠️ Common Pitfalls

### Pitfall 1: Small Files Explosion

**Problem:**
```python
# Streaming writes create 1 file per micro-batch
spark.readStream.format("kafka").load() \
    .writeStream.format("delta") \
    .start("s3://features/realtime/")

# After 1 week: 100,000 files (avg 500 KB)
# Query time: 10 minutes instead of 10 seconds
```

**Why It Happens:**
- Streaming writes default to 1 file per partition per batch
- No automatic compaction in vanilla Spark
- Each file incurs S3 API overhead (LIST, GET)

**Solution:**
```python
# Enable auto-compaction
spark.sql("""
ALTER TABLE delta.`s3://features/realtime/`
SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true'
)
""")

# Schedule daily compaction
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "s3://features/realtime/")
delta_table.optimize().executeCompaction()
```

**Prevention:**
- Monitor file count with `DESCRIBE DETAIL` or `aws s3 ls --summarize`
- Set up alerts for > 10K files per table
- Run weekly compaction jobs

### Pitfall 2: Wrong Compression for Workload

**Problem:**
```python
# Using Gzip for frequently accessed training data
df.write.option("compression", "gzip").parquet("s3://train/")

# Training loop reads data 100 times per day
# Decompression overhead: +300% CPU, 4x slower reads
```

**Why It Happens:**
- Developers optimize for storage cost without considering read patterns
- Default compression (Snappy) changed to Gzip for "better compression"
- No benchmarking on actual workload

**Solution:**
```python
# Use Snappy for read-heavy workloads
df.write.option("compression", "snappy").parquet("s3://train/")

# Benchmark shows:
# - Gzip: 180 GB, 8 min read time
# - Snappy: 280 GB, 2 min read time
# Trade-off: +100 GB storage (+$2.30/month) for 4x faster training
```

**Prevention:**
- Profile read vs write frequency before choosing compression
- Use Gzip/Zstd only for archival data (> 90 days retention)
- Benchmark on representative dataset (don't assume)

### Pitfall 3: Ignoring Row Group Size

**Problem:**
```python
# Using default 128 MB row groups for 10 TB dataset
# Predicate pushdown on time ranges reads entire row groups

df = spark.read.parquet("s3://data/")
df_filtered = df.filter("timestamp > '2025-10-01'")
# Reads 100 GB instead of 10 GB (90% wasted I/O)
```

**Why It Happens:**
- Default 128 MB row groups don't align with time-based filtering
- Row groups span multiple days/weeks
- Statistics (min/max timestamp) cover too broad a range

**Solution:**
```python
# Use larger row groups + Z-ORDER for time-based filtering
df.write.format("delta") \
    .option("parquet.block.size", 1024 * 1024 * 1024)  # 1 GB \
    .save("s3://data/")

# Apply Z-ORDER on timestamp
DeltaTable.forPath(spark, "s3://data/").optimize().executeZOrderBy("timestamp")

# Now filtering skips 90% of data
```

**Prevention:**
- Tune row group size based on query patterns
- Use Z-ORDER or CLUSTER BY for multi-column filtering
- Monitor "bytes scanned" vs "bytes returned" ratio

### Pitfall 4: Partitioning by High-Cardinality Column

**Problem:**
```python
# Partition by user_id (10M unique users)
df.write.partitionBy("user_id").parquet("s3://features/")

# Creates 10,000,000 directories
# S3 LIST operations: 10,000 requests × $0.005/1000 = $50/list
# Spark driver OOM from metadata
```

**Why It Happens:**
- Misunderstanding of partitioning vs clustering
- Trying to optimize point lookups with partitioning
- Not considering metadata overhead

**Solution:**
```python
# Use bloom filters for point lookups, partition by date
df.write.format("delta") \
    .partitionBy("date") \
    .save("s3://features/")

spark.sql("""
ALTER TABLE delta.`s3://features/`
SET TBLPROPERTIES (
  'delta.bloomFilter.user_id' = '10000000'  -- 10M expected items
)
""")

# Now lookups use bloom filter, partitions are manageable
```

**Prevention:**
- Partition by low-cardinality columns (< 10K values)
- Use Z-ORDER or bloom filters for high-cardinality lookups
- Calculate partition count: `df.select("partition_col").distinct().count()`

### Pitfall 5: Not Caching Frequently Accessed Data

**Problem:**
```python
# Training loop reads same validation set 100 times
for epoch in range(100):
    model.train(train_df)
    loss = model.evaluate(val_df)  # Reads from S3 every time

# Total I/O: 100 × 50 GB = 5 TB
# Cost: 5 TB × $0.09/GB (S3 transfer) = $450
```

**Why It Happens:**
- Developers don't realize data is read multiple times
- No awareness of caching mechanisms
- Premature concern about memory usage

**Solution:**
```python
# Cache validation set in memory
val_df = spark.read.parquet("s3://data/val/")
val_df.cache()
val_df.count()  # Trigger caching

for epoch in range(100):
    model.train(train_df)
    loss = model.evaluate(val_df)  # Reads from memory

# I/O: 1 × 50 GB = 50 GB (100x reduction)
# Cost: $4.50 (100x savings)
```

**Prevention:**
- Profile data access patterns (how many times is data read?)
- Cache datasets read > 2 times
- Monitor cache hit rates in Spark UI

### Pitfall 6: Missing Statistics and Bloom Filters

**Problem:**
```python
# Query with high selectivity
df = spark.read.parquet("s3://users/")
result = df.filter("user_id = 987654321")  # 1 row out of 10B

# Scans all 10 billion rows (5 TB)
# Query time: 5 minutes
```

**Why It Happens:**
- No column statistics collected
- No bloom filters configured
- Parquet statistics not leveraged

**Solution:**
```python
# Convert to Delta Lake with bloom filter
df.write.format("delta").save("s3://users_delta/")

spark.sql("""
ALTER TABLE delta.`s3://users_delta/`
SET TBLPROPERTIES (
  'delta.bloomFilter.user_id' = '10000000000'  -- 10B expected
)
""")

# Same query now skips 99.99% of data
result = spark.read.format("delta").load("s3://users_delta/") \
    .filter("user_id = 987654321")

# Query time: 0.5 seconds (600x improvement)
```

**Prevention:**
- Use Delta Lake or Iceberg for automatic statistics
- Configure bloom filters for high-cardinality point lookups
- Monitor data skipping effectiveness

### Pitfall 7: Inefficient Schema for Nested Data

**Problem:**
```python
# Storing 128-dim embeddings as JSON strings
df = spark.createDataFrame([
    (1, '{"embedding": [0.1, 0.2, ..., 0.128]}'),  # 2KB per row
    (2, '{"embedding": [0.3, 0.4, ..., 0.256]}')
], ["id", "embedding_json"])

df.write.parquet("s3://embeddings/")
# Result: 200 GB (should be 50 GB with proper schema)
```

**Why It Happens:**
- Laziness in schema design (using JSON strings for everything)
- Not understanding Parquet's support for nested types
- Data exported from JSON APIs without transformation

**Solution:**
```python
from pyspark.sql.types import *

# Proper schema with array type
schema = StructType([
    StructField("id", LongType()),
    StructField("embedding", ArrayType(FloatType()))
])

df = spark.createDataFrame([
    (1, [0.1, 0.2, ..., 0.128]),  # 512 bytes per row
    (2, [0.3, 0.4, ..., 0.256])
], schema)

df.write.parquet("s3://embeddings/")
# Result: 50 GB (4x smaller, 10x faster to read)
```

**Prevention:**
- Use native array/struct types for nested data
- Avoid JSON strings unless truly schema-less
- Validate schema before writing production datasets

### Pitfall 8: No File Consolidation in Streaming

**Problem:**
```python
# Streaming writes every 10 seconds
spark.readStream.format("kafka").load() \
    .writeStream.trigger(processingTime="10 seconds") \
    .format("delta").start("s3://events/")

# After 1 day: 8,640 micro-batches × 10 partitions = 86,400 files
# After 30 days: 2,592,000 files
```

**Why It Happens:**
- Streaming defaults to very frequent writes
- No automatic compaction enabled
- Focus on low latency over storage efficiency

**Solution:**
```python
# Enable auto-optimize
spark.sql("""
ALTER TABLE delta.`s3://events/`
SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true',
  'delta.deletedFileRetentionDuration' = 'interval 7 days'  -- Clean up old files
)
""")

# Schedule daily OPTIMIZE
delta_table = DeltaTable.forPath(spark, "s3://events/")
delta_table.optimize().executeCompaction()

# Vacuum old files weekly
delta_table.vacuum(168)  # 7 days in hours
```

**Prevention:**
- Enable auto-optimize from day 1
- Schedule regular compaction jobs
- Monitor file count growth over time

---

## 🏋️ Hands-On Exercises

### Exercise 1: Parquet Optimization Challenge (2-3 hours)

**Objective:** Optimize a poorly configured Parquet dataset for ML training.

**Scenario:** You have a 1 TB dataset of user activity logs stored as 50,000 CSV files (20 MB each). Training reads take 30 minutes. Your goal: Reduce to < 3 minutes.

**Tasks:**
1. Convert CSV to Parquet with Snappy compression
2. Consolidate to 1 GB files (target: 1,024 files)
3. Configure 256 MB row groups and 8 MB pages
4. Partition by `date` (assume 365 days of data)
5. Measure:
   - Storage size reduction
   - Read time improvement
   - Compression ratio
   - File count before/after

**Starter Code:**
```python
# Step 1: Read CSV
df = spark.read.csv("s3://raw/activity_logs/", header=True, inferSchema=True)

# Step 2: TODO - Repartition to 1024 partitions

# Step 3: TODO - Write as Parquet with configuration

# Step 4: TODO - Measure file count and size

# Step 5: TODO - Benchmark read time
```

**Success Criteria:**
- Storage: 1 TB → < 400 GB (> 2.5x compression)
- Read time: 30 min → < 3 min (> 10x improvement)
- Files: 50,000 → ~1,024
- Row group size: ~256 MB

**Stretch Goal:** Implement Z-ORDER on `user_id` and `date` for better data skipping.

---

### Exercise 2: Compression Algorithm Selection (1-2 hours)

**Objective:** Choose optimal compression for different ML workloads.

**Scenario:** You have three datasets:
1. **Training data**: 500 GB, read 100 times/day during training
2. **Feature store**: 2 TB, read 10 times/day for batch inference
3. **Archival logs**: 10 TB, read once/quarter for compliance audits

**Tasks:**
1. Benchmark Snappy, Zstandard, Gzip, and LZ4 on sample data (10 GB)
2. Measure:
   - Compression ratio
   - Write time (compression)
   - Read time (decompression)
   - CPU usage during reads
3. Calculate cost-benefit for each dataset:
   - Storage cost (S3 Standard: $0.023/GB/month)
   - Compute cost (EC2 c6i.2xlarge: $0.34/hour)
4. Select optimal compression for each dataset
5. Estimate monthly savings vs default (Snappy for all)

**Starter Code:**
```python
# Generate sample data
df = spark.range(100_000_000).selectExpr(
    "id as user_id",
    "cast(rand() * 100 as int) as age",
    "array(rand(), rand(), rand()) as features"  # 128-dim embedding
)

# TODO: Benchmark each compression
compressions = ['snappy', 'gzip', 'zstd', 'lz4']

for compression in compressions:
    # TODO: Write with compression and measure time
    # TODO: Read and measure time
    # TODO: Get file size
    # TODO: Calculate cost
```

**Success Criteria:**
- Training data: Choose LZ4 or Snappy (fast decompression > compression ratio)
- Feature store: Choose Zstandard level 3 (balanced)
- Archival: Choose Gzip or Zstd level 9 (max compression)
- Document trade-offs in a decision matrix

**Stretch Goal:** Implement adaptive compression that chooses algorithm based on file access frequency.

---

### Exercise 3: Small Files Remediation (3-4 hours)

**Objective:** Fix a production table with severe small files problem.

**Scenario:** A real-time feature store has accumulated 500,000 files over 6 months:
- 450,000 files < 10 MB (avg 2 MB)
- 45,000 files 10-100 MB
- 5,000 files 100 MB - 1 GB
- Total size: 800 GB
- Query time: 8 minutes (should be < 30 seconds)

**Tasks:**
1. Analyze file size distribution using Spark or AWS CLI
2. Implement bin-packing consolidation to target 1 GB files
3. Configure auto-optimize to prevent future small files
4. Measure before/after:
   - File count
   - Average file size
   - Query time (SELECT with WHERE clause)
   - S3 LIST API calls
5. Calculate monthly cost savings

**Starter Code:**
```python
from delta.tables import DeltaTable

# Step 1: Analyze files
spark.sql("""
SELECT
  COUNT(*) as total_files,
  AVG(file_size_in_bytes) / (1024*1024) as avg_file_mb,
  MIN(file_size_in_bytes) / (1024*1024) as min_file_mb,
  MAX(file_size_in_bytes) / (1024*1024) as max_file_mb
FROM (
  DESCRIBE DETAIL delta.`s3://feature-store/realtime/`
)
""").show()

# Step 2: TODO - Run OPTIMIZE

# Step 3: TODO - Enable auto-optimize

# Step 4: TODO - Benchmark query before/after

# Step 5: TODO - Calculate cost savings
```

**Success Criteria:**
- Files: 500,000 → < 1,000 (500x reduction)
- Average file size: 2 MB → 800 MB (400x larger)
- Query time: 8 min → < 30 sec (16x improvement)
- Monthly savings: > $100 (reduced S3 API calls + faster compute)

**Stretch Goal:** Implement a monitoring dashboard that alerts when avg file size < 64 MB.

---

### Exercise 4: Bloom Filter Implementation (2-3 hours)

**Objective:** Accelerate point lookups using bloom filters.

**Scenario:** Feature store with 10 billion user records (5 TB). Point lookups by `user_id` currently scan entire dataset (query time: 2 minutes). Target: < 1 second.

**Tasks:**
1. Create Delta Lake table with bloom filter on `user_id`
2. Size bloom filter appropriately (10B items, 1% FPR)
3. Benchmark lookups before/after bloom filter
4. Measure:
   - Query time improvement
   - Bytes scanned reduction
   - Bloom filter storage overhead
5. Test with different false positive rates (1%, 5%, 10%)

**Starter Code:**
```python
# Generate sample data
df = spark.range(10_000_000_000).selectExpr(
    "id as user_id",
    "array(rand(), rand(), rand()) as features"
)

# Step 1: Write without bloom filter
df.write.format("delta").save("s3://features/no_bloom/")

# Step 2: TODO - Write with bloom filter

# Step 3: TODO - Benchmark lookup
# Query: user_id = 987654321

# Step 4: TODO - Measure bytes scanned

# Step 5: TODO - Test different FPR values
```

**Success Criteria:**
- Query time: 120 sec → < 1 sec (> 100x improvement)
- Bytes scanned: 5 TB → < 5 GB (> 1000x reduction)
- Bloom filter size: < 50 GB (< 1% of dataset)
- False positive rate: < 1%

**Stretch Goal:** Implement bloom filters on multiple columns (`user_id`, `session_id`) and test multi-column queries.

---

## 🔗 Related Concepts

### Within This Course

- **[01.01] The Role of Data Engineering in ML**: Understanding ML data requirements
- **[02.03] Feature Engineering and Transformation Pipelines**: Producing data that needs storage optimization
- **[03.02] Lambda vs Kappa Architectures**: Storage trade-offs in hybrid architectures
- **[04.01] Data Lakes, Warehouses, and Lakehouses**: High-level storage architecture context
- **[04.02] Object Storage (S3, GCS, Azure Blob)**: Cloud storage fundamentals
- **[04.03] Distributed File Systems**: HDFS, Alluxio as storage substrates
- **[04.05] Data Partitioning and Organization**: Logical organization for query optimization
- **[04.06] Cost Optimization Techniques**: Balancing performance and cloud spending

### External Technologies

**File Formats:**
- **Apache Parquet**: Columnar storage format (https://parquet.apache.org/)
- **Apache ORC**: Optimized row columnar format (https://orc.apache.org/)
- **Apache Avro**: Row-oriented schema evolution (https://avro.apache.org/)
- **Apache Arrow**: In-memory columnar format (https://arrow.apache.org/)

**Table Formats:**
- **Delta Lake**: ACID transactions on data lakes (https://delta.io/)
- **Apache Iceberg**: Table format for huge analytic datasets (https://iceberg.apache.org/)
- **Apache Hudi**: Incremental data processing (https://hudi.apache.org/)

**Compression:**
- **Zstandard**: Fast compression by Facebook (https://facebook.github.io/zstd/)
- **LZ4**: Extremely fast compression (https://lz4.github.io/lz4/)
- **Snappy**: Compression by Google (https://google.github.io/snappy/)

**Tools:**
- **Petastorm**: Parquet for PyTorch/TensorFlow (https://github.com/uber/petastorm)
- **TensorFlow I/O**: TF data connectors (https://www.tensorflow.org/io)
- **PyArrow**: Python interface for Arrow (https://arrow.apache.org/docs/python/)

### Academic Foundations

- **Dremel Paper** (2010): Foundations of columnar nested data (Google)
- **Parquet Encoding** (2013): Dictionary and run-length encoding
- **Delta Lake Paper** (2020): ACID transactions on data lakes
- **Iceberg Design** (2020): Table format for petabyte-scale analytics

---

## 📚 Further Reading

### Books

1. **"Designing Data-Intensive Applications" by Martin Kleppmann** (Chapter 3: Storage and Retrieval)
   - In-depth coverage of storage engines, indexing, and columnar storage
   - Trade-offs between OLTP and OLAP storage formats
   - Page: 89-139

2. **"Fundamentals of Data Engineering" by Joe Reis & Matt Housley** (Chapter 7: Storage)
   - Practical guidance on storage selection for data engineering
   - Cost-performance trade-offs in cloud storage
   - Page: 187-220

3. **"High Performance Spark" by Holden Karau & Rachel Warren** (Chapter 5: Loading and Saving)
   - Parquet optimization for Spark workloads
   - File size tuning and partitioning strategies
   - Page: 117-148

4. **"Learning Spark, 2nd Edition" by Jules Damji et al.** (Chapter 9: Data Sources)
   - Working with Parquet, ORC, Avro in Spark
   - Compression and encoding strategies
   - Page: 225-260

### Papers

1. **"Dremel: Interactive Analysis of Web-Scale Datasets"** (2010)
   - Google's paper on columnar storage for nested data
   - Foundations of Parquet's encoding schemes
   - https://research.google/pubs/pub36632/

2. **"Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores"** (2020)
   - ACID transactions on data lakes
   - Optimization techniques (Z-ORDER, compaction)
   - https://databricks.com/research/delta-lake-high-performance-acid-table-storage

3. **"Apache Iceberg: A Table Format for Huge Analytic Datasets"** (2020)
   - Scalable table format design
   - Hidden partitioning and schema evolution
   - https://iceberg.apache.org/

4. **"Optimizing Space Amplification in RocksDB"** (2017)
   - LSM-tree compaction strategies applicable to Delta Lake
   - Trade-offs in storage amplification
   - https://arxiv.org/abs/1708.00147

### Documentation

1. **Apache Parquet Documentation**
   - Format specification and encoding schemes
   - https://parquet.apache.org/docs/

2. **Delta Lake Optimization Guide**
   - Auto-optimize, Z-ORDER, bloom filters
   - https://docs.delta.io/latest/optimizations-oss.html

3. **AWS S3 Performance Best Practices**
   - Request rate optimization, multipart uploads
   - https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html

4. **PyArrow Parquet Documentation**
   - Reading/writing Parquet in Python
   - https://arrow.apache.org/docs/python/parquet.html

### Blog Posts

1. **"Parquet File Internals and Best Practices"** (Uber Engineering, 2018)
   - Real-world Parquet optimization at scale
   - https://eng.uber.com/parquet/

2. **"Optimizing Apache Spark for Machine Learning"** (Databricks, 2021)
   - Storage optimization for ML training pipelines
   - https://databricks.com/blog/2021/01/19/optimizing-apache-spark-for-machine-learning.html

3. **"Delta Lake Small File Compaction"** (Databricks, 2020)
   - Detailed guide to file consolidation strategies
   - https://databricks.com/blog/2020/09/29/diving-into-delta-lake-enforcing-and-evolving-the-schema.html

4. **"Benchmarking Compression Algorithms"** (Cloudflare, 2022)
   - Zstandard vs Gzip vs Brotli performance
   - https://blog.cloudflare.com/compression-benchmarks/

### Tutorials

1. **"Optimizing Parquet Files with Spark"** (Towards Data Science)
   - Step-by-step Parquet tuning guide
   - https://towardsdatascience.com/

2. **"Delta Lake Quickstart"** (delta.io)
   - Hands-on Delta Lake setup and optimization
   - https://docs.delta.io/latest/quick-start.html

3. **"PyArrow and Parquet Performance"** (Arrow Documentation)
   - Using PyArrow for high-performance Parquet I/O
   - https://arrow.apache.org/cookbook/py/io.html

### Tools and Libraries

1. **parquet-tools**: CLI for inspecting Parquet files
   - `pip install parquet-tools`
   - `parquet-tools meta <file>` to see row groups, encodings, statistics

2. **pyarrow**: Python library for Arrow and Parquet
   - `pip install pyarrow`
   - Read/write Parquet with full control over encoding and compression

3. **Delta Lake**: ACID table format
   - `pip install delta-spark`
   - https://delta.io/

4. **Petastorm**: Parquet for deep learning
   - `pip install petastorm`
   - Bridge between Spark and PyTorch/TensorFlow

---

## 📝 Key Takeaways

1. **Columnar storage (Parquet) is essential for ML workloads**: Enables column pruning, better compression (3-10x), and predicate pushdown. Use Parquet as the default format for training data and feature stores.

2. **Compression choice depends on read/write patterns**: Snappy for frequent reads (training), Zstandard for balanced workloads (feature stores), Gzip for archival. Wrong choice can cost 4x read performance or 60% in storage costs.

3. **Small files problem kills performance**: 10,000 × 1 MB files are 10-50x slower than 10 × 1 GB files. Use `repartition()`, auto-optimize, and scheduled compaction to maintain 128 MB - 1 GB file sizes.

4. **Row group sizing affects query performance**: Larger row groups (256 MB - 1 GB) improve compression but reduce predicate pushdown granularity. Align row group size with query patterns and partition boundaries.

5. **Bloom filters accelerate point lookups**: Convert `SELECT WHERE user_id = X` from 2 minutes (full scan) to 1 second (99.9% data skipping). Essential for high-cardinality columns in large datasets.

6. **Caching prevents redundant I/O**: Cache preprocessed training data and validation sets to avoid repeated ETL. Can reduce training I/O by 10-100x and save hundreds of dollars per month.

7. **Dictionary encoding compresses categorical features**: Automatically applied by Parquet for low-cardinality columns (< 40K unique values). Provides 5-20x compression for categories, user segments, and product IDs.

8. **Statistics enable data skipping**: Min/max values, null counts, and bloom filters allow query engines to skip irrelevant row groups. Delta Lake/Iceberg collect statistics automatically; Parquet requires manual configuration.

9. **File consolidation is mandatory for streaming**: Streaming writes create millions of small files without auto-compaction. Enable Delta Lake auto-optimize or run daily OPTIMIZE jobs to maintain performance.

10. **Profile before optimizing**: Measure actual read/write times, compression ratios, and bytes scanned before making changes. A/B test different formats and compressions on sample data before production deployment.

---

## ✏️ Notes Section

**Personal Insights:**

---

**Questions:**

---

**Action Items:**

---

**Related Projects:**

---

**Code Snippets:**

---

**Further Exploration:**

---

**📌 Tags:** `#data-storage` `#parquet` `#compression` `#optimization` `#ml-systems` `#file-formats` `#delta-lake` `#bloom-filters` `#small-files` `#caching`

---

**Navigation:**
- ← Previous: [04.03 - Distributed File Systems](03.%20Distributed%20File%20Systems.md)
- → Next: [04.05 - Data Partitioning and Organization](05.%20Data%20Partitioning%20and%20Organization.md)
- ↑ Up: [Chapter 04 - Data Storage for ML](README.md)
- ⌂ Home: [DEforAI Course Index](../../README.md)

---

*Last updated: 2025-10-19*
