# 03. Distributed File Systems

**Date:** 2025-10-19
**Status:** #research
**Tags:** #data-engineering #ml-systems #distributed-file-systems #hdfs #alluxio #caching #storage

---

## üìã Overview

Distributed file systems (DFS) provide POSIX-like file operations across clusters of machines, enabling parallel data access for distributed ML training and processing. While object storage (S3, GCS) dominates cloud-native ML platforms, distributed file systems remain critical for specific use cases requiring high-throughput sequential I/O, data locality, and POSIX semantics.

**Key Distributed File Systems**:
- **HDFS** (Hadoop Distributed File System): Battle-tested for big data, foundational to Hadoop ecosystem
- **Alluxio** (formerly Tachyon): Memory-speed data orchestration layer, caches data from object storage
- **Lustre**: High-performance parallel file system for HPC and ML supercomputers
- **CephFS**: Software-defined storage with object, block, and file interfaces
- **JuiceFS**: Cloud-native file system built on object storage

**When to Use Distributed File Systems**:
- **High-throughput sequential I/O**: Reading large files (TB+) for distributed training
- **Data locality**: Co-locate compute and storage (HDFS on Spark nodes)
- **POSIX semantics**: Applications requiring file system operations (random writes, appends)
- **Caching hot data**: Alluxio caches frequently accessed datasets in memory
- **HPC workloads**: Multi-node training (Horovod, DeepSpeed) with shared file system

**Object Storage vs Distributed File Systems**:

| **Aspect** | **Object Storage (S3)** | **Distributed File System (HDFS)** |
|-----------|------------------------|-----------------------------------|
| **Interface** | HTTP REST API | POSIX (open, read, write) |
| **Scalability** | Unlimited (EB+) | Limited by cluster (PB) |
| **Throughput** | Moderate (GB/sec) | High (10+ GB/sec) |
| **Latency** | 100-200ms first byte | 1-10ms |
| **Cost** | $0.023/GB/month | $0.10+/GB (compute + storage) |
| **Data Locality** | None (remote) | Yes (co-located) |
| **Metadata** | Eventual consistency | Strong consistency |
| **Best For** | Data lakes, archives | Batch processing, training |

This subchapter covers HDFS architecture, Alluxio for caching, and modern alternatives for ML workloads.

---

## üéØ Learning Objectives

By the end of this subchapter, you will:

1. **Understand HDFS architecture** including NameNode, DataNode, and block replication
2. **Configure HDFS for ML workloads** with appropriate block sizes and replication factors
3. **Implement Alluxio** as a caching layer between object storage and compute
4. **Optimize data locality** for Spark/Hadoop jobs to maximize throughput
5. **Compare HDFS vs object storage** and select appropriate storage for different ML use cases
6. **Deploy JuiceFS** as a POSIX interface on top of S3/GCS
7. **Monitor HDFS performance** with NameNode metrics and DataNode health
8. **Implement HDFS federation** for scaling beyond single namespace limits
9. **Configure Alluxio tiered storage** (memory, SSD, HDD) for performance optimization
10. **Troubleshoot common issues** like block under-replication and NameNode failures

---

## üìö Core Concepts

### 1. HDFS Architecture

**HDFS (Hadoop Distributed File System)** is a distributed file system designed for storing very large files across clusters of commodity hardware.

**Core Components**:

```
HDFS Architecture:
‚îú‚îÄ‚îÄ NameNode (Master)
‚îÇ   ‚îú‚îÄ‚îÄ Metadata (file ‚Üí blocks mapping)
‚îÇ   ‚îú‚îÄ‚îÄ Namespace (directory tree)
‚îÇ   ‚îú‚îÄ‚îÄ Block locations (block ‚Üí DataNodes)
‚îÇ   ‚îî‚îÄ‚îÄ Edit log (transaction log)
‚îú‚îÄ‚îÄ DataNodes (Workers)
‚îÇ   ‚îú‚îÄ‚îÄ Store actual data blocks
‚îÇ   ‚îú‚îÄ‚îÄ Send heartbeats to NameNode
‚îÇ   ‚îî‚îÄ‚îÄ Report block health
‚îî‚îÄ‚îÄ Secondary NameNode
    ‚îî‚îÄ‚îÄ Checkpoint NameNode state
```

**How HDFS Works**:

1. **File Storage**:
   - Files split into blocks (default 128 MB)
   - Blocks replicated across DataNodes (default 3 replicas)
   - NameNode maintains metadata, DataNodes store blocks

2. **Write Operation**:
```
Client ‚Üí NameNode: "Create /ml-data/training.parquet"
NameNode ‚Üí Client: "Write to DataNode1, DataNode2, DataNode3"
Client ‚Üí DataNode1: Write block 1
DataNode1 ‚Üí DataNode2 ‚Üí DataNode3: Pipeline replication
Client ‚Üí NameNode: "Block 1 complete"
```

3. **Read Operation**:
```
Client ‚Üí NameNode: "Read /ml-data/training.parquet"
NameNode ‚Üí Client: "Block 1 on DataNode1,2,3; Block 2 on DataNode4,5,6"
Client ‚Üí DataNode1: Read block 1 (closest DataNode)
Client ‚Üí DataNode4: Read block 2
```

**Key Features**:
- **Block-based storage**: Large files split into blocks (128-256 MB)
- **Replication**: Data replicated across nodes for fault tolerance
- **Data locality**: Computation scheduled on nodes with data
- **Write-once-read-many**: Optimized for append-only workloads
- **Rack awareness**: Replicas placed on different racks for availability

**HDFS Configuration for ML**:
```xml
<!-- hdfs-site.xml -->
<configuration>
    <!-- Block size: 256 MB for large ML datasets -->
    <property>
        <name>dfs.blocksize</name>
        <value>268435456</value> <!-- 256 MB -->
    </property>

    <!-- Replication factor: 3 for production -->
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>

    <!-- NameNode memory: 4 GB for 100M files -->
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>100</value>
    </property>
</configuration>
```

---

### 2. Alluxio (Memory-Speed Data Orchestration)

**Alluxio** is a data orchestration layer that provides memory-speed access to data stored in slower systems (S3, HDFS, GCS).

**Architecture**:
```
ML Training (Spark/PyTorch)
       ‚Üì
   Alluxio (Memory Cache)
       ‚Üì
Under Storage (S3, HDFS, GCS)
```

**How Alluxio Works**:
1. Client requests file from Alluxio
2. If cached in memory ‚Üí serve immediately (GB/sec throughput)
3. If not cached ‚Üí fetch from under storage (S3), cache in memory, serve
4. Subsequent requests served from cache

**Benefits for ML**:
- **100x faster than S3**: Memory-speed access (10+ GB/sec vs 100 MB/sec)
- **Reduced S3 costs**: Cache frequently accessed data, reduce S3 GET requests
- **Unified namespace**: Single view of data across S3, HDFS, GCS
- **Data locality**: Co-locate cache with compute for maximum throughput

**Tiered Storage**:
```
Alluxio Tiered Storage:
‚îú‚îÄ‚îÄ Memory (fastest, expensive)
‚îÇ   ‚îî‚îÄ‚îÄ Hot data: active training datasets
‚îú‚îÄ‚îÄ SSD (fast, moderate cost)
‚îÇ   ‚îî‚îÄ‚îÄ Warm data: recent datasets
‚îî‚îÄ‚îÄ HDD (slow, cheap)
    ‚îî‚îÄ‚îÄ Cold data: archived datasets
```

**Example Configuration**:
```properties
# alluxio-site.properties

# Tiered storage levels
alluxio.worker.tieredstore.levels=3

# Level 0: Memory (32 GB)
alluxio.worker.tieredstore.level0.alias=MEM
alluxio.worker.tieredstore.level0.dirs.path=/dev/shm
alluxio.worker.tieredstore.level0.dirs.quota=32GB

# Level 1: SSD (256 GB)
alluxio.worker.tieredstore.level1.alias=SSD
alluxio.worker.tieredstore.level1.dirs.path=/mnt/ssd
alluxio.worker.tieredstore.level1.dirs.quota=256GB

# Level 2: HDD (1 TB)
alluxio.worker.tieredstore.level2.alias=HDD
alluxio.worker.tieredstore.level2.dirs.path=/mnt/hdd
alluxio.worker.tieredstore.level2.dirs.quota=1TB
```

---

### 3. Data Locality and Performance

**Data Locality**: Co-locating computation with data to minimize network transfer.

**Locality Levels** (Spark on HDFS):
1. **PROCESS_LOCAL**: Data in same JVM (cache)
2. **NODE_LOCAL**: Data on same node (HDFS block)
3. **RACK_LOCAL**: Data on same rack
4. **ANY**: Data on different rack (network transfer)

**Example**:
```
NODE_LOCAL (best):
Spark Task on Node1 ‚Üí HDFS Block on Node1 (local disk read, ~200 MB/sec)

RACK_LOCAL (moderate):
Spark Task on Node1 ‚Üí HDFS Block on Node2 (same rack, ~100 MB/sec)

ANY (worst):
Spark Task on Node1 ‚Üí HDFS Block on Node10 (different rack, ~50 MB/sec)
```

**Optimizing Data Locality**:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MLTraining") \
    .config("spark.locality.wait", "3s") \
    .getOrCreate()

# Read from HDFS (data locality automatically utilized)
df = spark.read.parquet("hdfs:///ml-data/training/")

# Check locality
df.rdd.getNumPartitions()  # Should match HDFS block count
df.rdd.glom().map(lambda x: len(list(x))).collect()  # Verify balanced partitions
```

---

### 4. HDFS vs Object Storage Trade-offs

**When to Use HDFS**:
- **Co-located compute and storage**: Spark/Hadoop cluster with local HDFS
- **High-throughput sequential I/O**: Reading large files (TB+) for training
- **Data locality**: Maximize throughput by reading local blocks
- **Low-latency metadata**: Strong consistency for file operations

**When to Use Object Storage (S3/GCS)**:
- **Cloud-native**: AWS, GCP, Azure environments
- **Scalability**: Unlimited storage without managing clusters
- **Cost**: 10x cheaper storage ($0.023/GB vs $0.10+/GB for HDFS)
- **Separation of compute/storage**: Ephemeral Spark clusters

**Migration Path**: HDFS ‚Üí Object Storage + Alluxio
```
Before: Spark + HDFS (co-located)
After: Spark (compute only) + S3 (storage) + Alluxio (cache)

Benefits:
- Lower cost (S3 storage vs HDFS)
- Elastic compute (scale Spark independently)
- Alluxio cache maintains high throughput
```

---

### 5. JuiceFS (Cloud-Native POSIX on Object Storage)

**JuiceFS** provides a POSIX-compliant file system interface on top of object storage (S3, GCS, Azure Blob).

**Architecture**:
```
Application (POSIX operations)
       ‚Üì
   JuiceFS Client
       ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ         ‚îÇ            ‚îÇ
Metadata DB  Object Store
(Redis/SQL) (S3/GCS)
```

**How JuiceFS Works**:
1. File metadata stored in Redis or SQL database
2. File data stored in object storage (S3, GCS)
3. Client translates POSIX operations to metadata + object storage calls

**Benefits**:
- **POSIX interface**: Existing applications work without code changes
- **Cloud-native**: Leverages cheap object storage
- **Distributed cache**: Local cache for frequently accessed files
- **Strong consistency**: Unlike S3, provides POSIX consistency

**Use Cases for ML**:
- Mount S3 as local file system for training
- Share datasets across multiple training nodes
- POSIX-compatible tools (rsync, ls, cp) on object storage

**Example Setup**:
```bash
# Install JuiceFS
curl -sSL https://d.juicefs.com/install | sh

# Format file system (create metadata schema)
juicefs format \
    --storage s3 \
    --bucket https://my-ml-bucket.s3.amazonaws.com \
    redis://localhost:6379/1 \
    ml-storage

# Mount file system
juicefs mount redis://localhost:6379/1 /mnt/jfs

# Use as regular file system
cp training_data.parquet /mnt/jfs/datasets/
ls -lh /mnt/jfs/datasets/
```

---

### 6. Lustre (High-Performance Parallel File System)

**Lustre** is a high-performance parallel file system used in HPC and large-scale ML training (supercomputers).

**Architecture**:
```
Lustre File System:
‚îú‚îÄ‚îÄ MDT (Metadata Target): File metadata
‚îú‚îÄ‚îÄ OST (Object Storage Target): File data (striped)
‚îî‚îÄ‚îÄ Clients: Compute nodes
```

**Performance**:
- **Throughput**: 100+ GB/sec (large clusters)
- **Striping**: Files striped across multiple OSTs for parallel I/O
- **Used by**: NVIDIA DGX SuperPOD, AWS FSx for Lustre

**Use Cases**:
- Large-scale distributed training (multi-node GPU clusters)
- Checkpointing for large models (100+ GB checkpoints)
- Reading training data at 10+ GB/sec

**AWS FSx for Lustre**:
```bash
# Create Lustre file system linked to S3
aws fsx create-file-system \
    --file-system-type LUSTRE \
    --storage-capacity 1200 \
    --lustre-configuration \
        DataRepositoryConfiguration={
            ImportPath=s3://my-ml-bucket/training/,
            ExportPath=s3://my-ml-bucket/output/
        }

# Mount on EC2 instances
sudo mount -t lustre fs-0123456789abcdef.fsx.us-west-2.amazonaws.com@tcp:/fsx /mnt/fsx
```

---

### 7. HDFS Federation

**Problem**: Single NameNode limits scalability (metadata bottleneck).

**Solution**: HDFS Federation - multiple independent NameNodes managing separate namespaces.

**Architecture**:
```
HDFS Federation:
‚îú‚îÄ‚îÄ NameNode1: /ml-data/images/
‚îú‚îÄ‚îÄ NameNode2: /ml-data/text/
‚îú‚îÄ‚îÄ NameNode3: /ml-data/tabular/
‚îî‚îÄ‚îÄ Shared DataNodes
```

**Benefits**:
- Scale beyond single NameNode limits (100M+ files per NameNode)
- Isolate workloads (images, text, tabular)
- Independent failure domains

**Configuration**:
```xml
<!-- hdfs-site.xml -->
<configuration>
    <!-- Federation: Multiple NameNodes -->
    <property>
        <name>dfs.nameservices</name>
        <value>ns1,ns2,ns3</value>
    </property>

    <!-- NameNode1 for ns1 -->
    <property>
        <name>dfs.namenode.rpc-address.ns1</name>
        <value>namenode1.example.com:8020</value>
    </property>

    <!-- NameNode2 for ns2 -->
    <property>
        <name>dfs.namenode.rpc-address.ns2</name>
        <value>namenode2.example.com:8020</value>
    </property>
</configuration>
```

---

### 8. Performance Monitoring

**HDFS Metrics to Monitor**:

1. **NameNode Metrics**:
   - Files and blocks count
   - Heap memory usage
   - RPC queue length (request backlog)
   - Block under-replication count

2. **DataNode Metrics**:
   - Disk utilization
   - Network throughput (bytes read/written)
   - Failed volumes
   - Heartbeat latency

**Example Monitoring** (Prometheus + Grafana):
```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'hdfs-namenode'
    static_configs:
      - targets: ['namenode:9870']
    metrics_path: '/jmx'
    params:
      qry: ['Hadoop:service=NameNode,name=FSNamesystem']

  - job_name: 'hdfs-datanode'
    static_configs:
      - targets: ['datanode1:9864', 'datanode2:9864']
```

**Key Alerts**:
- Under-replicated blocks > 100
- NameNode heap usage > 80%
- DataNode disk usage > 90%
- Missing heartbeats > 3

---

### 9. HDFS High Availability

**Problem**: NameNode is single point of failure.

**Solution**: NameNode High Availability with automatic failover.

**Architecture**:
```
Active NameNode ‚Üê‚Üí Standby NameNode
       ‚Üì                    ‚Üì
   Shared Edit Log (JournalNodes or NFS)
       ‚Üì
   DataNodes (report to both)
```

**How It Works**:
1. Active NameNode writes edits to shared journal
2. Standby NameNode reads edits, maintains synchronized state
3. If Active fails, Standby promoted to Active (automatic with Zookeeper)

**Configuration**:
```xml
<!-- hdfs-site.xml -->
<configuration>
    <!-- Enable HA -->
    <property>
        <name>dfs.nameservices</name>
        <value>mycluster</value>
    </property>

    <!-- NameNode IDs -->
    <property>
        <name>dfs.ha.namenodes.mycluster</name>
        <value>nn1,nn2</value>
    </property>

    <!-- Active NameNode -->
    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn1</name>
        <value>namenode1:8020</value>
    </property>

    <!-- Standby NameNode -->
    <property>
        <name>dfs.namenode.rpc-address.mycluster.nn2</name>
        <value>namenode2:8020</value>
    </property>

    <!-- Shared edits directory (JournalNodes) -->
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://jn1:8485;jn2:8485;jn3:8485/mycluster</value>
    </property>

    <!-- Automatic failover -->
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
</configuration>
```

---

### 10. Cloud-Native Alternatives

**Emerging Patterns** (2024-2025):

1. **S3 + Alluxio**: Object storage with memory cache
   - Cost: S3 storage prices + Alluxio compute
   - Performance: Near-HDFS throughput with S3 scalability

2. **JuiceFS**: POSIX on S3
   - Cost: S3 storage + small metadata DB
   - Use case: Legacy apps requiring POSIX

3. **AWS FSx for Lustre**: Managed Lustre linked to S3
   - Cost: $0.14/GB/month (scratch), $0.145/GB/month (persistent)
   - Use case: HPC, large-scale distributed training

4. **Databricks DBFS**: Abstraction over S3/Azure Blob
   - Cost: Included with Databricks
   - Use case: Databricks-native workflows

**Recommendation for ML (2025)**:
- **Default**: S3/GCS + Alluxio for caching
- **Legacy Hadoop**: Migrate HDFS ‚Üí S3 with Alluxio
- **HPC/Supercomputing**: Lustre (FSx for Lustre on AWS)
- **POSIX required**: JuiceFS on S3

---

## üí° Practical Examples

### Example 1: Setting Up Alluxio with S3 for ML Training

**Scenario**: Speed up PyTorch training by caching S3 data in Alluxio memory.

**Implementation**:

```bash
# 1. Download and install Alluxio
wget https://downloads.alluxio.io/downloads/files/2.9.0/alluxio-2.9.0-bin.tar.gz
tar -xzf alluxio-2.9.0-bin.tar.gz
cd alluxio-2.9.0

# 2. Configure Alluxio
cat > conf/alluxio-site.properties <<EOF
# Under storage: S3
alluxio.master.mount.table.root.ufs=s3://my-ml-bucket/

# AWS credentials
aws.accessKeyId=AKIAIOSFODNN7EXAMPLE
aws.secretKey=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY

# Tiered storage: 32 GB memory
alluxio.worker.tieredstore.levels=1
alluxio.worker.tieredstore.level0.alias=MEM
alluxio.worker.tieredstore.level0.dirs.path=/dev/shm
alluxio.worker.tieredstore.level0.dirs.quota=32GB
EOF

# 3. Format and start Alluxio
./bin/alluxio format
./bin/alluxio-start.sh local SudoMount

# 4. Verify Alluxio is running
./bin/alluxio fs ls /

# 5. Pre-load training data into cache
./bin/alluxio fs load /training/images/

# 6. Check cache status
./bin/alluxio fs stat /training/images/
# Output: In Memory: 100%
```

**PyTorch Integration**:
```python
import torch
from torch.utils.data import Dataset, DataLoader
from alluxio import Alluxio

class AlluxioImageDataset(Dataset):
    def __init__(self, alluxio_path):
        # Connect to Alluxio
        self.alluxio = Alluxio("localhost", 19998)
        self.alluxio_path = alluxio_path

        # List all images
        self.image_paths = self.alluxio.listdir(alluxio_path)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Read image from Alluxio (memory-speed if cached)
        image_path = self.image_paths[idx]
        image_bytes = self.alluxio.read(image_path)

        # Parse image
        from PIL import Image
        import io
        image = Image.open(io.BytesIO(image_bytes))

        return image

# Usage
dataset = AlluxioImageDataset("/training/images/")
dataloader = DataLoader(dataset, batch_size=32, num_workers=4)

# Training loop - reads from Alluxio (10+ GB/sec) instead of S3 (100 MB/sec)
for epoch in range(10):
    for images in dataloader:
        # Training logic
        pass
```

**Performance Comparison**:
```
S3 Direct:           100 MB/sec (network-bound)
HDFS:                500 MB/sec (local disk)
Alluxio (Memory):    10+ GB/sec (memory-speed)

Training time (10 GB dataset):
S3: 100 seconds
HDFS: 20 seconds
Alluxio: 1 second (after initial cache)
```

---

### Example 2: Migrating HDFS to S3 with Data Locality

**Scenario**: Migrate on-premise Hadoop cluster (HDFS) to cloud (S3) while maintaining performance.

**Migration Steps**:

```bash
# 1. Copy HDFS data to S3 using DistCp
hadoop distcp \
    hdfs:///ml-data/training/ \
    s3a://my-ml-bucket/training/

# 2. Verify data integrity
hadoop fs -ls s3a://my-ml-bucket/training/
hdfs dfs -count -q hdfs:///ml-data/training/
hdfs dfs -count -q s3a://my-ml-bucket/training/

# 3. Update Spark jobs to read from S3
# Before:
df = spark.read.parquet("hdfs:///ml-data/training/")

# After:
df = spark.read.parquet("s3a://my-ml-bucket/training/")

# 4. Deploy Alluxio for caching (optional, for performance)
# See Example 1 above

# 5. Benchmark performance
from pyspark.sql import functions as F
import time

# HDFS baseline
start = time.time()
df_hdfs = spark.read.parquet("hdfs:///ml-data/training/")
count_hdfs = df_hdfs.count()
hdfs_time = time.time() - start

# S3 (no cache)
start = time.time()
df_s3 = spark.read.parquet("s3a://my-ml-bucket/training/")
count_s3 = df_s3.count()
s3_time = time.time() - start

# S3 + Alluxio (cached)
start = time.time()
df_alluxio = spark.read.parquet("alluxio://localhost:19998/training/")
count_alluxio = df_alluxio.count()
alluxio_time = time.time() - start

print(f"HDFS: {hdfs_time:.2f}s")
print(f"S3: {s3_time:.2f}s ({s3_time/hdfs_time:.2f}x slower)")
print(f"S3 + Alluxio: {alluxio_time:.2f}s ({alluxio_time/hdfs_time:.2f}x)")

# Expected output:
# HDFS: 10.5s
# S3: 52.3s (5.0x slower)
# S3 + Alluxio: 11.2s (1.1x - nearly same as HDFS)
```

**Cost Analysis**:
```python
# HDFS (on-premise): 100 TB
# - Storage: 100 TB √ó $0.10/GB/month = $10,000/month
# - Compute: 20 nodes √ó $500/month = $10,000/month
# Total: $20,000/month

# S3 + Alluxio (cloud):
# - Storage: 100 TB √ó $0.023/GB/month = $2,300/month
# - Alluxio cache (5 TB SSD): 5 TB √ó $0.08/GB/month = $400/month
# - Compute (Spark on EMR, on-demand): $5,000/month
# Total: $7,700/month

# Savings: $12,300/month (62%)
```

---

### Example 3: JuiceFS for POSIX Access to S3

**Scenario**: Mount S3 bucket as local file system for legacy ML tools requiring POSIX.

**Implementation**:

```bash
# 1. Install JuiceFS
curl -sSL https://d.juicefs.com/install | sh

# 2. Start Redis for metadata
docker run -d --name redis -p 6379:6379 redis

# 3. Format JuiceFS file system
juicefs format \
    --storage s3 \
    --bucket https://my-ml-bucket.s3.us-west-2.amazonaws.com \
    --access-key AKIAIOSFODNN7EXAMPLE \
    --secret-key wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \
    redis://localhost:6379/1 \
    ml-training

# 4. Mount JuiceFS
mkdir /mnt/jfs
juicefs mount redis://localhost:6379/1 /mnt/jfs &

# 5. Use as regular file system
# Copy training data
cp -r /local/training_data /mnt/jfs/datasets/

# List files (POSIX ls command)
ls -lh /mnt/jfs/datasets/

# Use with ML tools (rsync, tar, etc.)
rsync -avz /local/models/ /mnt/jfs/models/

# 6. Configure distributed cache (for multi-node training)
juicefs mount \
    --cache-dir /mnt/ssd/jfs-cache \
    --cache-size 102400 \  # 100 GB cache
    redis://localhost:6379/1 /mnt/jfs

# 7. Monitor cache performance
juicefs stats /mnt/jfs
# Output:
# Cache hit rate: 95.3%
# Read throughput: 2.5 GB/s
```

**PyTorch with JuiceFS**:
```python
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os

class JuiceFSImageDataset(Dataset):
    def __init__(self, root_dir):
        # JuiceFS mounted at /mnt/jfs, backed by S3
        self.root_dir = root_dir
        self.image_paths = []

        # Use standard os.walk (POSIX interface)
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith(('.jpg', '.png')):
                    self.image_paths.append(os.path.join(root, file))

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        # Use standard file operations (POSIX)
        image_path = self.image_paths[idx]
        image = Image.open(image_path)  # JuiceFS transparently fetches from S3
        return image

# Usage - no code changes from local file system!
dataset = JuiceFSImageDataset('/mnt/jfs/datasets/imagenet/')
dataloader = DataLoader(dataset, batch_size=32, num_workers=4)

for images in dataloader:
    # Training logic
    pass
```

---

## üîß Code Examples

### Code Example 1: HDFS Performance Tuning

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Configure Spark for optimal HDFS access
spark = SparkSession.builder \
    .appName("HDFS-Optimized") \
    .config("spark.hadoop.dfs.blocksize", "268435456") \
    .config("spark.hadoop.dfs.replication", "3") \
    .config("spark.locality.wait", "3s") \
    .config("spark.sql.files.maxPartitionBytes", "268435456") \
    .config("spark.speculation", "true") \
    .config("spark.speculation.multiplier", "1.5") \
    .getOrCreate()

# Read from HDFS with optimal settings
df = spark.read \
    .option("mergeSchema", "false") \
    .option("compression", "snappy") \
    .parquet("hdfs:///ml-data/training/")

# Check partitioning (should align with HDFS blocks)
print(f"Partitions: {df.rdd.getNumPartitions()}")
print(f"Rows per partition: {df.rdd.glom().map(len).collect()}")

# Repartition if needed (to match HDFS block count)
hdfs_blocks = 100  # Get from: hdfs fsck /ml-data/training/ -files -blocks
df_repartitioned = df.repartition(hdfs_blocks)

# Write with optimal HDFS settings
df_repartitioned.write \
    .mode("overwrite") \
    .option("compression", "snappy") \
    .option("maxRecordsPerFile", 1000000) \
    .parquet("hdfs:///ml-data/output/")

# Monitor locality
spark.sparkContext.setLogLevel("INFO")
# Check Spark UI: Locality Level (should be NODE_LOCAL or PROCESS_LOCAL)
```

---

### Code Example 2: Alluxio Cache Warming

```python
import subprocess
import time

class AlluxioCacheManager:
    """
    Manage Alluxio cache warming for ML training datasets
    """

    def __init__(self, alluxio_cli="/opt/alluxio/bin/alluxio"):
        self.alluxio_cli = alluxio_cli

    def preload_dataset(self, alluxio_path, recursive=True):
        """
        Pre-load dataset into Alluxio cache before training
        """
        print(f"Pre-loading {alluxio_path} into Alluxio cache...")

        cmd = [self.alluxio_cli, "fs", "load", alluxio_path]
        if recursive:
            cmd.append("-r")

        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode == 0:
            print(f"Successfully loaded {alluxio_path}")
        else:
            print(f"Error loading {alluxio_path}: {result.stderr}")

    def get_cache_status(self, alluxio_path):
        """
        Check cache hit rate for a path
        """
        cmd = [self.alluxio_cli, "fs", "stat", alluxio_path]
        result = subprocess.run(cmd, capture_output=True, text=True)

        # Parse output for "In Memory: X%"
        for line in result.stdout.split('\n'):
            if "In Memory" in line:
                return line.strip()

        return "Cache status unknown"

    def evict_from_cache(self, alluxio_path):
        """
        Evict data from cache (free memory)
        """
        cmd = [self.alluxio_cli, "fs", "free", alluxio_path]
        subprocess.run(cmd)

    def pin_to_cache(self, alluxio_path):
        """
        Pin data in cache (prevent eviction)
        """
        cmd = [self.alluxio_cli, "fs", "pin", alluxio_path]
        subprocess.run(cmd)

    def benchmark_read_performance(self, alluxio_path, num_reads=10):
        """
        Benchmark read performance (cached vs uncached)
        """
        # First read (uncached)
        self.evict_from_cache(alluxio_path)

        start = time.time()
        cmd = [self.alluxio_cli, "fs", "cat", alluxio_path]
        subprocess.run(cmd, capture_output=True)
        uncached_time = time.time() - start

        # Second read (cached)
        start = time.time()
        subprocess.run(cmd, capture_output=True)
        cached_time = time.time() - start

        speedup = uncached_time / cached_time

        return {
            "uncached_time": uncached_time,
            "cached_time": cached_time,
            "speedup": speedup
        }

# Usage
cache_manager = AlluxioCacheManager()

# Pre-load training data before training starts
cache_manager.preload_dataset("/training/images/", recursive=True)

# Check cache status
status = cache_manager.get_cache_status("/training/images/")
print(status)  # "In Memory: 100%"

# Pin critical data to prevent eviction
cache_manager.pin_to_cache("/training/images/")

# Benchmark
benchmark = cache_manager.benchmark_read_performance("/training/sample.parquet")
print(f"Speedup: {benchmark['speedup']:.2f}x")
```

---

### Code Example 3: HDFS Health Monitoring

```python
import subprocess
import json
import requests

class HDFSHealthMonitor:
    """
    Monitor HDFS cluster health
    """

    def __init__(self, namenode_host="localhost", namenode_port=9870):
        self.namenode_url = f"http://{namenode_host}:{namenode_port}"
        self.hdfs_cli = "hdfs"

    def get_cluster_stats(self):
        """
        Get HDFS cluster statistics via JMX
        """
        jmx_url = f"{self.namenode_url}/jmx?qry=Hadoop:service=NameNode,name=FSNamesystem"
        response = requests.get(jmx_url)
        data = response.json()

        beans = data['beans'][0]

        return {
            "total_files": beans.get("FilesTotal", 0),
            "total_blocks": beans.get("BlocksTotal", 0),
            "missing_blocks": beans.get("MissingBlocks", 0),
            "under_replicated_blocks": beans.get("UnderReplicatedBlocks", 0),
            "capacity_used_percent": beans.get("PercentUsed", 0),
            "total_capacity_gb": beans.get("CapacityTotal", 0) / (1024**3),
            "used_capacity_gb": beans.get("CapacityUsed", 0) / (1024**3),
            "live_nodes": beans.get("NumLiveDataNodes", 0),
            "dead_nodes": beans.get("NumDeadDataNodes", 0)
        }

    def check_file_health(self, hdfs_path):
        """
        Check health of specific file/directory
        """
        cmd = [self.hdfs_cli, "fsck", hdfs_path, "-files", "-blocks"]
        result = subprocess.run(cmd, capture_output=True, text=True)

        # Parse fsck output
        health_status = {
            "path": hdfs_path,
            "status": "HEALTHY",
            "issues": []
        }

        for line in result.stdout.split('\n'):
            if "CORRUPT" in line:
                health_status["status"] = "CORRUPT"
                health_status["issues"].append(line)
            elif "UNDER REPLICATED" in line:
                health_status["status"] = "UNDER_REPLICATED"
                health_status["issues"].append(line)
            elif "MISSING" in line:
                health_status["status"] = "MISSING_BLOCKS"
                health_status["issues"].append(line)

        return health_status

    def get_datanode_health(self):
        """
        Get health of all DataNodes
        """
        jmx_url = f"{self.namenode_url}/jmx?qry=Hadoop:service=NameNode,name=FSNamesystemState"
        response = requests.get(jmx_url)
        data = response.json()

        live_nodes = json.loads(data['beans'][0].get("LiveNodes", "{}"))
        dead_nodes = json.loads(data['beans'][0].get("DeadNodes", "{}"))

        datanode_health = []

        for node_name, node_info in live_nodes.items():
            datanode_health.append({
                "name": node_name,
                "status": "LIVE",
                "capacity_gb": node_info.get("capacity", 0) / (1024**3),
                "used_gb": node_info.get("used", 0) / (1024**3),
                "used_percent": node_info.get("usedSpace", 0) / node_info.get("capacity", 1) * 100,
                "num_blocks": node_info.get("numBlocks", 0)
            })

        for node_name, node_info in dead_nodes.items():
            datanode_health.append({
                "name": node_name,
                "status": "DEAD",
                "last_contact": node_info.get("lastContact", "unknown")
            })

        return datanode_health

    def generate_health_report(self):
        """
        Generate comprehensive health report
        """
        cluster_stats = self.get_cluster_stats()
        datanode_health = self.get_datanode_health()

        report = f"""
HDFS Cluster Health Report
{'='*50}

Cluster Statistics:
- Total Files: {cluster_stats['total_files']:,}
- Total Blocks: {cluster_stats['total_blocks']:,}
- Missing Blocks: {cluster_stats['missing_blocks']}
- Under-Replicated Blocks: {cluster_stats['under_replicated_blocks']}
- Capacity Used: {cluster_stats['used_capacity_gb']:.2f} GB / {cluster_stats['total_capacity_gb']:.2f} GB ({cluster_stats['capacity_used_percent']:.1f}%)
- Live DataNodes: {cluster_stats['live_nodes']}
- Dead DataNodes: {cluster_stats['dead_nodes']}

DataNode Health:
"""

        for node in datanode_health:
            if node['status'] == 'LIVE':
                report += f"- {node['name']}: {node['status']} | Used: {node['used_percent']:.1f}% | Blocks: {node['num_blocks']:,}\n"
            else:
                report += f"- {node['name']}: {node['status']} | Last Contact: {node['last_contact']}\n"

        # Health warnings
        report += "\n"
        if cluster_stats['missing_blocks'] > 0:
            report += f"‚ö†Ô∏è  WARNING: {cluster_stats['missing_blocks']} missing blocks detected!\n"
        if cluster_stats['under_replicated_blocks'] > 100:
            report += f"‚ö†Ô∏è  WARNING: {cluster_stats['under_replicated_blocks']} under-replicated blocks!\n"
        if cluster_stats['dead_nodes'] > 0:
            report += f"‚ö†Ô∏è  WARNING: {cluster_stats['dead_nodes']} dead DataNodes!\n"
        if cluster_stats['capacity_used_percent'] > 90:
            report += f"‚ö†Ô∏è  WARNING: Capacity usage at {cluster_stats['capacity_used_percent']:.1f}%!\n"

        return report

# Usage
monitor = HDFSHealthMonitor(namenode_host="namenode.example.com")

# Generate health report
report = monitor.generate_health_report()
print(report)

# Check specific file health
file_health = monitor.check_file_health("/ml-data/training/")
if file_health['status'] != 'HEALTHY':
    print(f"‚ö†Ô∏è  Issues detected: {file_health['issues']}")

# Continuous monitoring (run in background)
import time

while True:
    stats = monitor.get_cluster_stats()

    # Alert if critical issues
    if stats['missing_blocks'] > 0:
        print(f"üö® CRITICAL: {stats['missing_blocks']} missing blocks!")
        # Send alert (email, Slack, PagerDuty)

    if stats['under_replicated_blocks'] > 1000:
        print(f"‚ö†Ô∏è  WARNING: {stats['under_replicated_blocks']} under-replicated blocks")

    time.sleep(60)  # Check every minute
```

---

## ‚úÖ Best Practices

### 1. Use Alluxio for S3 Caching

**Principle**: Cache frequently accessed data in memory for 100x speedup.

**Recommendation**:
- Deploy Alluxio on Spark/training nodes
- Pre-load training datasets before training
- Pin critical data to prevent eviction

---

### 2. Optimize HDFS Block Size

**Principle**: Larger blocks reduce NameNode metadata overhead.

**Recommendation**:
- **128 MB**: Default, good for most use cases
- **256-512 MB**: Large ML datasets (TB+)
- **64 MB**: Small files (rare for ML)

---

### 3. Monitor Under-Replicated Blocks

**Principle**: Under-replication indicates DataNode failures or network issues.

**Action**:
- Alert if under-replicated blocks > 100
- Investigate DataNode health
- Manually re-replicate critical data

---

### 4. Enable HDFS HA for Production

**Principle**: Single NameNode is single point of failure.

**Recommendation**:
- Deploy Active-Standby NameNode pair
- Use JournalNodes for shared edits
- Enable automatic failover with Zookeeper

---

### 5. Migrate to Cloud-Native Storage

**Principle**: S3/GCS is cheaper and more scalable than HDFS.

**Migration Path**:
1. Copy HDFS ‚Üí S3 with DistCp
2. Deploy Alluxio for caching
3. Update Spark jobs to read from S3
4. Decommission HDFS cluster

---

### 6. Use JuiceFS for POSIX Compatibility

**Principle**: Provide POSIX interface on S3 for legacy tools.

**Use Cases**:
- rsync, tar, cp commands on S3
- Legacy ML frameworks requiring file system

---

### 7. Benchmark Before Migration

**Principle**: Validate performance before committing to migration.

**Benchmark**:
- HDFS baseline
- S3 direct
- S3 + Alluxio
- Compare to HDFS

---

### 8. Configure Tiered Storage

**Principle**: Optimize cost/performance with memory, SSD, HDD tiers.

**Recommendation (Alluxio)**:
- **Memory (hot)**: Active training datasets
- **SSD (warm)**: Recent datasets
- **HDD (cold)**: Archived datasets

---

### 9. Test Disaster Recovery

**Principle**: Validate HDFS HA failover and backup procedures.

**Testing**:
- Simulate NameNode failure
- Verify automatic failover
- Test restore from backup

---

### 10. Monitor Storage Costs

**Principle**: Track HDFS vs S3 costs to justify migration.

**Cost Tracking**:
- HDFS: Compute + storage (bundled)
- S3: Storage + requests + egress
- Alluxio: Compute for cache nodes

---

## ‚ö†Ô∏è Common Pitfalls

### 1. Single NameNode Without HA

**Pitfall**: NameNode failure brings down entire cluster.

**Solution**: Enable NameNode HA with automatic failover

---

### 2. Small Files Problem

**Pitfall**: Millions of small files (<1 MB) overwhelm NameNode.

**Impact**: Each file consumes 150 bytes of NameNode memory

**Solution**: Combine small files or use archiving (HAR, SequenceFile)

---

### 3. Skipping Alluxio for S3 Migration

**Pitfall**: S3 direct access is 5-10x slower than HDFS.

**Solution**: Deploy Alluxio cache layer

---

### 4. Not Monitoring Block Health

**Pitfall**: Corrupt blocks go undetected until training fails.

**Solution**: Run `hdfs fsck` regularly, alert on issues

---

### 5. Ignoring Rack Awareness

**Pitfall**: All replicas on same rack ‚Üí data loss if rack fails.

**Solution**: Configure rack topology, ensure cross-rack replication

---

### 6. Over-Provisioning Alluxio Cache

**Pitfall**: Allocating more memory than needed wastes resources.

**Solution**: Monitor cache hit rate, right-size cache

---

### 7. Not Testing Failover

**Pitfall**: HA configured but never tested ‚Üí fails in production.

**Solution**: Simulate failures quarterly

---

### 8. Using HDFS for Archival

**Pitfall**: HDFS is expensive for infrequently accessed data.

**Solution**: Archive to S3 Glacier, delete from HDFS

---

### 9. Mixing HDFS and S3 Without Abstraction

**Pitfall**: Code tightly coupled to storage type.

**Solution**: Use abstraction layer (Alluxio, JuiceFS)

---

### 10. Not Planning for Growth

**Pitfall**: NameNode runs out of memory as file count grows.

**Solution**: Implement HDFS Federation for horizontal scaling

---

## üèãÔ∏è Hands-On Exercises

### Exercise 1: Deploy Alluxio with S3 Backend (Intermediate, 8-10 hours)

**Objective**: Set up Alluxio cluster with S3 under storage and benchmark performance.

**Tasks**:

1. **Install Alluxio** (3-node cluster)
2. **Configure S3 backend**:
   - Mount S3 bucket as under storage
   - Configure tiered storage (memory + SSD)
3. **Pre-load dataset** into cache
4. **Benchmark**:
   - S3 direct read throughput
   - Alluxio cached read throughput
   - Compare speedup

**Validation**:
- Cache hit rate >90%
- Alluxio 10x faster than S3 direct

**Deliverables**:
- Alluxio configuration files
- Benchmark results
- Performance comparison chart

---

### Exercise 2: HDFS to S3 Migration (Advanced, 12-15 hours)

**Objective**: Migrate 1 TB HDFS data to S3, maintain performance with Alluxio.

**Tasks**:

1. **Baseline**: Benchmark Spark job on HDFS
2. **Migration**:
   - Use DistCp to copy HDFS ‚Üí S3
   - Verify data integrity
3. **Deploy Alluxio** for caching
4. **Update Spark jobs** to read from S3
5. **Benchmark**: Compare HDFS vs S3+Alluxio
6. **Cost analysis**: Calculate savings

**Validation**:
- Data migration complete (1 TB)
- Performance within 20% of HDFS
- Cost reduced by 50%+

**Deliverables**:
- Migration script
- Performance comparison
- Cost analysis spreadsheet

---

### Exercise 3: HDFS High Availability Setup (Advanced, 10-12 hours)

**Objective**: Configure HDFS HA with automatic failover.

**Tasks**:

1. **Deploy HA**:
   - 2 NameNodes (Active + Standby)
   - 3 JournalNodes
   - Zookeeper for automatic failover
2. **Test failover**:
   - Kill Active NameNode
   - Verify Standby promoted to Active
   - Measure downtime
3. **Monitor**:
   - Set up Prometheus + Grafana
   - Track NameNode state

**Validation**:
- Automatic failover in <30 seconds
- Zero data loss during failover

**Deliverables**:
- HA configuration files
- Failover test results
- Monitoring dashboard

---

### Exercise 4: JuiceFS for POSIX on S3 (Intermediate, 6-8 hours)

**Objective**: Deploy JuiceFS to provide POSIX interface for S3.

**Tasks**:

1. **Setup**:
   - Install JuiceFS
   - Configure Redis metadata store
   - Mount S3 bucket as file system
2. **Test POSIX operations**:
   - cp, mv, ls, rm on mounted file system
   - Verify data written to S3
3. **Benchmark**:
   - Sequential read/write throughput
   - Random read/write IOPS
   - Cache hit rate

**Validation**:
- POSIX operations work correctly
- Data persisted to S3
- Performance acceptable for use case

**Deliverables**:
- JuiceFS configuration
- Benchmark results
- Use case documentation

---

## üîó Related Concepts

### Within This Course:
- [[01. Data Lakes, Warehouses, and Lakehouses]] - Architecture using distributed file systems
- [[02. Object Storage (S3, GCS, Azure Blob)]] - Alternative to HDFS
- [[04. Storage Optimization for ML Workloads]] - Performance tuning
- [[07. Batch Processing for ML Data]] - Reading from HDFS/S3

### External Connections:
- **Data Processing**: [[07. Batch Processing for ML Data]] - Spark on HDFS
- **Feature Stores**: [[10. Feature Stores]] - Storage backend
- **MLOps**: [[13. MLOps & Data Pipeline Integration]] - Model artifact storage

---

## üìö Further Reading

### Essential Documentation:

1. **Apache Hadoop HDFS Documentation**
   - https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html
   - HDFS architecture and design

2. **Alluxio Documentation**
   - https://docs.alluxio.io/
   - Comprehensive Alluxio guide

3. **JuiceFS Documentation**
   - https://juicefs.com/docs/
   - POSIX on object storage

### Key Papers:

1. **"The Hadoop Distributed File System" (Yahoo, 2010)**
   - https://storageconference.us/2010/Papers/MSST/Shvachko.pdf
   - HDFS architecture and implementation

2. **"Tachyon: Reliable, Memory Speed Storage for Cluster Computing Frameworks" (Berkeley, 2014)**
   - https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-135.pdf
   - Alluxio (formerly Tachyon) design

### Blogs and Articles:

1. **Alluxio Blog - "10x Faster Spark with Alluxio"**
   - https://www.alluxio.io/blog/
   - Performance optimization case studies

2. **Databricks Blog - "Migrating from HDFS to S3"**
   - https://databricks.com/blog/
   - Best practices for HDFS migration

3. **AWS Big Data Blog - "FSx for Lustre for ML Training"**
   - https://aws.amazon.com/blogs/big-data/
   - Using Lustre for ML workloads

### Video Courses:

1. **"Hadoop Fundamentals" (Cloudera)**
   - HDFS architecture and administration

2. **"Alluxio for Data Orchestration" (Alluxio)**
   - https://www.alluxio.io/resources/
   - Hands-on Alluxio training

---

## üìù Key Takeaways

1. **HDFS provides high-throughput sequential I/O**: Co-located compute and storage enables 500+ MB/sec throughput vs 100 MB/sec for S3 direct.

2. **Alluxio bridges the gap between S3 and HDFS**: Memory-speed caching (10+ GB/sec) on S3 backend combines S3 cost with HDFS performance.

3. **Object storage is winning**: Modern ML platforms prefer S3/GCS + Alluxio over HDFS for cost, scalability, and operational simplicity.

4. **Data locality dramatically improves performance**: Co-locating Spark tasks with HDFS blocks or Alluxio cache reduces network transfer by 10x.

5. **NameNode is the HDFS bottleneck**: Single NameNode limits scalability; HDFS Federation or migration to S3 solves this.

6. **JuiceFS enables POSIX on object storage**: Legacy tools requiring POSIX semantics can work with S3/GCS transparently.

7. **HDFS High Availability is critical for production**: Active-Standby NameNode with automatic failover prevents single point of failure.

8. **Block size optimization matters**: 256-512 MB blocks for large ML datasets reduce NameNode metadata overhead.

9. **Lustre excels at HPC workloads**: AWS FSx for Lustre provides 100+ GB/sec throughput for large-scale distributed training.

10. **Migration path: HDFS ‚Üí S3 + Alluxio**: Reduce costs by 60% while maintaining performance with Alluxio caching layer.

---

## ‚úèÔ∏è Notes Section

**Personal Insights**:

**Questions to Explore**:

**Related Projects**:

---

*Last updated: 2025-10-19*
*Part of: [[DEforAI - Data Engineering for AI/ML]]*
*Chapter: [[04. Data Storage for ML]]*
