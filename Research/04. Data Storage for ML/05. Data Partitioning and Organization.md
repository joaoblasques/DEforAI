# Data Partitioning and Organization

**Course:** Data Engineering for AI/ML
**Chapter:** 04 - Data Storage for ML
**Subchapter:** 05 - Data Partitioning and Organization
**Created:** 2025-10-19
**Updated:** 2025-10-19

---

## ğŸ“‹ Overview

Data partitioning is the practice of dividing large datasets into smaller, more manageable segments based on specific columns or criteria. For machine learning workloads, effective partitioning can reduce query times by 10-1000x, lower compute costs by 50-90%, and improve data freshness by enabling incremental processing.

Unlike traditional analytics where partitioning primarily optimizes query performance, ML partitioning must also consider:
- **Training data freshness**: Efficiently access recent data for model retraining
- **Temporal consistency**: Ensure features and labels align by time windows
- **Experiment isolation**: Separate data by model version or experiment ID
- **Multi-tenant access**: Partition by team, project, or customer for access control
- **Incremental processing**: Process only new data since last run (CDC patterns)

Poor partitioning strategies lead to:
- **Full table scans**: Reading 100 TB when you need 10 GB
- **Partition skew**: 90% of data in one partition, crushing a single worker
- **Metadata explosion**: Millions of partitions causing driver OOM errors
- **Failed joins**: Misaligned partitions preventing broadcast optimizations
- **Wasted compute**: Reprocessing unchanged historical data daily

This subchapter covers partitioning strategies tailored for ML systems, including time-based partitioning for training pipelines, Z-ORDER clustering for multi-dimensional queries, and advanced techniques like bucketing and dynamic partitioning. We'll focus on Delta Lake, Iceberg, and Hudi as they provide superior partition management compared to raw Parquet.

**Key Topics:**
- Physical vs logical partitioning in data lakes
- Hive-style partitioning and directory structures
- Time-based partitioning for ML pipelines (daily, hourly, streaming)
- Z-ORDER and CLUSTER BY for multi-column filtering
- Hash-based partitioning and bucketing for joins
- Dynamic partitioning and partition evolution
- Partition pruning and query optimization
- Multi-dimensional partitioning strategies

**Prerequisites:**
- Understanding of data lakes and lakehouses (Subchapter 04.01)
- Familiarity with Parquet file format (Subchapter 04.04)
- Basic SQL and Spark DataFrame operations
- Knowledge of ML training and feature engineering workflows

---

## ğŸ¯ Learning Objectives

By the end of this subchapter, you will be able to:

1. **Design partition schemes** that align with ML access patterns (training, inference, feature engineering)
2. **Implement time-based partitioning** for incremental training and data freshness
3. **Apply Z-ORDER clustering** to optimize multi-column filters (e.g., `WHERE user_id = X AND date > Y`)
4. **Avoid partition pitfalls** like over-partitioning (millions of directories) and under-partitioning (no pruning benefit)
5. **Optimize joins** using bucketing and co-located partitioning
6. **Manage partition evolution** when adding new features or changing granularity (hourly â†’ daily)
7. **Measure partition effectiveness** using metrics like partition pruning ratio and data skipping percentage
8. **Implement dynamic partitioning** for streaming and real-time feature updates
9. **Balance partition count** to achieve 128 MB - 1 GB per partition (sweet spot for cloud storage)
10. **Design multi-tenant partitioning** for ML platforms with team/project isolation

---

## ğŸ“š Core Concepts

### 1. Physical vs Logical Partitioning

**Physical Partitioning (Hive-Style):**

Physical partitioning creates separate directories on disk for each partition value:

```
s3://ml-data/features/
â”œâ”€â”€ date=2025-10-15/
â”‚   â”œâ”€â”€ part-00000.parquet
â”‚   â”œâ”€â”€ part-00001.parquet
â”œâ”€â”€ date=2025-10-16/
â”‚   â”œâ”€â”€ part-00000.parquet
â”œâ”€â”€ date=2025-10-17/
â”‚   â”œâ”€â”€ part-00000.parquet
â”‚   â””â”€â”€ part-00001.parquet
```

**Characteristics:**
- Partition column values appear in directory paths
- Query engines skip entire directories during partition pruning
- Simple to understand and implement
- Limited to 10K-100K partitions before metadata overhead becomes prohibitive

**Logical Partitioning (Iceberg Hidden Partitioning):**

Logical partitioning stores partition information in metadata, not file paths:

```
s3://ml-data/features/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ file-001.parquet  (contains date=2025-10-15 data)
â”‚   â”œâ”€â”€ file-002.parquet  (contains date=2025-10-15 + date=2025-10-16 data)
â”‚   â””â”€â”€ file-003.parquet  (contains date=2025-10-17 data)
â””â”€â”€ metadata/
    â””â”€â”€ manifest-files (tracks which file contains which partition)
```

**Characteristics:**
- Partition values stored in metadata manifests
- Can support millions of partitions without directory explosion
- Allows partition evolution without rewriting data
- More complex to implement (requires table format like Iceberg)

**ML Use Case Comparison:**

| Aspect | Physical (Hive) | Logical (Iceberg) |
|--------|-----------------|-------------------|
| **Partitions supported** | 10K-100K | Millions |
| **Partition evolution** | Requires rewrite | Free (metadata only) |
| **Access pattern** | Batch training (date ranges) | Real-time serving (user lookups) |
| **Implementation** | Simpler (Spark, Hive, Presto) | Requires Iceberg/Hudi |
| **Metadata overhead** | High (many directories) | Low (manifest files) |

**Recommendation:** Use **Hive-style partitioning** for batch ML training (date/hour partitions). Use **Iceberg** for real-time feature stores with high-cardinality partitioning (user_id, session_id).

### 2. Hive-Style Partitioning Fundamentals

**Partition Column Selection:**

Choose partition columns that:
1. Are frequently used in `WHERE` clauses
2. Have low-to-medium cardinality (< 10K unique values ideal)
3. Distribute data relatively evenly (avoid 90% in one partition)
4. Are immutable (values don't change after write)

**Example for ML Training Data:**

```python
# Good: Partition by date (365 partitions per year)
df.write.partitionBy("date").parquet("s3://features/user_features/")

# Bad: Partition by user_id (10M partitions)
df.write.partitionBy("user_id").parquet("s3://features/user_features/")

# Better: Partition by date and model_version
df.write.partitionBy("date", "model_version").parquet("s3://features/user_features/")
```

**Partition Granularity Trade-offs:**

| Granularity | Partitions/Year | Avg Partition Size (1 TB/year) | Use Case |
|-------------|-----------------|--------------------------------|----------|
| **Yearly** | 1 | 1 TB | Long-term archival |
| **Monthly** | 12 | 83 GB | Quarterly model retraining |
| **Weekly** | 52 | 19 GB | Weekly experiments |
| **Daily** | 365 | 2.7 GB | Standard ML training |
| **Hourly** | 8,760 | 114 MB | Real-time streaming pipelines |
| **Minutely** | 525,600 | 2 MB | **Avoid** (metadata explosion) |

**Sweet Spot:** Daily partitions for batch training, hourly for streaming, monthly for archival.

### 3. Z-ORDER Clustering

**Problem with Single-Column Partitioning:**

```sql
-- Partitioned by date only
SELECT * FROM features
WHERE date = '2025-10-15'
  AND user_id = 123456  -- No partition pruning on user_id
  AND category = 'electronics'  -- Full scan of date partition
```

Only `date` benefits from partitioning; `user_id` and `category` require full scan of the partition.

**Z-ORDER Solution:**

Z-ORDER (also called space-filling curves) co-locates related data by interleaving multiple column values:

```
Traditional ordering:     Z-ORDER clustering:
user_id  | category       user_id  | category
---------|--------        ---------|--------
1        | A              1        | A
2        | A              1        | B
3        | A              2        | A
1        | B              2        | B
2        | B              3        | A
3        | B              3        | B
```

With Z-ORDER on `(user_id, category)`, data for `user_id=1, category=B` is stored nearby, enabling efficient multi-column filtering.

**How Z-ORDER Works:**

1. Convert each dimension to binary
2. Interleave bits from each dimension
3. Sort data by resulting Z-order value

**Example:**
```
user_id=5 (binary: 0101), category=3 (binary: 011)
Z-order: 0 0 1 1 0 1 1 = 27 (decimal)
    â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘
    u c u c u c c
```

Files are sorted by Z-order value, so multi-dimensional queries can skip large portions of data.

**Z-ORDER in Delta Lake:**

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "s3://features/user_features/")

# Apply Z-ORDER on frequently filtered columns
delta_table.optimize().executeZOrderBy("user_id", "category")

# Now multi-column queries benefit from data skipping
df = spark.read.format("delta").load("s3://features/user_features/")
result = df.filter("user_id = 123456 AND category = 'electronics'")
# Skips 80-95% of data vs no Z-ORDER
```

**When to Use Z-ORDER:**

- âœ… Multi-column `WHERE` clauses (e.g., `user_id AND date`)
- âœ… High-cardinality columns that can't be partitioned
- âœ… Queries with range filters on multiple columns
- âŒ Single-column queries (regular sorting is better)
- âŒ Columns with very few distinct values (< 10)

### 4. Hash-Based Partitioning and Bucketing

**Hash Partitioning:**

Distribute data evenly using a hash function:

```python
# Partition by hash of user_id (ensures even distribution)
df.write.partitionBy(F.hash("user_id") % 100) \
    .parquet("s3://features/user_features/")

# Creates 100 partitions with roughly equal size
```

**Bucketing (for Joins):**

Bucketing pre-shuffles data by hash, enabling shuffle-free joins:

```python
# Bucket features table by user_id (100 buckets)
features_df.write.bucketBy(100, "user_id") \
    .sortBy("timestamp") \
    .saveAsTable("features.user_features")

# Bucket labels table by user_id (same 100 buckets)
labels_df.write.bucketBy(100, "user_id") \
    .saveAsTable("features.user_labels")

# Join without shuffle (10-100x faster for large tables)
training_df = spark.sql("""
SELECT f.*, l.label
FROM features.user_features f
JOIN features.user_labels l
ON f.user_id = l.user_id
""")
```

**Bucketing Benefits:**

- **Shuffle-free joins**: Matching buckets are already co-located
- **Faster aggregations**: `GROUP BY user_id` doesn't shuffle
- **Consistent partitioning**: Guarantees data distribution

**Bucketing Limitations:**

- Only works with Hive tables (not Delta Lake out-of-the-box)
- Fixed bucket count (hard to change after creation)
- Requires same bucket count across joined tables

**Hash Partitioning vs Bucketing:**

| Feature | Hash Partitioning | Bucketing |
|---------|------------------|-----------|
| **Use case** | Even data distribution | Optimized joins |
| **Shuffle-free joins** | No | Yes |
| **Flexibility** | Can change anytime | Fixed at creation |
| **Storage format** | Any (Parquet, Delta) | Hive tables only |
| **ML use case** | Feature stores | Training data joins |

### 5. Dynamic Partitioning

**Dynamic vs Static Partitioning:**

**Static Partitioning:**
```python
# Write each partition explicitly
df_2025_10_15 = df.filter("date = '2025-10-15'")
df_2025_10_15.write.parquet("s3://data/date=2025-10-15/")

df_2025_10_16 = df.filter("date = '2025-10-16'")
df_2025_10_16.write.parquet("s3://data/date=2025-10-16/")
# Tedious, error-prone
```

**Dynamic Partitioning:**
```python
# Automatically creates partitions based on data
df.write.partitionBy("date").mode("append").parquet("s3://data/")
# Spark infers partition values from data
```

**Dynamic Partition Modes (Hive/Spark):**

1. **Strict Mode** (default in Hive):
   - Requires at least one static partition
   - Prevents accidental full-table overwrites

2. **Nonstrict Mode**:
   - All partitions can be dynamic
   - More convenient but riskier

**Configuration:**
```python
# Enable nonstrict dynamic partitioning
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")
spark.conf.set("hive.exec.dynamic.partition.mode", "nonstrict")

# Append new data with dynamic partitions
new_data.write.mode("append") \
    .partitionBy("date") \
    .parquet("s3://features/")
```

**ML Use Case - Streaming Feature Updates:**

```python
# Streaming query dynamically creates hourly partitions
spark.readStream \
    .format("kafka") \
    .load() \
    .selectExpr(
        "cast(value as string) as json",
        "date_format(current_timestamp(), 'yyyy-MM-dd-HH') as partition_hour"
    ) \
    .writeStream \
    .format("delta") \
    .partitionBy("partition_hour") \
    .option("checkpointLocation", "s3://checkpoints/") \
    .start("s3://features/realtime/")
```

### 6. Partition Pruning and Query Optimization

**Partition Pruning:**

Query engines analyze `WHERE` clauses and skip reading irrelevant partitions:

```sql
-- Partitioned by date
SELECT * FROM features
WHERE date >= '2025-10-01' AND date <= '2025-10-07'

-- Partition pruning:
-- Scans: date=2025-10-01, ..., date=2025-10-07 (7 partitions)
-- Skips: All other partitions (358 partitions if yearly data)
```

**Pruning Effectiveness Metrics:**

```python
# Check how many partitions are scanned
spark.conf.set("spark.sql.adaptive.enabled", "true")

result = spark.sql("""
SELECT * FROM features
WHERE date = '2025-10-15' AND model_version = 'v2'
""")

# View query plan
result.explain(mode="extended")

# Look for:
# PartitionFilters: [isnotnull(date#123), (date#123 = 2025-10-15)]
# PushedFilters: [IsNotNull(model_version), EqualTo(model_version,v2)]
```

**Measuring Partition Skipping:**

```python
# Compare bytes scanned with/without partition pruning
from pyspark.sql.utils import AnalysisException

# Without filter (full scan)
full_scan = spark.read.format("delta").load("s3://features/")
bytes_full = full_scan._jdf.queryExecution().optimizedPlan().stats().sizeInBytes()

# With partition filter
filtered = spark.read.format("delta").load("s3://features/") \
    .filter("date = '2025-10-15'")
bytes_filtered = filtered._jdf.queryExecution().optimizedPlan().stats().sizeInBytes()

pruning_ratio = (bytes_full - bytes_filtered) / bytes_full * 100
print(f"Partition pruning skipped {pruning_ratio:.1f}% of data")
```

**Common Pruning Anti-Patterns:**

âŒ **Functions on partition columns disable pruning:**
```sql
-- BAD: No pruning (function on partition column)
SELECT * FROM features
WHERE year(date) = 2025

-- GOOD: Direct comparison on partition column
SELECT * FROM features
WHERE date >= '2025-01-01' AND date < '2026-01-01'
```

âŒ **OR conditions on non-partition columns:**
```sql
-- BAD: OR requires scanning all partitions
SELECT * FROM features
WHERE date = '2025-10-15' OR user_id = 123

-- GOOD: Separate queries and UNION if needed
```

### 7. Multi-Dimensional Partitioning

**Hierarchical Partitioning:**

Partition by multiple columns in order of cardinality (low â†’ high):

```python
# Hierarchical: date (365 values) â†’ model_version (10 values)
df.write.partitionBy("date", "model_version") \
    .parquet("s3://features/")

# Directory structure:
# s3://features/
# â”œâ”€â”€ date=2025-10-15/
# â”‚   â”œâ”€â”€ model_version=v1/
# â”‚   â”‚   â””â”€â”€ part-00000.parquet
# â”‚   â””â”€â”€ model_version=v2/
# â”‚       â””â”€â”€ part-00000.parquet
# â”œâ”€â”€ date=2025-10-16/
# â”‚   â”œâ”€â”€ model_version=v1/
# â”‚   â””â”€â”€ model_version=v2/
```

**Partition Count Calculation:**

```python
# Total partitions = product of distinct values
date_cardinality = 365  # 1 year of daily data
model_version_cardinality = 10  # 10 model versions

total_partitions = date_cardinality * model_version_cardinality
# = 3,650 partitions (manageable)
```

**When to Use Multi-Dimensional Partitioning:**

âœ… **Use hierarchical partitioning when:**
- Both columns are frequently filtered together
- Combined cardinality < 10K partitions
- Partition sizes remain 128 MB - 1 GB

âŒ **Avoid hierarchical partitioning when:**
- Creates > 10K partitions
- Partitions are too small (< 10 MB)
- Only one column is typically filtered

**Alternative: Z-ORDER Instead of Multi-Partitioning:**

```python
# Instead of:
df.write.partitionBy("date", "category", "region") \
    .parquet("s3://features/")  # 365 Ã— 20 Ã— 50 = 365K partitions!

# Use:
df.write.partitionBy("date") \
    .format("delta") \
    .save("s3://features/")

DeltaTable.forPath(spark, "s3://features/") \
    .optimize() \
    .executeZOrderBy("category", "region")

# Same query performance, 365 partitions instead of 365K
```

### 8. Partition Evolution and Management

**Partition Evolution Scenarios:**

1. **Changing granularity**: Daily â†’ hourly partitions
2. **Adding partition columns**: `date` â†’ `date, region`
3. **Removing partition columns**: `date, model_version` â†’ `date`
4. **Fixing over-partitioning**: 1M partitions â†’ 10K partitions

**Scenario 1: Increasing Granularity (Daily â†’ Hourly)**

```python
# Old: Daily partitions
# s3://features/date=2025-10-15/

# New: Hourly partitions
# s3://features/date=2025-10-15/hour=00/
# s3://features/date=2025-10-15/hour=01/

# Migration strategy:
from delta.tables import DeltaTable

# 1. Read old data
old_data = spark.read.format("delta").load("s3://features/daily/")

# 2. Add hour column
from pyspark.sql.functions import hour
new_data = old_data.withColumn("hour", hour("timestamp"))

# 3. Write with new partitioning
new_data.write.format("delta") \
    .partitionBy("date", "hour") \
    .mode("overwrite") \
    .save("s3://features/hourly/")

# 4. Update references to point to new table
```

**Scenario 2: Iceberg Hidden Partitioning (Schema Evolution)**

```python
# Iceberg allows partition evolution without rewriting data
from pyspark.sql.functions import days

# Initial partitioning by day
spark.sql("""
CREATE TABLE features.user_features (
  user_id BIGINT,
  features ARRAY<DOUBLE>,
  timestamp TIMESTAMP
)
USING iceberg
PARTITIONED BY (days(timestamp))
""")

# Later: Switch to hourly without rewriting
spark.sql("""
ALTER TABLE features.user_features
SET PARTITION SPEC (hours(timestamp))
""")

# Old data remains in daily partitions, new data uses hourly
# Queries work seamlessly across both
```

**Partition Consolidation (Fixing Over-Partitioning):**

```python
# Problem: 1M tiny partitions
# s3://features/user_id=1/, user_id=2/, ..., user_id=1000000/

# Solution: Repartition by hash
df = spark.read.parquet("s3://features/")

# Group into 1000 partitions by hash
df_rehashed = df.withColumn(
    "partition_hash",
    F.hash("user_id") % 1000
)

df_rehashed.write.partitionBy("partition_hash") \
    .mode("overwrite") \
    .parquet("s3://features_optimized/")

# Result: 1M partitions â†’ 1K partitions
```

---

## ğŸ’¡ Practical Examples

### Example 1: Time-Based Partitioning for Training Data

**Scenario:** 1 TB of user clickstream data for ML model training. New data arrives daily. Need to efficiently retrain on last 90 days of data.

**Problem (No Partitioning):**
```python
# Full scan of 1 TB every day
df = spark.read.parquet("s3://clickstream/")
df_last_90_days = df.filter("timestamp >= current_date() - interval 90 days")
# Query time: 15 minutes, reads entire 1 TB
```

**Solution (Daily Partitioning):**
```python
# Step 1: Repartition existing data by date
df = spark.read.parquet("s3://clickstream/raw/")

df_partitioned = df.withColumn("date", F.to_date("timestamp"))

df_partitioned.write.format("delta") \
    .partitionBy("date") \
    .mode("overwrite") \
    .save("s3://clickstream/partitioned/")

# Step 2: Query last 90 days (partition pruning)
df_train = spark.read.format("delta") \
    .load("s3://clickstream/partitioned/") \
    .filter("date >= current_date() - interval 90 days")

# Query time: 30 seconds (30x faster)
# Reads: 90 partitions Ã— 2.7 GB = 243 GB (24% of data)
```

**Incremental Append:**
```python
# Daily pipeline appends new data
new_data = spark.read.parquet("s3://clickstream/raw/2025-10-17/")

new_data.withColumn("date", F.to_date("timestamp")) \
    .write.format("delta") \
    .partitionBy("date") \
    .mode("append") \
    .save("s3://clickstream/partitioned/")

# Automatically creates date=2025-10-17 partition
```

### Example 2: Z-ORDER for Multi-Column Filtering

**Scenario:** Feature store with 10 billion user events. Queries filter by `user_id` (10M unique) and `date` (365 days). Can't partition by `user_id` (too many partitions).

**Problem (Partition by Date Only):**
```python
# Partitioned by date
df.write.format("delta") \
    .partitionBy("date") \
    .save("s3://feature-store/events/")

# Query filters by both date AND user_id
result = spark.read.format("delta").load("s3://feature-store/events/") \
    .filter("date = '2025-10-15' AND user_id = 123456")

# Partition pruning works for date, but still scans entire partition for user_id
# Scans: 27 GB (entire date=2025-10-15 partition)
# Returns: 100 KB (user 123456's events)
# Waste: 99.99% of data read unnecessarily
```

**Solution (Z-ORDER on user_id):**
```python
from delta.tables import DeltaTable

# Apply Z-ORDER to cluster data by user_id within each date partition
delta_table = DeltaTable.forPath(spark, "s3://feature-store/events/")
delta_table.optimize().executeZOrderBy("user_id")

# Same query now benefits from data skipping
result = spark.read.format("delta").load("s3://feature-store/events/") \
    .filter("date = '2025-10-15' AND user_id = 123456")

# Scans: 300 MB (5% of partition, thanks to Z-ORDER)
# Returns: 100 KB
# Improvement: 90x reduction in data scanned
```

**Measured Impact:**
```python
# Before Z-ORDER
# - Query time: 45 seconds
# - Bytes scanned: 27 GB
# - Data skipping: 0%

# After Z-ORDER
# - Query time: 2 seconds (22.5x faster)
# - Bytes scanned: 300 MB (90x less)
# - Data skipping: 98.9%
```

### Example 3: Bucketing for Join Optimization

**Scenario:** Join user features (100 GB) with labels (10 GB) on `user_id` for training data preparation. Standard join causes expensive shuffle.

**Problem (Standard Join):**
```python
features = spark.read.parquet("s3://features/user_features/")
labels = spark.read.parquet("s3://labels/user_labels/")

training = features.join(labels, "user_id")

# Shuffle write: 110 GB
# Shuffle read: 110 GB
# Join time: 10 minutes
```

**Solution (Bucketed Tables):**
```python
# One-time: Bucket both tables by user_id
features.write.format("parquet") \
    .bucketBy(200, "user_id") \
    .sortBy("timestamp") \
    .mode("overwrite") \
    .saveAsTable("ml.user_features")

labels.write.format("parquet") \
    .bucketBy(200, "user_id") \
    .sortBy("timestamp") \
    .mode("overwrite") \
    .saveAsTable("ml.user_labels")

# Join without shuffle (matching buckets already co-located)
training = spark.table("ml.user_features").join(
    spark.table("ml.user_labels"),
    "user_id"
)

# Shuffle write: 0 GB
# Shuffle read: 0 GB
# Join time: 30 seconds (20x faster)
```

**Bucket Size Selection:**
```python
# Rule of thumb: 128-512 MB per bucket
data_size_gb = 100
target_bucket_size_mb = 256

num_buckets = int((data_size_gb * 1024) / target_bucket_size_mb)
# = (100 * 1024) / 256 = 400 buckets

# Round to power of 2 for better distribution
import math
num_buckets = 2 ** int(math.log2(num_buckets))
# = 256 buckets
```

### Example 4: Dynamic Partitioning for Streaming

**Scenario:** Real-time clickstream ingestion from Kafka. Create hourly partitions dynamically as data arrives.

**Implementation:**
```python
from pyspark.sql.functions import from_json, col, date_format

# Define schema
schema = StructType([
    StructField("user_id", LongType()),
    StructField("event_type", StringType()),
    StructField("timestamp", TimestampType())
])

# Read from Kafka
stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "clickstream") \
    .load()

# Parse JSON and add partition columns
parsed = stream.selectExpr("CAST(value AS STRING) as json") \
    .select(from_json(col("json"), schema).alias("data")) \
    .select("data.*") \
    .withColumn("partition_date", date_format("timestamp", "yyyy-MM-dd")) \
    .withColumn("partition_hour", date_format("timestamp", "HH"))

# Write with dynamic partitioning
query = parsed.writeStream \
    .format("delta") \
    .partitionBy("partition_date", "partition_hour") \
    .option("checkpointLocation", "s3://checkpoints/clickstream/") \
    .trigger(processingTime="5 minutes") \
    .start("s3://clickstream/events/")

# Automatically creates partitions:
# s3://clickstream/events/partition_date=2025-10-15/partition_hour=00/
# s3://clickstream/events/partition_date=2025-10-15/partition_hour=01/
# ...
```

**Partition Management:**
```python
# Compact small files hourly
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "s3://clickstream/events/")

# Compact last 24 hours of partitions
yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")

delta_table.optimize() \
    .where(f"partition_date >= '{yesterday}'") \
    .executeCompaction()
```

### Example 5: Partition Evolution (Daily â†’ Hourly)

**Scenario:** Initially partitioned by day, but need hourly granularity for real-time model updates.

**Migration Strategy:**
```python
from pyspark.sql.functions import hour, to_date

# Step 1: Read existing daily partitions
daily_data = spark.read.format("delta").load("s3://features/daily/")

# Step 2: Add hour column
hourly_data = daily_data.withColumn("hour", hour("timestamp"))

# Step 3: Write to new location with hourly partitioning
hourly_data.write.format("delta") \
    .partitionBy("date", "hour") \
    .mode("overwrite") \
    .save("s3://features/hourly/")

# Step 4: Update pipeline to write to new location
def process_batch(batch_df, batch_id):
    batch_df.withColumn("date", to_date("timestamp")) \
        .withColumn("hour", hour("timestamp")) \
        .write.format("delta") \
        .partitionBy("date", "hour") \
        .mode("append") \
        .save("s3://features/hourly/")

# Step 5: Cutover queries to new table
# OLD: spark.read.format("delta").load("s3://features/daily/")
# NEW: spark.read.format("delta").load("s3://features/hourly/")
```

**Backward Compatibility:**
```python
# Create view that unions old (daily) and new (hourly) data
spark.sql("""
CREATE OR REPLACE VIEW features.user_features AS
SELECT * FROM delta.`s3://features/daily/`
WHERE date < '2025-10-01'  -- Old data
UNION ALL
SELECT * FROM delta.`s3://features/hourly/`
WHERE date >= '2025-10-01'  -- New data
""")

# Queries work transparently
df = spark.table("features.user_features")
```

### Example 6: Partition Health Monitoring

**Scenario:** Monitor partition size distribution to detect over/under-partitioning.

**Implementation:**
```python
class PartitionMonitor:
    def __init__(self, spark, table_path):
        self.spark = spark
        self.table_path = table_path

    def analyze_partitions(self):
        """Analyze partition size distribution"""
        # Get file-level metadata
        files_df = self.spark.sql(f"""
            SELECT
                input_file_name() as file_path,
                COUNT(*) as row_count
            FROM delta.`{self.table_path}`
            GROUP BY input_file_name()
        """)

        # Extract partition columns from file paths
        import re
        def extract_partition(file_path):
            # Parse Hive-style partition from path
            # e.g., s3://bucket/table/date=2025-10-15/hour=10/part-00000.parquet
            match = re.search(r'date=([^/]+)/hour=([^/]+)', file_path)
            if match:
                return (match.group(1), match.group(2))
            return (None, None)

        from pyspark.sql.functions import udf
        from pyspark.sql.types import StructType, StructField, StringType

        partition_schema = StructType([
            StructField("date", StringType()),
            StructField("hour", StringType())
        ])

        extract_partition_udf = udf(extract_partition, partition_schema)

        files_with_partitions = files_df.withColumn(
            "partition",
            extract_partition_udf(col("file_path"))
        ).select(
            "partition.date",
            "partition.hour",
            "row_count"
        )

        # Aggregate by partition
        partition_stats = files_with_partitions.groupBy("date", "hour").agg(
            F.sum("row_count").alias("total_rows"),
            F.count("*").alias("file_count")
        )

        return partition_stats

    def detect_issues(self, partition_stats):
        """Detect over/under-partitioning issues"""
        from pyspark.sql.functions import col

        # Issue 1: Too many files per partition (small files problem)
        small_files = partition_stats.filter(col("file_count") > 100)

        # Issue 2: Too few rows per partition (over-partitioning)
        under_partitioned = partition_stats.filter(col("total_rows") < 10000)

        # Issue 3: Partition skew (10x difference in size)
        avg_rows = partition_stats.agg(F.avg("total_rows")).collect()[0][0]
        skewed = partition_stats.filter(
            (col("total_rows") > avg_rows * 10) |
            (col("total_rows") < avg_rows / 10)
        )

        print(f"âš ï¸ Small files partitions: {small_files.count()}")
        print(f"âš ï¸ Under-partitioned: {under_partitioned.count()}")
        print(f"âš ï¸ Skewed partitions: {skewed.count()}")

        return {
            "small_files": small_files,
            "under_partitioned": under_partitioned,
            "skewed": skewed
        }

# Usage
monitor = PartitionMonitor(spark, "s3://features/user_features/")
stats = monitor.analyze_partitions()
issues = monitor.detect_issues(stats)

stats.show(20)
```

---

## ğŸ”§ Code Examples

### Code Example 1: Comprehensive Partitioning Strategy

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from delta.tables import DeltaTable

class MLDataPartitioner:
    """Manages partitioning for ML datasets"""

    def __init__(self, spark):
        self.spark = spark

    def create_partitioned_table(
        self,
        source_path,
        target_path,
        partition_cols,
        z_order_cols=None,
        target_file_size_mb=1024
    ):
        """
        Create optimally partitioned table from source data

        Args:
            source_path: Path to source data (Parquet, CSV, etc.)
            target_path: Path to write partitioned Delta table
            partition_cols: List of columns to partition by (e.g., ['date', 'model_version'])
            z_order_cols: List of columns to Z-ORDER (e.g., ['user_id', 'category'])
            target_file_size_mb: Target size per file in MB
        """

        # Read source data
        df = self.spark.read.parquet(source_path)

        # Calculate optimal partition count
        row_count = df.count()
        avg_row_size = df.select(
            sum(length(to_json(struct("*")))).alias("total_bytes")
        ).collect()[0][0] / row_count

        total_size_mb = (row_count * avg_row_size) / (1024 ** 2)
        num_files = max(1, int(total_size_mb / target_file_size_mb))

        print(f"ğŸ“Š Source Analysis:")
        print(f"  Rows: {row_count:,}")
        print(f"  Avg row size: {avg_row_size:.0f} bytes")
        print(f"  Total size: {total_size_mb:.2f} MB")
        print(f"  Target files: {num_files}")

        # Repartition
        df_partitioned = df.repartition(num_files, *partition_cols)

        # Write as Delta Lake
        df_partitioned.write.format("delta") \
            .partitionBy(*partition_cols) \
            .mode("overwrite") \
            .save(target_path)

        print(f"âœ… Created partitioned table at {target_path}")

        # Apply Z-ORDER if specified
        if z_order_cols:
            self.apply_z_order(target_path, z_order_cols)

    def apply_z_order(self, table_path, z_order_cols):
        """Apply Z-ORDER clustering"""
        print(f"ğŸ”§ Applying Z-ORDER on: {z_order_cols}")

        delta_table = DeltaTable.forPath(self.spark, table_path)
        delta_table.optimize().executeZOrderBy(*z_order_cols)

        print(f"âœ… Z-ORDER complete")

    def validate_partitioning(self, table_path, expected_partition_count=None):
        """Validate partition health"""
        print(f"ğŸ” Validating partitions at {table_path}")

        # Get partition stats
        detail = self.spark.sql(f"DESCRIBE DETAIL delta.`{table_path}`")

        num_files = detail.select("numFiles").collect()[0][0]
        size_bytes = detail.select("sizeInBytes").collect()[0][0]
        avg_file_size_mb = (size_bytes / num_files) / (1024 ** 2) if num_files > 0 else 0

        print(f"ğŸ“Š Partition Stats:")
        print(f"  Total files: {num_files:,}")
        print(f"  Avg file size: {avg_file_size_mb:.2f} MB")

        # Check for issues
        if avg_file_size_mb < 64:
            print(f"âš ï¸ WARNING: Small files detected (avg {avg_file_size_mb:.1f} MB)")
        elif avg_file_size_mb > 2048:
            print(f"âš ï¸ WARNING: Large files detected (avg {avg_file_size_mb:.1f} MB)")
        else:
            print(f"âœ… File sizes look healthy")

        if expected_partition_count and num_files > expected_partition_count * 1.5:
            print(f"âš ï¸ WARNING: More files than expected ({num_files} > {expected_partition_count})")

        return {
            "num_files": num_files,
            "avg_file_size_mb": avg_file_size_mb,
            "total_size_gb": size_bytes / (1024 ** 3)
        }

# Usage
partitioner = MLDataPartitioner(spark)

# Create partitioned training data
partitioner.create_partitioned_table(
    source_path="s3://raw/clickstream/",
    target_path="s3://features/clickstream_partitioned/",
    partition_cols=["date"],
    z_order_cols=["user_id", "event_type"],
    target_file_size_mb=1024
)

# Validate
stats = partitioner.validate_partitioning(
    "s3://features/clickstream_partitioned/",
    expected_partition_count=365
)
```

### Code Example 2: Dynamic Partition Writer for Streaming

```python
from pyspark.sql.streaming import StreamingQuery
from pyspark.sql.functions import *
from datetime import datetime, timedelta

class StreamingPartitionWriter:
    """Manages partitioned writes for streaming data"""

    def __init__(self, spark):
        self.spark = spark

    def write_with_dynamic_partitions(
        self,
        stream_df,
        output_path,
        partition_cols,
        checkpoint_path,
        trigger_interval="5 minutes",
        enable_compaction=True
    ):
        """
        Write streaming data with dynamic partitioning

        Args:
            stream_df: Streaming DataFrame
            output_path: Path to write partitioned data
            partition_cols: List of partition columns
            checkpoint_path: Checkpoint location
            trigger_interval: Trigger interval (e.g., "5 minutes")
            enable_compaction: Enable auto-compaction
        """

        # Configure auto-optimize
        if enable_compaction:
            self.spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
            self.spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")

        # Write stream
        query = stream_df.writeStream \
            .format("delta") \
            .partitionBy(*partition_cols) \
            .option("checkpointLocation", checkpoint_path) \
            .trigger(processingTime=trigger_interval) \
            .start(output_path)

        print(f"âœ… Started streaming write to {output_path}")
        print(f"  Partitions: {partition_cols}")
        print(f"  Trigger interval: {trigger_interval}")
        print(f"  Auto-compaction: {enable_compaction}")

        return query

    def scheduled_compaction(self, table_path, partition_filter=None):
        """
        Run compaction on recent partitions

        Args:
            table_path: Delta table path
            partition_filter: SQL filter for partitions (e.g., "date >= '2025-10-15'")
        """
        from delta.tables import DeltaTable

        delta_table = DeltaTable.forPath(self.spark, table_path)

        if partition_filter:
            print(f"ğŸ”§ Compacting partitions where {partition_filter}")
            delta_table.optimize().where(partition_filter).executeCompaction()
        else:
            print(f"ğŸ”§ Compacting all partitions")
            delta_table.optimize().executeCompaction()

        print(f"âœ… Compaction complete")

# Usage

# Set up streaming write
writer = StreamingPartitionWriter(spark)

# Read from Kafka
stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .load()

# Parse and add partition columns
parsed = stream.selectExpr("CAST(value AS STRING) as json") \
    .select(from_json(col("json"), schema).alias("data")) \
    .select("data.*") \
    .withColumn("partition_date", to_date("timestamp")) \
    .withColumn("partition_hour", hour("timestamp"))

# Write with dynamic partitions
query = writer.write_with_dynamic_partitions(
    stream_df=parsed,
    output_path="s3://features/realtime_events/",
    partition_cols=["partition_date", "partition_hour"],
    checkpoint_path="s3://checkpoints/realtime_events/",
    trigger_interval="5 minutes",
    enable_compaction=True
)

# Schedule hourly compaction
import schedule

def compact_recent_partitions():
    yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    writer.scheduled_compaction(
        table_path="s3://features/realtime_events/",
        partition_filter=f"partition_date >= '{yesterday}'"
    )

schedule.every().hour.do(compact_recent_partitions)
```

### Code Example 3: Partition Pruning Analyzer

```python
class PartitionPruningAnalyzer:
    """Analyze and measure partition pruning effectiveness"""

    def __init__(self, spark):
        self.spark = spark

    def measure_pruning(self, table_path, query_filter):
        """
        Measure how much data is skipped by partition pruning

        Args:
            table_path: Delta table path
            query_filter: SQL WHERE clause (e.g., "date = '2025-10-15'")

        Returns:
            Dict with pruning statistics
        """
        from delta.tables import DeltaTable

        # Get full table size
        delta_table = DeltaTable.forPath(self.spark, table_path)
        full_size_bytes = self.spark.sql(f"DESCRIBE DETAIL delta.`{table_path}`") \
            .select("sizeInBytes").collect()[0][0]

        # Execute query with filter
        df_filtered = self.spark.read.format("delta").load(table_path) \
            .filter(query_filter)

        # Get query plan
        plan = df_filtered._jdf.queryExecution().executedPlan().toString()

        # Extract scanned bytes from plan
        # (This is a simplified version; real implementation would parse metrics)
        scanned_bytes = self._estimate_scanned_bytes(df_filtered)

        # Calculate pruning ratio
        pruning_ratio = (full_size_bytes - scanned_bytes) / full_size_bytes * 100

        results = {
            "table_size_gb": full_size_bytes / (1024 ** 3),
            "scanned_gb": scanned_bytes / (1024 ** 3),
            "pruning_ratio_pct": pruning_ratio,
            "query_filter": query_filter
        }

        print(f"ğŸ“Š Partition Pruning Analysis:")
        print(f"  Table size: {results['table_size_gb']:.2f} GB")
        print(f"  Scanned: {results['scanned_gb']:.2f} GB")
        print(f"  Pruned: {results['pruning_ratio_pct']:.1f}%")

        return results

    def _estimate_scanned_bytes(self, df):
        """Estimate bytes scanned from query plan"""
        # Simplified: count rows and estimate size
        # Real implementation would use Spark metrics
        row_count = df.count()
        avg_row_size = 1000  # Rough estimate
        return row_count * avg_row_size

    def benchmark_queries(self, table_path, queries):
        """
        Benchmark multiple queries and compare pruning effectiveness

        Args:
            table_path: Delta table path
            queries: List of (name, filter) tuples
        """
        import time

        results = []

        for query_name, query_filter in queries:
            print(f"\nğŸ” Benchmarking: {query_name}")

            start = time.time()

            # Execute query
            df = self.spark.read.format("delta").load(table_path) \
                .filter(query_filter)

            row_count = df.count()

            elapsed = time.time() - start

            # Measure pruning
            pruning_stats = self.measure_pruning(table_path, query_filter)

            results.append({
                "query_name": query_name,
                "query_filter": query_filter,
                "execution_time_s": elapsed,
                "row_count": row_count,
                **pruning_stats
            })

        # Display results
        import pandas as pd
        results_df = pd.DataFrame(results)
        print("\nğŸ“Š Benchmark Results:")
        print(results_df.to_string(index=False))

        return results_df

# Usage
analyzer = PartitionPruningAnalyzer(spark)

# Benchmark different query patterns
queries = [
    ("Single day", "date = '2025-10-15'"),
    ("Date range (7 days)", "date >= '2025-10-15' AND date <= '2025-10-21'"),
    ("Date range (30 days)", "date >= '2025-09-15' AND date <= '2025-10-14'"),
    ("Multi-column filter", "date = '2025-10-15' AND model_version = 'v2'"),
    ("No partition filter", "user_id = 123456")  # Worst case
]

results = analyzer.benchmark_queries(
    table_path="s3://features/user_features/",
    queries=queries
)
```

### Code Example 4: Partition Evolution Manager

```python
class PartitionEvolutionManager:
    """Manage partition schema evolution"""

    def __init__(self, spark):
        self.spark = spark

    def change_granularity(
        self,
        source_path,
        target_path,
        old_partition_col,
        new_partition_cols,
        timestamp_col="timestamp"
    ):
        """
        Change partition granularity (e.g., daily â†’ hourly)

        Args:
            source_path: Source Delta table
            target_path: Target Delta table with new partitioning
            old_partition_col: Old partition column (e.g., "date")
            new_partition_cols: New partition columns (e.g., ["date", "hour"])
            timestamp_col: Timestamp column to derive partitions from
        """

        print(f"ğŸ”„ Migrating partitions:")
        print(f"  From: {old_partition_col}")
        print(f"  To: {new_partition_cols}")

        # Read source data
        df = self.spark.read.format("delta").load(source_path)

        # Add new partition columns
        df_repartitioned = df

        for col_name in new_partition_cols:
            if col_name == "date":
                df_repartitioned = df_repartitioned.withColumn("date", to_date(timestamp_col))
            elif col_name == "hour":
                df_repartitioned = df_repartitioned.withColumn("hour", hour(timestamp_col))
            elif col_name == "year":
                df_repartitioned = df_repartitioned.withColumn("year", year(timestamp_col))
            elif col_name == "month":
                df_repartitioned = df_repartitioned.withColumn("month", month(timestamp_col))
            # Add more as needed

        # Write with new partitioning
        df_repartitioned.write.format("delta") \
            .partitionBy(*new_partition_cols) \
            .mode("overwrite") \
            .save(target_path)

        print(f"âœ… Migration complete: {target_path}")

        # Validate
        self._validate_migration(source_path, target_path)

    def _validate_migration(self, source_path, target_path):
        """Validate row count matches after migration"""
        source_count = self.spark.read.format("delta").load(source_path).count()
        target_count = self.spark.read.format("delta").load(target_path).count()

        if source_count == target_count:
            print(f"âœ… Validation passed: {source_count:,} rows in both tables")
        else:
            print(f"âš ï¸ WARNING: Row count mismatch!")
            print(f"  Source: {source_count:,}")
            print(f"  Target: {target_count:,}")

    def consolidate_partitions(
        self,
        source_path,
        target_path,
        hash_col,
        num_buckets
    ):
        """
        Consolidate over-partitioned data using hash bucketing

        Args:
            source_path: Over-partitioned table
            target_path: Target table with hash partitions
            hash_col: Column to hash (e.g., "user_id")
            num_buckets: Number of hash buckets
        """

        print(f"ğŸ”§ Consolidating partitions:")
        print(f"  Hash column: {hash_col}")
        print(f"  Target buckets: {num_buckets}")

        df = self.spark.read.format("delta").load(source_path)

        # Add hash partition column
        df_hashed = df.withColumn(
            "partition_hash",
            hash(col(hash_col)) % num_buckets
        )

        # Write with hash partitioning
        df_hashed.write.format("delta") \
            .partitionBy("partition_hash") \
            .mode("overwrite") \
            .save(target_path)

        # Compare partition counts
        source_partitions = self.spark.sql(f"DESCRIBE DETAIL delta.`{source_path}`") \
            .select("numFiles").collect()[0][0]
        target_partitions = self.spark.sql(f"DESCRIBE DETAIL delta.`{target_path}`") \
            .select("numFiles").collect()[0][0]

        reduction = (source_partitions - target_partitions) / source_partitions * 100

        print(f"âœ… Consolidation complete:")
        print(f"  Source partitions: {source_partitions:,}")
        print(f"  Target partitions: {target_partitions:,}")
        print(f"  Reduction: {reduction:.1f}%")

# Usage
evolution_mgr = PartitionEvolutionManager(spark)

# Change from daily to hourly partitions
evolution_mgr.change_granularity(
    source_path="s3://features/daily_partitions/",
    target_path="s3://features/hourly_partitions/",
    old_partition_col="date",
    new_partition_cols=["date", "hour"],
    timestamp_col="timestamp"
)

# Consolidate over-partitioned table
evolution_mgr.consolidate_partitions(
    source_path="s3://features/user_id_partitions/",  # 10M partitions
    target_path="s3://features/hash_partitions/",     # 1K partitions
    hash_col="user_id",
    num_buckets=1000
)
```

---

## âœ… Best Practices

### 1. Partition Column Selection

**DO:**
- âœ… Partition by columns frequently used in `WHERE` clauses (e.g., `date`, `region`, `model_version`)
- âœ… Choose low-to-medium cardinality columns (< 10K unique values)
- âœ… Ensure partitions have relatively uniform size (avoid 90% in one partition)
- âœ… Use immutable columns (values don't change after write)
- âœ… Partition by time for ML training data (daily or hourly)

**DON'T:**
- âŒ Partition by high-cardinality columns (user_id with 10M values)
- âŒ Partition by columns never used in queries
- âŒ Create > 10K partitions (metadata overhead)
- âŒ Partition by mutable columns (status that changes)
- âŒ Partition by continuous numerical values (use bucketing instead)

### 2. Partition Granularity

**DO:**
- âœ… Use **daily partitions** for standard batch ML training
- âœ… Use **hourly partitions** for streaming and real-time features
- âœ… Use **monthly partitions** for archival and long-term storage
- âœ… Target 128 MB - 1 GB per partition
- âœ… Adjust granularity based on query patterns (if you always query 7 days, weekly partitions may work)

**DON'T:**
- âŒ Use minutely or secondly partitions (creates millions of tiny partitions)
- âŒ Use yearly partitions for active training data (too coarse, no pruning benefit)
- âŒ Mix granularities (some daily, some hourly) without clear strategy
- âŒ Create partitions < 10 MB (small files problem)

### 3. Z-ORDER and Clustering

**DO:**
- âœ… Apply Z-ORDER on high-cardinality columns that can't be partitioned (`user_id`, `session_id`)
- âœ… Z-ORDER on columns frequently used together in `WHERE` clauses
- âœ… Run Z-ORDER after bulk writes or periodically (weekly for active tables)
- âœ… Limit Z-ORDER to 2-4 columns (diminishing returns beyond that)
- âœ… Combine partitioning + Z-ORDER (partition by `date`, Z-ORDER by `user_id`)

**DON'T:**
- âŒ Z-ORDER on low-cardinality columns (< 100 unique values)
- âŒ Z-ORDER on columns never used in queries
- âŒ Run Z-ORDER on every write (expensive, use auto-optimize instead)
- âŒ Z-ORDER without measuring effectiveness (benchmark before/after)

### 4. Bucketing for Joins

**DO:**
- âœ… Bucket join keys with same number of buckets (e.g., both tables have 200 buckets on `user_id`)
- âœ… Choose bucket count: `data_size_GB * 1024 / target_bucket_size_MB`
- âœ… Use powers of 2 for bucket count (128, 256, 512) for better distribution
- âœ… Sort bucketed data by frequently filtered column (`sortBy("timestamp")`)
- âœ… Use bucketing for large joins (> 10 GB) that shuffle frequently

**DON'T:**
- âŒ Bucket tables that are never joined
- âŒ Use different bucket counts across joined tables
- âŒ Bucket on columns with extreme skew (one value dominates)
- âŒ Over-bucket small tables (< 1 GB doesn't benefit)

### 5. Dynamic Partitioning

**DO:**
- âœ… Enable dynamic partitioning for streaming writes (`partitionBy("date")`)
- âœ… Use Delta Lake auto-optimize to prevent small files
- âœ… Set `spark.sql.sources.partitionOverwriteMode = "dynamic"` for incremental updates
- âœ… Schedule compaction jobs for streaming tables (hourly or daily)
- âœ… Monitor partition count growth over time

**DON'T:**
- âŒ Use dynamic partitioning without compaction (leads to millions of small files)
- âŒ Overwrite entire table when you only need to update one partition
- âŒ Ignore partition count limits (cap at 10K partitions)
- âŒ Create new partitions without archiving old ones

### 6. Partition Evolution

**DO:**
- âœ… Plan partition strategy upfront (harder to change later)
- âœ… Use Iceberg for frequent partition evolution needs
- âœ… Test migration on sample data before production
- âœ… Create backup before major partition schema changes
- âœ… Validate row count after partition migration

**DON'T:**
- âŒ Change partitioning without downtime window (creates inconsistency)
- âŒ Delete old partitioned data before validating new one
- âŒ Mix old and new partition schemes in same table
- âŒ Skip documentation of partition evolution history

### 7. Partition Pruning Optimization

**DO:**
- âœ… Write queries with direct comparisons on partition columns (`date = '2025-10-15'`)
- âœ… Use `EXPLAIN` to verify partition pruning is happening
- âœ… Monitor "bytes scanned" vs "bytes returned" ratio (should be < 10x)
- âœ… Collect statistics on partition columns (`ANALYZE TABLE COMPUTE STATISTICS`)
- âœ… Use partition filters early in query (before joins)

**DON'T:**
- âŒ Apply functions to partition columns (`WHERE year(date) = 2025` disables pruning)
- âŒ Use `OR` conditions across partitions (`date = X OR user_id = Y`)
- âŒ Rely on pruning without measuring (verify in query plan)
- âŒ Ignore partition skew (90% of queries hit 10% of partitions)

### 8. Monitoring and Maintenance

**DO:**
- âœ… Monitor partition count, file count per partition, and average partition size
- âœ… Set up alerts for > 10K partitions or avg file size < 64 MB
- âœ… Run weekly compaction on streaming tables
- âœ… Vacuum old partition files regularly (`VACUUM` in Delta Lake)
- âœ… Track query performance metrics (execution time, bytes scanned)

**DON'T:**
- âŒ Ignore partition health (let small files accumulate)
- âŒ Assume partitioning is "set it and forget it"
- âŒ Skip testing partition strategy on sample queries
- âŒ Over-optimize without measuring actual query patterns

### 9. Multi-Dimensional Partitioning

**DO:**
- âœ… Order partition columns by cardinality (low â†’ high): e.g., `partitionBy("date", "region")`
- âœ… Limit hierarchical partitioning to 2-3 levels
- âœ… Calculate total partitions: `cardinality(col1) Ã— cardinality(col2)`
- âœ… Use Z-ORDER instead of 3+ level partitioning
- âœ… Validate partition count < 10K before production

**DON'T:**
- âŒ Create > 3 levels of partitioning (metadata explosion)
- âŒ Partition by uncorrelated columns (date, random_id)
- âŒ Ignore total partition count calculation
- âŒ Use hierarchical partitioning when Z-ORDER suffices

### 10. Cost Optimization

**DO:**
- âœ… Partition to minimize data scanned (reduces compute costs)
- âœ… Consolidate small files to reduce S3 API costs
- âœ… Archive old partitions to cheaper storage (S3 Glacier)
- âœ… Use partition pruning to reduce query costs (BigQuery, Athena charge per byte scanned)
- âœ… Monitor storage costs per partition (identify opportunities to archive)

**DON'T:**
- âŒ Create excessive partitions (increases metadata costs)
- âŒ Keep all historical partitions in hot storage
- âŒ Ignore small files (increases S3 LIST costs)
- âŒ Run full table scans when partition filter would suffice

---

## âš ï¸ Common Pitfalls

### Pitfall 1: Over-Partitioning by High-Cardinality Column

**Problem:**
```python
# Partitioning by user_id (10M unique users)
df.write.partitionBy("user_id").parquet("s3://features/")

# Creates 10,000,000 directories
# Metadata overhead: 500 MB just for directory listings
# Spark driver OOM when reading table
```

**Why It Happens:**
- Misunderstanding that partitioning is for low-cardinality columns
- Attempting to optimize point lookups with partitioning
- Not calculating total partition count beforehand

**Solution:**
```python
# Use hash partitioning instead (1000 partitions)
df.withColumn("partition_hash", hash("user_id") % 1000) \
    .write.partitionBy("partition_hash") \
    .parquet("s3://features/")

# OR use Z-ORDER for point lookups
df.write.format("delta").save("s3://features/")
DeltaTable.forPath(spark, "s3://features/").optimize().executeZOrderBy("user_id")
```

**Prevention:**
- Calculate total partitions: `df.select("partition_col").distinct().count()`
- Limit partitions to < 10K
- Use bloom filters or Z-ORDER for high-cardinality lookups

### Pitfall 2: Functions on Partition Columns Disable Pruning

**Problem:**
```sql
-- Partition column: date

-- BAD: No partition pruning
SELECT * FROM features
WHERE year(date) = 2025 AND month(date) = 10

-- Scans all 365+ partitions
```

**Why It Happens:**
- Query engine can't infer partition values from function results
- Common when developers use date functions out of habit

**Solution:**
```sql
-- GOOD: Direct comparison enables pruning
SELECT * FROM features
WHERE date >= '2025-10-01' AND date < '2025-11-01'

-- Scans only October partitions (31 partitions)
```

**Prevention:**
- Always use direct comparisons on partition columns
- Use `EXPLAIN` to verify partition pruning
- Educate team on partition-safe query patterns

### Pitfall 3: Partition Skew (Uneven Distribution)

**Problem:**
```python
# Partition by region (3 values: US, EU, APAC)
# But 90% of data is in US region

df.write.partitionBy("region").parquet("s3://data/")

# Result:
# region=US/ : 900 GB (one massive partition)
# region=EU/ : 50 GB
# region=APAC/ : 50 GB

# Queries on US region are 18x slower than EU/APAC
```

**Why It Happens:**
- Choosing partition column without analyzing value distribution
- Real-world data is often skewed (Zipfian distribution)

**Solution:**
```python
# Add secondary partition for skewed partitions
df.withColumn("date", to_date("timestamp")) \
    .write.partitionBy("region", "date") \
    .parquet("s3://data/")

# Now US region is split across 365 date partitions
# region=US/date=2025-10-15/ : 2.5 GB (manageable)
```

**Prevention:**
- Analyze value distribution before choosing partition column
- Use hierarchical partitioning to break up large partitions
- Consider hash partitioning for even distribution

### Pitfall 4: Small Files from Streaming Without Compaction

**Problem:**
```python
# Streaming writes every 1 minute
spark.readStream.format("kafka").load() \
    .writeStream.format("delta") \
    .trigger(processingTime="1 minute") \
    .start("s3://events/")

# After 1 day: 1,440 micro-batches Ã— 10 partitions = 14,400 files
# After 30 days: 432,000 files
# Query time: 10 minutes (should be 10 seconds)
```

**Why It Happens:**
- Streaming creates one file per partition per micro-batch
- No automatic compaction enabled
- Focus on low latency at expense of storage efficiency

**Solution:**
```python
# Enable auto-optimize
spark.sql("""
ALTER TABLE delta.`s3://events/`
SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true'
)
""")

# Schedule hourly compaction
from delta.tables import DeltaTable

def compact_recent():
    DeltaTable.forPath(spark, "s3://events/") \
        .optimize() \
        .where(f"date >= current_date() - interval 1 day") \
        .executeCompaction()

schedule.every().hour.do(compact_recent)
```

**Prevention:**
- Enable auto-optimize from day 1
- Schedule regular compaction jobs
- Monitor file count growth

### Pitfall 5: Changing Partition Scheme Without Migration Plan

**Problem:**
```python
# Old: Partitioned by date
# s3://data/date=2025-10-15/

# Developer changes to hourly without migration
df.write.partitionBy("date", "hour") \
    .mode("overwrite") \
    .save("s3://data/")

# Result: Old data lost, or schema mismatch errors
```

**Why It Happens:**
- Partition evolution not planned
- Using "overwrite" mode carelessly
- No backup before schema change

**Solution:**
```python
# Proper migration:
# 1. Backup old data
spark.read.format("delta").load("s3://data/") \
    .write.format("delta").save("s3://data_backup/")

# 2. Create new table with new partitioning
old_data = spark.read.format("delta").load("s3://data/")
old_data.withColumn("hour", hour("timestamp")) \
    .write.format("delta") \
    .partitionBy("date", "hour") \
    .mode("overwrite") \
    .save("s3://data_new/")

# 3. Validate
assert old_data.count() == spark.read.format("delta").load("s3://data_new/").count()

# 4. Cutover
# Update references: s3://data/ â†’ s3://data_new/
```

**Prevention:**
- Plan partition evolution strategy upfront
- Use Iceberg for easier partition evolution
- Always backup before schema changes
- Validate row count after migration

### Pitfall 6: Not Using Z-ORDER for Multi-Column Filters

**Problem:**
```python
# Partitioned by date only
df.write.format("delta").partitionBy("date").save("s3://data/")

# Query filters by date AND user_id
result = spark.read.format("delta").load("s3://data/") \
    .filter("date = '2025-10-15' AND user_id = 123456")

# Partition pruning works for date, but entire partition scanned for user_id
# Reads: 2.7 GB (entire partition)
# Returns: 100 KB (one user)
```

**Why It Happens:**
- Not aware of Z-ORDER optimization
- Assuming partition on one column is sufficient
- No measurement of data skipping effectiveness

**Solution:**
```python
# Apply Z-ORDER on user_id
DeltaTable.forPath(spark, "s3://data/") \
    .optimize() \
    .executeZOrderBy("user_id")

# Same query now skips 95% of data
result = spark.read.format("delta").load("s3://data/") \
    .filter("date = '2025-10-15' AND user_id = 123456")

# Reads: 135 MB (5% of partition)
# Returns: 100 KB
# Improvement: 20x reduction in data scanned
```

**Prevention:**
- Use Z-ORDER for multi-column filter queries
- Measure data skipping before/after Z-ORDER
- Apply Z-ORDER on high-cardinality columns

### Pitfall 7: Bucketing Mismatch in Joins

**Problem:**
```python
# features table: 200 buckets on user_id
features.write.bucketBy(200, "user_id").saveAsTable("features")

# labels table: 100 buckets on user_id (mismatch!)
labels.write.bucketBy(100, "user_id").saveAsTable("labels")

# Join still shuffles (no benefit from bucketing)
result = spark.table("features").join(spark.table("labels"), "user_id")
```

**Why It Happens:**
- Tables bucketed at different times without coordination
- Not documenting bucket counts
- Assuming Spark will auto-adjust

**Solution:**
```python
# Rebucket labels to match features
labels = spark.table("labels").repartition(200, "user_id")

labels.write.bucketBy(200, "user_id") \
    .mode("overwrite") \
    .saveAsTable("labels")

# Now join is shuffle-free
result = spark.table("features").join(spark.table("labels"), "user_id")
```

**Prevention:**
- Document bucket counts in table metadata
- Use consistent bucketing across related tables
- Validate bucket counts before joins

### Pitfall 8: Ignoring Partition Count Limits

**Problem:**
```python
# Hierarchical partitioning without calculating total
df.write.partitionBy("date", "region", "product_category") \
    .parquet("s3://data/")

# date: 365 partitions
# region: 50 partitions
# product_category: 1000 partitions
# Total: 365 Ã— 50 Ã— 1000 = 18,250,000 partitions!

# Driver OOM, query failures
```

**Why It Happens:**
- Not calculating total partition count
- Assuming more partitions = better performance
- No testing before production deployment

**Solution:**
```python
# Use only date partitioning + Z-ORDER
df.write.format("delta") \
    .partitionBy("date") \
    .save("s3://data/")

# Z-ORDER on other columns
DeltaTable.forPath(spark, "s3://data/") \
    .optimize() \
    .executeZOrderBy("region", "product_category")

# Same query performance, 365 partitions instead of 18M
```

**Prevention:**
- Calculate total partitions before creating
- Limit to < 10K partitions
- Use Z-ORDER for additional dimensions

---

## ğŸ‹ï¸ Hands-On Exercises

### Exercise 1: Design Optimal Partitioning Strategy (2-3 hours)

**Objective:** Design and implement partitioning for a multi-billion row ML dataset.

**Scenario:**
- 5 billion user events (10 TB)
- Columns: `user_id` (10M unique), `event_type` (50 unique), `timestamp`, `features` (128-dim array)
- Query patterns:
  - 80% of queries filter by date range (last 7-30 days)
  - 15% filter by date AND user_id
  - 5% filter by event_type

**Tasks:**
1. Analyze query patterns and choose partition columns
2. Calculate total partitions for different strategies
3. Implement partitioning with Delta Lake
4. Apply Z-ORDER for multi-column queries
5. Benchmark query performance before/after
6. Measure partition pruning effectiveness

**Starter Code:**
```python
# Step 1: Analyze value distributions
df = spark.read.parquet("s3://raw/events/")

df.select("user_id").distinct().count()  # â†’ 10,000,000
df.select("event_type").distinct().count()  # â†’ 50
df.select(to_date("timestamp").alias("date")).distinct().count()  # â†’ 365

# Step 2: TODO - Design partition strategy
# Consider: date, event_type, hash(user_id)?

# Step 3: TODO - Implement partitioning

# Step 4: TODO - Apply Z-ORDER

# Step 5: TODO - Benchmark queries
```

**Success Criteria:**
- Total partitions < 10,000
- Date range queries skip > 90% of data
- Multi-column queries skip > 80% of data
- Query time < 5 seconds for 7-day range

**Stretch Goal:** Implement partition health monitoring dashboard.

---

### Exercise 2: Fix Over-Partitioned Table (2-3 hours)

**Objective:** Remediate a table with millions of partitions.

**Scenario:**
- Table partitioned by `user_id` (10M partitions)
- Total size: 2 TB
- Average partition size: 200 KB (way too small)
- Query time: 10 minutes (should be < 1 minute)

**Tasks:**
1. Measure current partition distribution
2. Design hash-based partitioning scheme (target: 1K partitions)
3. Migrate data to new partitioning
4. Validate row count and data integrity
5. Benchmark queries before/after
6. Calculate cost savings (S3 API calls)

**Starter Code:**
```python
# Step 1: Analyze current state
df = spark.read.format("delta").load("s3://data/over_partitioned/")

# TODO: Count partitions, measure sizes

# Step 2: Design hash partitioning
# TODO: Calculate target bucket count

# Step 3: Migrate data
# TODO: Add hash column, repartition

# Step 4: Validate
# TODO: Compare row counts

# Step 5: Benchmark
# TODO: Measure query time improvement
```

**Success Criteria:**
- Partitions: 10M â†’ 1K (10,000x reduction)
- Average partition size: 200 KB â†’ 2 MB (10x larger)
- Query time: 10 min â†’ < 30 sec (20x faster)
- Row count validation: 100% match

**Stretch Goal:** Implement automated partition consolidation pipeline.

---

### Exercise 3: Streaming Partition Management (3-4 hours)

**Objective:** Implement partition management for streaming data.

**Scenario:**
- Kafka stream ingestion (100 events/sec)
- Hourly partitions
- Data retention: 90 days
- Need: Automatic compaction and archival

**Tasks:**
1. Set up streaming write with dynamic partitioning
2. Enable auto-optimize for compaction
3. Implement hourly compaction job
4. Create partition lifecycle policy (archive after 30 days, delete after 90 days)
5. Monitor file count and partition size over time
6. Benchmark query performance on recent vs archived data

**Starter Code:**
```python
# Step 1: Streaming write
stream = spark.readStream.format("kafka").load()

# TODO: Add partition columns (date, hour)
# TODO: Write with partitionBy

# Step 2: Auto-optimize
# TODO: Enable Delta Lake auto-compaction

# Step 3: Compaction job
# TODO: Implement scheduled compaction

# Step 4: Lifecycle policy
# TODO: Archive old partitions to Glacier

# Step 5: Monitoring
# TODO: Track file count per partition
```

**Success Criteria:**
- Files per partition: < 100 (auto-compaction working)
- Partitions created: 1 per hour (dynamic partitioning working)
- Old partitions archived to S3 Glacier
- Recent queries (last 7 days) < 10 sec
- Archived queries (30-90 days) < 60 sec

**Stretch Goal:** Implement time-weighted archival (more recent = more frequent compaction).

---

### Exercise 4: Multi-Dimensional Partitioning Optimization (2-3 hours)

**Objective:** Optimize a poorly partitioned multi-dimensional dataset.

**Scenario:**
- Current partitioning: `date`, `region`, `category` (365 Ã— 10 Ã— 100 = 365K partitions)
- Problem: Too many partitions, small file sizes
- Query patterns: 70% filter by date, 20% by date+region, 10% by date+category

**Tasks:**
1. Reduce to 2-level partitioning (date, region)
2. Apply Z-ORDER on category
3. Measure partition pruning before/after
4. Benchmark all three query patterns
5. Calculate storage savings from larger files

**Starter Code:**
```python
# Step 1: Read current table
df = spark.read.format("delta").load("s3://data/multi_partitioned/")

# TODO: Analyze partition distribution

# Step 2: Repartition to date + region only
# TODO: Implement

# Step 3: Apply Z-ORDER on category
# TODO: Implement

# Step 4: Benchmark
# TODO: Test all query patterns

# Step 5: Calculate savings
# TODO: Compare file counts, storage costs
```

**Success Criteria:**
- Partitions: 365K â†’ 3,650 (100x reduction)
- Date queries: Same performance
- Date+region queries: < 10% slower (acceptable trade-off)
- Date+category queries: 50% faster (thanks to Z-ORDER)

**Stretch Goal:** Implement dynamic partition strategy based on query patterns.

---

## ğŸ”— Related Concepts

### Within This Course

- **[02.02] Data Ingestion Patterns**: Batch vs streaming impacts partitioning strategy
- **[02.03] Feature Engineering Pipelines**: Partitioning for feature freshness and incremental updates
- **[03.02] Lambda vs Kappa Architectures**: Partitioning in hybrid batch/streaming systems
- **[04.01] Data Lakes, Warehouses, Lakehouses**: Table formats (Delta, Iceberg) enable better partitioning
- **[04.02] Object Storage**: S3 directory structure and partitioning
- **[04.04] Storage Optimization**: File sizing impacts partition size
- **[04.06] Cost Optimization**: Partition pruning reduces query costs

### External Technologies

**Table Formats:**
- **Delta Lake**: ACID transactions + Z-ORDER (https://delta.io/)
- **Apache Iceberg**: Hidden partitioning + partition evolution (https://iceberg.apache.org/)
- **Apache Hudi**: Incremental processing + partition pruning (https://hudi.apache.org/)

**Query Engines:**
- **Apache Spark**: Partition pruning and dynamic partitioning
- **Presto/Trino**: Distributed query with partition awareness
- **AWS Athena**: Pay-per-scan pricing (partition pruning = cost savings)
- **BigQuery**: Clustered tables (similar to Z-ORDER)

**Optimization Techniques:**
- **Z-ORDER clustering**: Space-filling curves for multi-dimensional data
- **Hilbert curves**: Alternative to Z-ORDER for spatial data
- **Bloom filters**: Probabilistic data structures for data skipping

### Academic Foundations

- **Hive Partitioning**: Original design for Hadoop data warehouses
- **R-trees and Space-Filling Curves**: Foundations of Z-ORDER
- **Partition Pruning**: Query optimization technique from databases
- **LSM-trees**: Compaction strategies applicable to partition consolidation

---

## ğŸ“š Further Reading

### Books

1. **"Designing Data-Intensive Applications" by Martin Kleppmann** (Chapter 3: Storage and Retrieval)
   - Indexing, partitioning, and replication strategies
   - Page: 89-139

2. **"Fundamentals of Data Engineering" by Joe Reis & Matt Housley** (Chapter 7: Storage)
   - Practical partitioning for data lakes
   - Page: 187-220

3. **"High Performance Spark" by Holden Karau & Rachel Warren** (Chapter 6: Partitioning)
   - Spark-specific partitioning strategies
   - Page: 149-180

4. **"Learning Spark, 2nd Edition"** (Chapter 8: Partitioning and Bucketing)
   - Detailed guide to Spark partitioning
   - Page: 195-224

### Papers

1. **"Delta Lake: High-Performance ACID Table Storage"** (2020)
   - Z-ORDER and data skipping techniques
   - https://databricks.com/research/delta-lake

2. **"Apache Iceberg: A Table Format for Huge Analytic Datasets"** (2020)
   - Hidden partitioning and partition evolution
   - https://iceberg.apache.org/

3. **"Optimizing Space Amplification in RocksDB"** (2017)
   - Compaction strategies for LSM-trees
   - https://arxiv.org/abs/1708.00147

### Documentation

1. **Delta Lake Optimization Guide**
   - Z-ORDER, auto-optimize, partition management
   - https://docs.delta.io/latest/optimizations-oss.html

2. **Spark SQL Partitioning Guide**
   - Dynamic partitioning, bucketing, partition pruning
   - https://spark.apache.org/docs/latest/sql-data-sources-parquet.html

3. **Iceberg Partitioning**
   - Hidden partitioning and partition evolution
   - https://iceberg.apache.org/docs/latest/partitioning/

### Blog Posts

1. **"Partitioning Strategies for Apache Spark"** (Databricks)
   - Best practices for partitioning ML datasets
   - https://databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html

2. **"Delta Lake Small File Compaction"** (Databricks)
   - Auto-optimize and compaction strategies
   - https://databricks.com/blog/2020/09/29/diving-into-delta-lake-enforcing-and-evolving-the-schema.html

3. **"Iceberg Hidden Partitioning"** (Netflix)
   - Partition evolution without rewrites
   - https://netflixtechblog.com/

### Tools

1. **Delta Lake**: `pip install delta-spark`
2. **PyIceberg**: `pip install pyiceberg`
3. **Apache Hudi**: https://hudi.apache.org/docs/quick-start-guide

---

## ğŸ“ Key Takeaways

1. **Partition by low-cardinality, frequently filtered columns**: Choose columns like `date`, `region`, `model_version` with < 10K unique values. Avoid high-cardinality columns like `user_id` (use Z-ORDER instead).

2. **Target 128 MB - 1 GB per partition**: Larger partitions improve compression and reduce metadata overhead. Smaller partitions enable finer-grained pruning. Find the sweet spot based on query patterns.

3. **Limit total partitions to < 10K**: More partitions = exponential metadata overhead. Calculate total partitions: `cardinality(col1) Ã— cardinality(col2) Ã— ...`. Use Z-ORDER for additional dimensions.

4. **Use daily partitions for batch ML training**: Daily granularity balances query pruning and partition count. Use hourly for streaming, monthly for archival.

5. **Apply Z-ORDER for multi-column filters**: When queries filter by `date AND user_id`, partition by `date` and Z-ORDER by `user_id`. Reduces data scanned by 80-95%.

6. **Enable auto-optimize for streaming tables**: Streaming writes create millions of small files without compaction. Delta Lake auto-optimize consolidates files automatically.

7. **Avoid functions on partition columns**: `WHERE year(date) = 2025` disables partition pruning. Use `WHERE date >= '2025-01-01' AND date < '2026-01-01'` instead.

8. **Bucket tables with same count for shuffle-free joins**: Bucketing by `user_id` with 200 buckets on both tables eliminates shuffle. 10-100x faster for large joins.

9. **Measure partition pruning effectiveness**: Track "bytes scanned" vs "table size". Target > 90% pruning for partitioned queries. Use `EXPLAIN` to verify.

10. **Plan partition evolution upfront**: Changing partition granularity (daily â†’ hourly) requires data rewrite. Use Iceberg for easier evolution, or plan migration strategy carefully.

---

## âœï¸ Notes Section

**Personal Insights:**

---

**Questions:**

---

**Action Items:**

---

**Related Projects:**

---

**Code Snippets:**

---

**Further Exploration:**

---

**ğŸ“Œ Tags:** `#partitioning` `#delta-lake` `#z-order` `#data-organization` `#ml-systems` `#query-optimization` `#bucketing` `#partition-pruning` `#streaming` `#iceberg`

---

**Navigation:**
- â† Previous: [04.04 - Storage Optimization for ML Workloads](04.%20Storage%20Optimization%20for%20ML%20Workloads.md)
- â†’ Next: [04.06 - Cost Optimization Techniques](06.%20Cost%20Optimization%20Techniques.md)
- â†‘ Up: [Chapter 04 - Data Storage for ML](README.md)
- âŒ‚ Home: [DEforAI Course Index](../../README.md)

---

*Last updated: 2025-10-19*
