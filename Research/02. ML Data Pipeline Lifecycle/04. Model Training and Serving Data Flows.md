# 04. Model Training and Serving Data Flows

**Chapter:** ML Data Pipeline Lifecycle
**Topic:** Architecting data pipelines for model training and production inference

---

## 📋 Overview

The journey from raw data to production predictions requires carefully designed data flows for both training and serving. This subchapter examines the architectural patterns, infrastructure choices, and best practices for building scalable, reliable ML pipelines that maintain consistency across development and production environments.

**Key Industry Evolution (2024-2025):** The Feature-Training-Inference (FTI) pipeline pattern has become the standard architecture, separating concerns into three independent, loosely coupled pipelines. Organizations are increasingly adopting KServe as the de facto standard for model serving on Kubernetes, while Ray Serve has emerged for scaling complex microservices.

**Critical Challenge:** Training-serving skew remains one of the top causes of ML system failure in production, occurring when data flows differ between training and inference environments.

---

## 🎯 Learning Objectives

After completing this subchapter, you will be able to:
- Design and implement FTI (Feature-Training-Inference) pipeline architecture
- Choose appropriate serving patterns (batch, online, streaming)
- Build model serving microservices with proper infrastructure
- Implement deployment strategies (canary, blue-green, A/B testing)
- Prevent training-serving skew through unified data flows
- Optimize inference latency and throughput
- Manage model artifacts, versioning, and metadata
- Monitor data quality throughout training and serving pipelines

---

## 📚 Core Concepts

### 1. FTI Pipeline Architecture

#### The Three-Pipeline Pattern

**Modern ML systems separate into three independent pipelines:**

```
┌──────────────────────────────────────────────────────────────┐
│                 FEATURE PIPELINE                              │
│  Raw Data → Transformations → Feature Store (Offline/Online) │
│  (Batch & Streaming)                                         │
└──────────────────────────────────────────────────────────────┘
                             ↓
┌──────────────────────────────────────────────────────────────┐
│                 TRAINING PIPELINE                             │
│  Features + Labels → Train → Evaluate → Model Registry       │
│  (Scheduled or Event-Driven)                                 │
└──────────────────────────────────────────────────────────────┘
                             ↓
┌──────────────────────────────────────────────────────────────┐
│                 INFERENCE PIPELINE                            │
│  Request → Feature Retrieval → Prediction → Response         │
│  (Batch, Online, or Streaming)                               │
└──────────────────────────────────────────────────────────────┘
```

**Benefits:**
- **Independent Development:** Teams can iterate on each pipeline separately
- **Scalability:** Scale training and serving infrastructure independently
- **Reusability:** Features computed once, used in training and inference
- **Testability:** Unit test each pipeline in isolation

**Shared Storage Layer:**
- **Feature Store:** Offline (historical features) + Online (real-time features)
- **Model Registry:** Versioned models with metadata and lineage
- **Experiment Tracking:** MLflow, Weights & Biases, Neptune

---

#### Training Data Pipeline

**Purpose:** Transform raw data and features into training-ready datasets.

**Architecture:**

```python
from feast import FeatureStore
from sklearn.model_selection import train_test_split
import mlflow
import pandas as pd

class TrainingDataPipeline:
    """
    Orchestrate training dataset creation with point-in-time correctness
    """

    def __init__(self, feature_store_path: str):
        self.feature_store = FeatureStore(repo_path=feature_store_path)

    def create_training_dataset(
        self,
        entity_df: pd.DataFrame,
        feature_views: List[str],
        label_col: str,
        dataset_version: str
    ) -> dict:
        """
        Create point-in-time correct training dataset

        Args:
            entity_df: DataFrame with entity IDs and event_timestamp
            feature_views: List of feature views to retrieve
            label_col: Column name containing labels
            dataset_version: Version identifier for dataset

        Returns:
            Dict with train/val/test splits and metadata
        """
        # Get historical features with point-in-time joins
        training_df = self.feature_store.get_historical_features(
            entity_df=entity_df,
            features=feature_views
        ).to_df()

        # Validate data quality
        self._validate_dataset(training_df)

        # Split data
        train_df, temp_df = train_test_split(
            training_df,
            test_size=0.3,
            random_state=42,
            stratify=training_df[label_col] if label_col in training_df else None
        )
        val_df, test_df = train_test_split(
            temp_df,
            test_size=0.5,
            random_state=42
        )

        # Compute dataset statistics
        stats = self._compute_stats(train_df, val_df, test_df)

        # Version and store
        dataset_path = f"s3://ml-datasets/training/{dataset_version}/"
        train_df.to_parquet(f"{dataset_path}train.parquet")
        val_df.to_parquet(f"{dataset_path}val.parquet")
        test_df.to_parquet(f"{dataset_path}test.parquet")

        # Log with MLflow
        with mlflow.start_run(run_name=f"dataset-{dataset_version}"):
            mlflow.log_param("dataset_version", dataset_version)
            mlflow.log_param("num_features", len(feature_views))
            mlflow.log_params(stats)
            mlflow.log_artifact(dataset_path)

        return {
            "train": train_df,
            "val": val_df,
            "test": test_df,
            "path": dataset_path,
            "stats": stats
        }

    def _validate_dataset(self, df: pd.DataFrame):
        """Validate dataset quality"""
        # Check for missing values
        null_pct = (df.isnull().sum() / len(df)) * 100
        if (null_pct > 10).any():
            problematic_cols = null_pct[null_pct > 10].to_dict()
            raise ValueError(f"Columns with >10% nulls: {problematic_cols}")

        # Check for data leakage indicators
        if 'event_timestamp' in df.columns and 'label_timestamp' in df.columns:
            future_data = df[df['event_timestamp'] > df['label_timestamp']]
            if len(future_data) > 0:
                raise ValueError(f"Data leakage detected: {len(future_data)} rows with future data")

        # Check for duplicates
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            print(f"⚠️  Warning: {duplicates} duplicate rows found")

    def _compute_stats(self, train_df, val_df, test_df) -> dict:
        """Compute dataset statistics for logging"""
        return {
            "train_rows": len(train_df),
            "val_rows": len(val_df),
            "test_rows": len(test_df),
            "train_label_dist": train_df.iloc[:, -1].value_counts().to_dict(),
            "num_features": len(train_df.columns) - 1
        }
```

**Key Principles:**

1. **Point-in-Time Correctness:** Use only data available before each label's timestamp
2. **Stratified Splitting:** Maintain label distribution across splits
3. **Data Validation:** Check for leakage, nulls, duplicates before training
4. **Versioning:** Track dataset version with models
5. **Reproducibility:** Seed random splits, log all parameters

---

### 2. Batch vs Online Inference

#### Batch Inference (Offline Predictions)

**Definition:** Generate predictions for a large batch of data asynchronously, storing results for later retrieval.

**Characteristics:**
- **Latency:** Minutes to hours acceptable
- **Throughput:** Very high (millions of predictions)
- **Cost:** Most cost-effective for high volume
- **Infrastructure:** Spark clusters, scheduled jobs
- **Storage:** Predictions cached in database

**Use Cases:**
- Recommendation systems (precompute top-N for all users)
- Lead scoring (daily batch of prospects)
- Churn prediction (weekly risk scores)
- Email marketing (segment users overnight)

**Architecture:**

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
import mlflow

class BatchInferencePipeline:
    """
    Batch inference pipeline using Spark for scalability
    """

    def __init__(self, model_uri: str, feature_store: FeatureStore):
        self.spark = SparkSession.builder \
            .appName("BatchInference") \
            .config("spark.executor.memory", "8g") \
            .getOrCreate()

        # Load model
        self.model = mlflow.sklearn.load_model(model_uri)
        self.feature_store = feature_store

    def run_batch_inference(
        self,
        entity_ids: List[str],
        prediction_date: str,
        output_table: str
    ):
        """
        Generate predictions for all entities in batch
        """
        # Create entity DataFrame
        entity_df = pd.DataFrame({
            "entity_id": entity_ids,
            "event_timestamp": pd.to_datetime(prediction_date)
        })

        # Retrieve features (from offline store)
        features_df = self.feature_store.get_historical_features(
            entity_df=entity_df,
            features=[
                "user_features:transactions_30d",
                "user_features:total_spent_30d",
                "user_features:avg_basket_size"
            ]
        ).to_df()

        # Convert to Spark DataFrame for parallel processing
        features_spark = self.spark.createDataFrame(features_df)

        # Predict in parallel
        predictions_spark = features_spark.rdd.mapPartitions(
            lambda partition: self._predict_partition(partition)
        ).toDF()

        # Write predictions to database
        predictions_spark.write \
            .mode("overwrite") \
            .format("jdbc") \
            .option("url", f"jdbc:postgresql://db:5432/{output_table}") \
            .option("dbtable", f"predictions_{prediction_date}") \
            .save()

        # Log metrics
        total_predictions = predictions_spark.count()
        avg_score = predictions_spark.select(F.avg("score")).collect()[0][0]

        print(f"✅ Batch inference complete:")
        print(f"   - Total predictions: {total_predictions}")
        print(f"   - Average score: {avg_score:.4f}")
        print(f"   - Output table: predictions_{prediction_date}")

    def _predict_partition(self, partition):
        """Predict on a partition of data"""
        rows = list(partition)
        features = [row.features for row in rows]
        predictions = self.model.predict_proba(features)[:, 1]

        for row, pred in zip(rows, predictions):
            yield {
                "entity_id": row.entity_id,
                "score": float(pred),
                "prediction_date": row.event_timestamp,
                "model_version": self.model.metadata.version
            }


# Airflow DAG for scheduled batch inference
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'ml-team',
    'start_date': datetime(2025, 1, 1),
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'daily_batch_inference',
    default_args=default_args,
    schedule_interval='0 2 * * *',  # 2 AM daily
    catchup=False
) as dag:

    def run_inference(**context):
        pipeline = BatchInferencePipeline(
            model_uri="models:/churn-predictor/production",
            feature_store=FeatureStore(repo_path=".")
        )

        # Get all active users
        entity_ids = fetch_active_user_ids()  # Query database

        pipeline.run_batch_inference(
            entity_ids=entity_ids,
            prediction_date=context['ds'],
            output_table="churn_predictions"
        )

    inference_task = PythonOperator(
        task_id='batch_inference',
        python_callable=run_inference
    )
```

**Pros:**
- ✅ Cost-effective for high volume
- ✅ Can use complex models (high compute)
- ✅ Simple infrastructure

**Cons:**
- ❌ High latency (not real-time)
- ❌ Stale predictions (updated daily/hourly)
- ❌ Storage overhead

---

#### Online Inference (Real-Time Predictions)

**Definition:** Generate predictions on-demand in response to individual requests with low latency.

**Characteristics:**
- **Latency:** <10ms to <100ms required
- **Throughput:** Lower (100s-1000s QPS per instance)
- **Cost:** Higher per prediction
- **Infrastructure:** Always-on API servers, load balancers
- **Serving:** Synchronous HTTP/gRPC endpoints

**Use Cases:**
- Fraud detection (real-time transaction scoring)
- Search ranking (personalized results)
- Real-time recommendations (dynamic product suggestions)
- Ad serving (bidding decisions in milliseconds)

**Architecture:**

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from feast import FeatureStore
import mlflow
import uvicorn
from typing import Dict, List
import numpy as np

app = FastAPI(title="ML Model Serving API")

# Load model and feature store at startup
model = None
feature_store = None

@app.on_event("startup")
async def load_model():
    """Load model and feature store on server startup"""
    global model, feature_store

    model = mlflow.sklearn.load_model("models:/fraud-detector/production")
    feature_store = FeatureStore(repo_path="/opt/feast/")

    print("✅ Model loaded successfully")
    print(f"   - Model version: {model.metadata.version}")
    print(f"   - Model URI: models:/fraud-detector/production")


class PredictionRequest(BaseModel):
    """Request schema for predictions"""
    entity_id: str
    context: Dict = {}


class PredictionResponse(BaseModel):
    """Response schema for predictions"""
    entity_id: str
    score: float
    prediction: int
    model_version: str
    latency_ms: float


@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """
    Real-time prediction endpoint

    Latency budget: <50ms p99
    """
    import time
    start_time = time.time()

    try:
        # 1. Retrieve features from online store (<10ms)
        online_features = feature_store.get_online_features(
            entity_rows=[{"user_id": request.entity_id}],
            features=[
                "user_features:transactions_30d",
                "user_features:total_spent_30d",
                "fraud_features:velocity_5min",
                "fraud_features:unique_devices"
            ]
        ).to_dict()

        # 2. Compute on-demand features (<1ms)
        contextual_features = compute_contextual_features(request.context)

        # 3. Combine features
        all_features = {**online_features, **contextual_features}
        feature_vector = np.array([all_features.values()])

        # 4. Predict (<5ms)
        score = model.predict_proba(feature_vector)[0, 1]
        prediction = int(score > 0.5)

        # 5. Log for monitoring
        latency_ms = (time.time() - start_time) * 1000

        return PredictionResponse(
            entity_id=request.entity_id,
            score=float(score),
            prediction=prediction,
            model_version=model.metadata.version,
            latency_ms=latency_ms
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """Health check endpoint for load balancer"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "feature_store_connected": feature_store is not None
    }


def compute_contextual_features(context: Dict) -> Dict:
    """Compute features from request context"""
    return {
        "hour_of_day": datetime.now().hour,
        "is_weekend": int(datetime.now().weekday() >= 5),
        "device_type": context.get("device_type", "unknown"),
        "ip_country": context.get("ip_country", "unknown")
    }


# Run server
if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=4  # Multi-process for concurrency
    )
```

**Deployment with Docker + Kubernetes:**

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fraud-detection-model
  namespace: ml-production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: fraud-detection
  template:
    metadata:
      labels:
        app: fraud-detection
        version: v2.1.0
    spec:
      containers:
      - name: model-server
        image: ml-registry.company.com/fraud-detection:v2.1.0
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_URI
          value: "models:/fraud-detector/production"
        - name: FEAST_REPO_PATH
          value: "/opt/feast/"
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2000m"
            memory: "4Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: fraud-detection-service
  namespace: ml-production
spec:
  type: LoadBalancer
  selector:
    app: fraud-detection
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fraud-detection-hpa
  namespace: ml-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fraud-detection-model
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
```

**Pros:**
- ✅ Low latency (<100ms)
- ✅ Fresh predictions
- ✅ Real-time data incorporation

**Cons:**
- ❌ Higher cost per prediction
- ❌ Complex infrastructure (autoscaling, load balancing)
- ❌ Model size constraints

---

### 3. Model Serving Patterns

#### Pattern 1: Embedded Model (Monolithic)

**Description:** Model embedded directly in application code.

**Architecture:**
```
┌────────────────────────────────┐
│     Application Server         │
│  ┌─────────────────────────┐  │
│  │  Application Logic      │  │
│  ├─────────────────────────┤  │
│  │  ML Model (in-memory)   │  │
│  └─────────────────────────┘  │
└────────────────────────────────┘
```

**Pros:**
- Lowest latency (no network calls)
- Simple deployment
- No additional infrastructure

**Cons:**
- Tight coupling (redeploying app to update model)
- Resource contention
- Hard to scale model independently

**When to Use:** Simple models, low traffic, prototypes

---

#### Pattern 2: Model as Microservice

**Description:** Model served as independent service, called via API.

**Architecture:**
```
┌──────────────┐         ┌──────────────────┐
│ Application  │─ HTTP ─→│  Model Service   │
│   Server     │         │  (FastAPI/gRPC)  │
└──────────────┘         └──────────────────┘
                                  │
                          ┌───────┴────────┐
                          │  Load Balancer │
                          └───────┬────────┘
                     ┌────────────┼────────────┐
                     ↓            ↓            ↓
                ┌─────────┐ ┌─────────┐ ┌─────────┐
                │ Model-1 │ │ Model-2 │ │ Model-3 │
                └─────────┘ └─────────┘ └─────────┘
```

**Pros:**
- Independent scaling
- Model updates without app redeployment
- Technology flexibility (Python ML, Java app)

**Cons:**
- Added network latency
- More infrastructure complexity
- Additional failure points

**When to Use:** Production systems, multiple models, frequent updates

---

#### Pattern 3: Serverless Inference

**Description:** Model served via serverless functions (AWS Lambda, Azure Functions).

**Implementation:**

```python
# lambda_handler.py
import json
import boto3
import numpy as np

# Load model from S3 at cold start
s3 = boto3.client('s3')
model = None

def lambda_handler(event, context):
    """AWS Lambda handler for model inference"""
    global model

    # Load model if not cached (cold start)
    if model is None:
        model_bytes = s3.get_object(
            Bucket='ml-models',
            Key='fraud-detector/v2.1.0/model.pkl'
        )['Body'].read()
        model = pickle.loads(model_bytes)

    # Parse request
    request_data = json.loads(event['body'])

    # Get features
    features = get_features_from_dynamodb(request_data['entity_id'])

    # Predict
    prediction = model.predict_proba([features])[0, 1]

    return {
        'statusCode': 200,
        'body': json.dumps({
            'score': float(prediction),
            'entity_id': request_data['entity_id']
        })
    }
```

**Pros:**
- Zero infrastructure management
- Auto-scaling
- Pay-per-request pricing

**Cons:**
- Cold start latency (100ms-1s)
- Execution time limits (15 min AWS Lambda)
- Memory/package size limits

**When to Use:** Sporadic traffic, cost-sensitive deployments, simple models

---

### 4. Deployment Strategies

#### Canary Deployment

**Description:** Gradually route traffic to new model version while monitoring metrics.

```python
# canary.py
from typing import Dict
import random

class CanaryDeployment:
    """
    Gradually shift traffic from old model to new model
    """

    def __init__(self, model_v1, model_v2, initial_traffic_pct: float = 5.0):
        self.model_v1 = model_v1
        self.model_v2 = model_v2
        self.traffic_pct = initial_traffic_pct  # % to new model

    def predict(self, features: Dict) -> Dict:
        """Route request to old or new model based on traffic split"""

        # Route to new model with probability = traffic_pct
        use_new_model = random.random() * 100 < self.traffic_pct

        if use_new_model:
            prediction = self.model_v2.predict(features)
            model_version = "v2.0"
        else:
            prediction = self.model_v1.predict(features)
            model_version = "v1.0"

        # Log for monitoring
        log_prediction(
            prediction=prediction,
            model_version=model_version,
            features=features
        )

        return {
            "prediction": prediction,
            "model_version": model_version
        }

    def increase_traffic(self, increment: float = 5.0):
        """Incrementally increase traffic to new model"""
        self.traffic_pct = min(100.0, self.traffic_pct + increment)
        print(f"📊 Canary traffic increased to {self.traffic_pct}%")

    def rollback(self):
        """Rollback to old model"""
        self.traffic_pct = 0.0
        print("🔄 Canary rolled back to v1.0")


# Deployment script
def deploy_canary():
    """
    Canary deployment workflow:
    - Start with 5% traffic
    - Monitor for 1 hour
    - Increase by 10% every hour if metrics are good
    - Rollback if error rate spikes
    """
    canary = CanaryDeployment(model_v1, model_v2, initial_traffic_pct=5.0)

    stages = [5, 15, 30, 50, 75, 100]

    for target_pct in stages:
        canary.traffic_pct = target_pct
        print(f"📊 Canary at {target_pct}%")

        # Monitor for 1 hour
        metrics = monitor_canary_metrics(duration_minutes=60)

        # Check success criteria
        if metrics['error_rate'] > 0.01 or metrics['p99_latency'] > 100:
            print("❌ Canary metrics degraded, rolling back")
            canary.rollback()
            return False

        print(f"✅ Canary metrics healthy at {target_pct}%")

    print("🎉 Canary deployment complete, v2.0 at 100%")
    return True
```

---

#### A/B Testing

**Description:** Run multiple model versions in parallel, measure business impact.

```python
class ABTestFramework:
    """
    A/B test framework for comparing model performance
    """

    def __init__(self, models: Dict[str, Any], assignment_strategy: str = "random"):
        self.models = models  # {"control": model_v1, "treatment": model_v2}
        self.assignment_strategy = assignment_strategy
        self.assignments = {}  # user_id -> variant

    def assign_variant(self, user_id: str) -> str:
        """Assign user to variant (deterministic)"""
        if user_id in self.assignments:
            return self.assignments[user_id]

        # Hash user_id for deterministic assignment
        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        variant = "control" if hash_value % 2 == 0 else "treatment"

        self.assignments[user_id] = variant
        return variant

    def predict(self, user_id: str, features: Dict) -> Dict:
        """Get prediction from assigned variant"""
        variant = self.assign_variant(user_id)
        model = self.models[variant]

        prediction = model.predict(features)

        # Log for analysis
        log_ab_test_event(
            user_id=user_id,
            variant=variant,
            prediction=prediction,
            timestamp=datetime.now()
        )

        return {
            "prediction": prediction,
            "variant": variant
        }

    def analyze_results(self, metric: str = "conversion_rate") -> Dict:
        """
        Statistical analysis of A/B test results

        Uses two-proportion z-test for conversion metrics
        """
        from scipy import stats

        control_data = get_ab_test_data(variant="control", metric=metric)
        treatment_data = get_ab_test_data(variant="treatment", metric=metric)

        # Two-proportion z-test
        control_conversions = control_data['conversions']
        control_total = control_data['total']
        treatment_conversions = treatment_data['conversions']
        treatment_total = treatment_data['total']

        z_stat, p_value = stats.proportions_ztest(
            count=[control_conversions, treatment_conversions],
            nobs=[control_total, treatment_total]
        )

        # Calculate lift
        control_rate = control_conversions / control_total
        treatment_rate = treatment_conversions / treatment_total
        lift = ((treatment_rate - control_rate) / control_rate) * 100

        return {
            "control_rate": control_rate,
            "treatment_rate": treatment_rate,
            "lift_pct": lift,
            "p_value": p_value,
            "statistically_significant": p_value < 0.05,
            "winner": "treatment" if lift > 0 and p_value < 0.05 else "control"
        }
```

---

## 💡 Practical Examples

### Example 1: End-to-End Recommendation System

**Scenario:** E-commerce product recommendations with batch and online inference

**Architecture:**

```python
class RecommendationSystem:
    """
    Hybrid recommendation system with batch and online components
    """

    def __init__(self):
        self.feature_store = FeatureStore(repo_path=".")
        self.collaborative_model = mlflow.sklearn.load_model("models:/collab-filter/prod")
        self.ranking_model = mlflow.sklearn.load_model("models:/ranking/prod")
        self.redis_client = redis.Redis(host='localhost', port=6379)

    def batch_candidate_generation(self, user_ids: List[str], date: str):
        """
        Nightly batch job: Generate top-100 candidate products per user
        """
        from pyspark.sql import SparkSession

        spark = SparkSession.builder.appName("CandidateGeneration").getOrCreate()

        # Load interaction matrix
        interactions = spark.read.parquet("s3://data/interactions/")

        # Compute collaborative filtering scores
        for user_id in user_ids:
            # Get user embedding
            user_embedding = self.collaborative_model.get_user_embedding(user_id)

            # Find similar items
            candidate_products = self.collaborative_model.get_top_n(
                user_embedding,
                n=100
            )

            # Cache in Redis for fast lookup
            self.redis_client.setex(
                f"candidates:{user_id}",
                86400,  # 24 hour TTL
                json.dumps(candidate_products)
            )

        print(f"✅ Generated candidates for {len(user_ids)} users")

    def online_ranking(self, user_id: str, context: Dict) -> List[str]:
        """
        Real-time: Rank candidates with personalization
        """
        # 1. Retrieve candidates from cache (<1ms)
        candidates_json = self.redis_client.get(f"candidates:{user_id}")
        if not candidates_json:
            # Fallback to popular items
            candidates = self._get_popular_items()
        else:
            candidates = json.loads(candidates_json)

        # 2. Get real-time features (<10ms)
        user_features = self.feature_store.get_online_features(
            entity_rows=[{"user_id": user_id}],
            features=["user_features:recent_categories", "user_features:price_range"]
        ).to_dict()

        product_features = self.feature_store.get_online_features(
            entity_rows=[{"product_id": pid} for pid in candidates],
            features=["product_features:popularity", "product_features:category"]
        ).to_df()

        # 3. Compute contextual features (<1ms)
        contextual = {
            "hour_of_day": datetime.now().hour,
            "device": context.get("device", "desktop"),
            "page": context.get("page", "home")
        }

        # 4. Rank with ML model (<5ms)
        feature_matrix = self._create_feature_matrix(
            user_features,
            product_features,
            contextual
        )

        scores = self.ranking_model.predict_proba(feature_matrix)[:, 1]

        # 5. Sort and return top-10
        ranked_products = [
            pid for pid, _ in sorted(
                zip(candidates, scores),
                key=lambda x: x[1],
                reverse=True
            )[:10]
        ]

        return ranked_products
```

---

### Example 2: Fraud Detection with Real-Time and Batch Components

```python
class FraudDetectionSystem:
    """
    Multi-stage fraud detection with streaming features and batch retraining
    """

    def __init__(self):
        self.feature_store = FeatureStore(repo_path=".")
        self.model = mlflow.sklearn.load_model("models:/fraud-detector/production")

    def streaming_feature_pipeline(self):
        """
        Flink streaming job: Compute real-time velocity features
        """
        from pyflink.datastream import StreamExecutionEnvironment
        from pyflink.table import StreamTableEnvironment

        env = StreamExecutionEnvironment.get_execution_environment()
        t_env = StreamTableEnvironment.create(env)

        # Define transaction stream
        t_env.execute_sql("""
            CREATE TABLE transactions (
                transaction_id STRING,
                user_id STRING,
                amount DOUBLE,
                merchant_id STRING,
                timestamp TIMESTAMP(3),
                WATERMARK FOR timestamp AS timestamp - INTERVAL '10' SECOND
            ) WITH (
                'connector' = 'kafka',
                'topic' = 'transactions',
                'properties.bootstrap.servers' = 'kafka:9092',
                'format' = 'json'
            )
        """)

        # Compute velocity features
        t_env.execute_sql("""
            CREATE TABLE user_velocity AS
            SELECT
                user_id,
                TUMBLE_END(timestamp, INTERVAL '5' MINUTE) as window_end,
                COUNT(*) as transactions_5min,
                SUM(amount) as total_amount_5min,
                COUNT(DISTINCT merchant_id) as unique_merchants_5min
            FROM transactions
            GROUP BY user_id, TUMBLE(timestamp, INTERVAL '5' MINUTE)
        """)

        # Sink to feature store
        t_env.execute_sql("""
            CREATE TABLE feature_sink (
                user_id STRING,
                transactions_5min BIGINT,
                total_amount_5min DOUBLE,
                unique_merchants_5min BIGINT,
                PRIMARY KEY (user_id) NOT ENFORCED
            ) WITH (
                'connector' = 'redis',
                'host' = 'redis',
                'port' = '6379',
                'key-pattern' = 'features:${user_id}'
            )
        """)

        t_env.execute_sql("""
            INSERT INTO feature_sink
            SELECT user_id, transactions_5min, total_amount_5min, unique_merchants_5min
            FROM user_velocity
        """)

    def real_time_scoring(self, transaction: Dict) -> Dict:
        """
        Score transaction in real-time (<50ms)
        """
        # Get streaming features
        velocity_features = self.redis_client.hgetall(f"features:{transaction['user_id']}")

        # Get batch features from feature store
        batch_features = self.feature_store.get_online_features(
            entity_rows=[{"user_id": transaction['user_id']}],
            features=["user_features:avg_transaction_amount", "user_features:fraud_history"]
        ).to_dict()

        # Combine features
        all_features = {**velocity_features, **batch_features}

        # Predict
        fraud_score = self.model.predict_proba([all_features])[0, 1]

        # Decision logic
        if fraud_score > 0.9:
            action = "BLOCK"
        elif fraud_score > 0.5:
            action = "REVIEW"
        else:
            action = "APPROVE"

        return {
            "transaction_id": transaction['transaction_id'],
            "fraud_score": float(fraud_score),
            "action": action,
            "model_version": self.model.metadata.version
        }
```

---

## 🔧 Code Examples

### Complete Training Pipeline with MLflow

```python
import mlflow
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, precision_recall_curve
import pandas as pd

class ModelTrainingPipeline:
    """
    End-to-end training pipeline with experiment tracking
    """

    def __init__(self, experiment_name: str):
        mlflow.set_experiment(experiment_name)

    def train(
        self,
        train_df: pd.DataFrame,
        val_df: pd.DataFrame,
        hyperparams: Dict,
        model_name: str
    ):
        """
        Train model with full MLflow tracking
        """
        with mlflow.start_run() as run:
            # Log dataset info
            mlflow.log_param("train_rows", len(train_df))
            mlflow.log_param("val_rows", len(val_df))
            mlflow.log_param("num_features", len(train_df.columns) - 1)

            # Log hyperparameters
            mlflow.log_params(hyperparams)

            # Split features and labels
            X_train = train_df.drop("label", axis=1)
            y_train = train_df["label"]
            X_val = val_df.drop("label", axis=1)
            y_val = val_df["label"]

            # Train model
            model = RandomForestClassifier(**hyperparams)
            model.fit(X_train, y_train)

            # Evaluate
            train_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1])
            val_auc = roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])

            mlflow.log_metric("train_auc", train_auc)
            mlflow.log_metric("val_auc", val_auc)

            # Log model
            mlflow.sklearn.log_model(
                model,
                "model",
                registered_model_name=model_name
            )

            # Promote to production if better than current
            self._evaluate_promotion(model_name, val_auc, run.info.run_id)

            return model

    def _evaluate_promotion(self, model_name: str, val_auc: float, run_id: str):
        """Promote model to production if it beats current champion"""
        client = mlflow.tracking.MlflowClient()

        # Get current production model
        try:
            prod_version = client.get_latest_versions(model_name, stages=["Production"])[0]
            prod_run = client.get_run(prod_version.run_id)
            prod_auc = prod_run.data.metrics.get("val_auc", 0.0)

            if val_auc > prod_auc:
                # Promote new model
                client.transition_model_version_stage(
                    name=model_name,
                    version=prod_version.version,
                    stage="Archived"
                )
                print(f"✅ New model promoted: AUC {val_auc:.4f} > {prod_auc:.4f}")
            else:
                print(f"📊 Current production model still best: {prod_auc:.4f}")

        except IndexError:
            # No production model yet
            print("✅ First production model registered")
```

---

## ✅ Best Practices

### 1. Maintain Training-Serving Parity
- Use same feature computation code
- Use same data preprocessing logic
- Test features end-to-end in CI/CD
- Monitor for drift

### 2. Version Everything Together
- Dataset version + Model version + Feature version
- Store lineage in model registry
- Enable reproducible training
- Support rollbacks

### 3. Implement Comprehensive Monitoring
- Model performance (AUC, precision, recall)
- Inference latency (p50, p95, p99)
- Feature quality (drift, nulls, staleness)
- Business metrics (CTR, revenue)

### 4. Design for Failure
- Fallback models (simple heuristics)
- Circuit breakers for downstream services
- Graceful degradation
- Error budgets and SLAs

### 5. Optimize Inference Performance
- Model compression (quantization, pruning)
- Feature caching (Redis, in-memory)
- Batch prediction where possible
- GPU acceleration for deep learning

### 6. Use Canary Deployments
- Start with 5-10% traffic
- Monitor metrics for 1 hour per stage
- Auto-rollback on degradation
- Gradually increase to 100%

### 7. Separate Concerns (FTI Pattern)
- Feature pipeline: Reusable, independently scalable
- Training pipeline: Scheduled or event-driven
- Inference pipeline: Optimized for latency

### 8. Test in Production
- A/B testing for business metrics
- Shadow mode for new models
- Multi-armed bandits for exploration

---

## ⚠️ Common Pitfalls

### 1. Training-Serving Skew (Most Common)

**Problem:** Features computed differently in training vs serving.

**Solution:** See Feature Engineering chapter, use feature stores.

---

### 2. Not Monitoring Production Metrics

**Problem:** Model degrades silently without alerts.

**Example:**
```python
# ❌ BAD: No monitoring
def predict(features):
    return model.predict(features)

# ✅ GOOD: Log everything
def predict(features):
    prediction = model.predict(features)

    # Log to monitoring system
    monitor.log_prediction(
        prediction=prediction,
        features=features,
        model_version=MODEL_VERSION,
        timestamp=datetime.now()
    )

    return prediction
```

---

### 3. Serving Large Models Synchronously

**Problem:** 5GB model loaded in memory per instance = high cost.

**Solution:**
- Use batch inference for large models
- Model compression (distillation, quantization)
- Serverless with cold start caching

---

### 4. Not Implementing Rollback Mechanisms

**Problem:** Bad model deployed, no way to quickly revert.

**Solution:**
```python
class ModelRegistry:
    def rollback_to_previous_version(self, model_name: str):
        """Rollback to last known good version"""
        client = mlflow.tracking.MlflowClient()

        # Get current production
        current_prod = client.get_latest_versions(model_name, stages=["Production"])[0]

        # Get previous archived version
        archived = client.get_latest_versions(model_name, stages=["Archived"])
        if not archived:
            raise ValueError("No previous version to rollback to")

        previous_version = archived[0]

        # Swap versions
        client.transition_model_version_stage(
            name=model_name,
            version=current_prod.version,
            stage="Archived"
        )
        client.transition_model_version_stage(
            name=model_name,
            version=previous_version.version,
            stage="Production"
        )

        print(f"✅ Rolled back to v{previous_version.version}")
```

---

### 5. Ignoring Inference Latency

**Problem:** p99 latency > 1 second, poor user experience.

**Solution:**
- Set SLAs (e.g., p95 < 100ms)
- Profile and optimize hot paths
- Cache frequently accessed features
- Use async I/O where possible

---

## 🏋️ Hands-On Exercises

### Exercise 1: Build FTI Pipeline End-to-End

**Difficulty:** Advanced
**Time:** 10-12 hours

**Objective:** Implement complete Feature-Training-Inference pipeline for churn prediction.

**Tasks:**

1. **Feature Pipeline:**
   - Batch: Spark job to compute user aggregations
   - Streaming: Flink job for real-time activity
   - Store in Feast (offline + online)

2. **Training Pipeline:**
   - Create point-in-time correct dataset
   - Train RandomForest with MLflow tracking
   - Evaluate and register model

3. **Inference Pipeline:**
   - Build FastAPI serving endpoint
   - Deploy to Kubernetes
   - Implement health checks and monitoring

**Validation:**
- Training reproduces same dataset given date
- Serving latency p95 < 50ms
- No training-serving skew

---

### Exercise 2: Implement Canary Deployment

**Difficulty:** Intermediate
**Time:** 6-7 hours

**Objective:** Deploy new model version with gradual traffic shift.

**Requirements:**
- Start at 10% traffic
- Monitor error rate, latency, business metrics
- Auto-rollback if metrics degrade
- Promote to 100% if successful

**Deliverables:**
- Canary deployment script
- Monitoring dashboard
- Rollback runbook

---

### Exercise 3: Batch vs Online Inference Performance

**Difficulty:** Intermediate
**Time:** 5-6 hours

**Objective:** Compare batch and online inference for same model.

**Tasks:**
1. Implement batch pipeline (Spark)
2. Implement online API (FastAPI)
3. Benchmark:
   - Throughput (predictions/sec)
   - Latency (p50, p95, p99)
   - Cost ($/1000 predictions)

**Expected Results:**
- Batch: 100K+ predictions/sec, high latency, low cost
- Online: 100-1K predictions/sec, <100ms latency, higher cost

---

### Exercise 4: A/B Test Two Model Versions

**Difficulty:** Advanced
**Time:** 8-10 hours

**Objective:** Run A/B test comparing model v1 vs v2.

**Requirements:**
- 50/50 traffic split (deterministic by user_id)
- Track conversion rates for 1 week
- Statistical significance test (z-test)
- Declare winner

**Deliverables:**
- A/B test framework code
- Results analysis with p-values
- Recommendation (promote v2 or keep v1)

---

## 📚 Further Reading

### Essential Books

1. **"Machine Learning Engineering" by Andriy Burkov**
   - Chapter 7: Model Deployment
   - Chapter 8: Model Serving

2. **"Building Machine Learning Powered Applications" by Emmanuel Ameisen**
   - Chapter 8-11: Deployment and monitoring

3. **"Designing Data-Intensive Applications" by Martin Kleppmann**
   - Batch and stream processing patterns

### Key Papers

1. **"TFX: A TensorFlow-Based Production-Scale Machine Learning Platform"** (Google, 2017)
   - [Paper](https://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform)
   - End-to-end ML pipelines at Google scale

2. **"Continuous Training for Production ML in the TensorFlow Extended (TFX) Platform"** (Baylor et al., 2019)
   - [Paper](https://www.usenix.org/conference/opml19/presentation/baylor)
   - Automated retraining strategies

### Blogs and Documentation

1. **Uber Engineering: Michelangelo**
   - ["Meet Michelangelo: Uber's Machine Learning Platform"](https://www.uber.com/blog/michelangelo-machine-learning-platform/)
   - End-to-end ML platform architecture

2. **Netflix: Model Serving**
   - ["Open Sourcing Metaflow, a Human-Centric Framework for Data Science"](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9)

3. **KServe Documentation**
   - [Official KServe Docs](https://kserve.github.io/website/)
   - Model serving on Kubernetes

4. **Ray Serve**
   - [Ray Serve Docs](https://docs.ray.io/en/latest/serve/index.html)
   - Scalable model serving framework

5. **MLflow Model Registry**
   - [MLflow Registry](https://mlflow.org/docs/latest/model-registry.html)

### Video Courses

1. **"Full Stack Deep Learning" (UC Berkeley)**
   - [Course Website](https://fullstackdeeplearning.com/)
   - Production ML infrastructure

2. **"Machine Learning Engineering for Production (MLOps)" (Coursera - DeepLearning.AI)**
   - Deployment patterns and serving

---

## 📝 Key Takeaways

1. **FTI Architecture is the Standard**
   - Separate feature, training, and inference pipelines
   - Independent development and scaling
   - Shared storage layer (feature store, model registry)

2. **Choose Serving Pattern Based on Requirements**
   - **Batch:** High throughput, high latency, low cost
   - **Online:** Low latency, lower throughput, higher cost
   - **Hybrid:** Batch for candidates, online for ranking

3. **Training-Serving Parity is Critical**
   - Use same feature computation code
   - Validate consistency in CI/CD
   - Monitor for drift

4. **Model Registry is Essential**
   - Version models with metadata
   - Track lineage (data → features → model)
   - Enable rollbacks

5. **Deployment Requires Strategy**
   - Canary: Gradual traffic shift with monitoring
   - A/B Testing: Measure business impact
   - Blue-Green: Instant switchover

6. **Monitoring is Non-Negotiable**
   - Model metrics: AUC, precision, recall
   - System metrics: Latency, throughput, errors
   - Business metrics: Revenue, conversions, engagement

7. **Optimize for Latency**
   - Cache features in Redis
   - Model compression
   - Async I/O
   - GPU for deep learning

8. **Design for Failure**
   - Fallback models (heuristics)
   - Circuit breakers
   - Graceful degradation
   - Health checks and retries

9. **Infrastructure Matters**
   - Kubernetes for orchestration
   - Load balancers for high availability
   - Auto-scaling for variable traffic
   - Monitoring and alerting

10. **Batch and Online Complement Each Other**
    - Batch: Generate candidates, precompute embeddings
    - Online: Real-time ranking, personalization
    - Hybrid architectures are common in production

---

## 📝 Notes Section

### My Key Insights:
-

### Questions to Explore Further:
-

### How This Applies to My Work:
-

### Tools to Investigate:
-

### Action Items:
-

---

## 🔗 Related Concepts

- [[02. Data Storage and Versioning Strategies.md|Previous: Data Versioning]]
- [[03. Feature Engineering Workflows.md|Previous: Feature Engineering]]
- [[05. Monitoring and Feedback Loops.md|Next: Monitoring]]
- [[../01. Introduction to DE for AI-ML/README.md|Chapter 1: Introduction]]

---

*Created: October 18, 2025*
*Last Updated: October 18, 2025*
*Status: ✅ Completed - Ready for study*
