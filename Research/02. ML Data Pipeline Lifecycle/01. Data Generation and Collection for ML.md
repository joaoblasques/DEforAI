# 01. Data Generation and Collection for ML

**Chapter:** ML Data Pipeline Lifecycle
**Topic:** Understanding data sources and ingestion patterns for ML systems

---

## üìã Overview

This subchapter explores how data is generated and collected for ML applications. Unlike traditional analytics pipelines, ML data collection requires consideration of temporal consistency, feature freshness, the dual nature of batch (training) and streaming (serving) requirements, and the critical challenge of preventing data leakage.

**Key Insight:** As emphasized in recent MLOps research (2024-2025), success hinges not just on models but on the data pipelines that feed those models. Data ingestion and validation ensure data is clean and correctly formatted‚Äîthe foundation of any production ML system.

---

## üéØ Learning Objectives

After completing this subchapter, you will be able to:
- Identify and classify different ML data sources and their characteristics
- Design appropriate ingestion patterns for batch and streaming data
- Understand data freshness requirements across different ML applications
- Implement data collection strategies that prevent data leakage
- Build scalable ingestion pipelines that handle real-world challenges (late data, schema changes, duplicates)
- Choose between batch, streaming, or hybrid architectures based on ML requirements

---

## üìö Core Concepts

### 1. Data Sources for ML Systems

ML systems typically draw data from multiple heterogeneous sources. Understanding their characteristics is crucial for pipeline design.

#### A. Operational Databases (OLTP)

**Description:** Transactional systems powering applications (PostgreSQL, MySQL, MongoDB)

**Characteristics:**
- Real-time updates with low latency
- Normalized schema optimized for writes
- Row-oriented storage
- ACID guarantees

**Common Data Types:**
- User profiles and authentication data
- Transaction records (purchases, payments)
- Inventory and catalog information
- Order and fulfillment history

**Ingestion Methods:**
- **Change Data Capture (CDC):** Capture database changes in real-time (Debezium, Maxwell, AWS DMS)
- **Batch Export:** Scheduled SQL queries or database dumps
- **Materialized Views:** Pre-aggregated views for ML pipelines

**ML Use Cases:**
- Customer churn prediction
- Fraud detection
- Demand forecasting
- Personalization

**CDC Example (Debezium Configuration):**
```json
{
  "name": "postgres-cdc-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "postgres-prod.example.com",
    "database.port": "5432",
    "database.user": "ml_pipeline_user",
    "database.password": "${DB_PASSWORD}",
    "database.dbname": "production_db",
    "database.server.name": "production",
    "table.include.list": "public.users,public.transactions,public.orders",
    "snapshot.mode": "initial",
    "publication.autocreate.mode": "filtered",
    "transforms": "route",
    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
    "transforms.route.replacement": "ml_raw_$3"
  }
}
```

**Best Practice:** Use CDC for high-value transactional data where freshness matters. Batch exports work well for slowly changing dimension tables (user profiles, product catalogs).

---

#### B. Event Streams

**Description:** Real-time user interactions and system events

**Characteristics:**
- High volume (thousands to millions of events/second)
- Time-series nature with timestamps
- Append-only, immutable
- Schema can evolve over time

**Common Data Types:**
- User clickstream (page views, clicks, scrolls)
- Application events (search queries, add-to-cart, purchases)
- IoT sensor data (device telemetry, GPS coordinates)
- System logs and metrics

**Ingestion Architecture:**
```
User Action ‚Üí Event Collector ‚Üí Message Queue ‚Üí Stream Processor ‚Üí Feature Store
     ‚Üì                           (Kafka/Kinesis)        ‚Üì               ‚Üì
Client SDK                                         Data Lake      Online Store
```

**ML Use Cases:**
- Real-time recommendation systems
- Anomaly detection
- Real-time personalization
- Predictive maintenance (IoT)

**Event Schema Design (Avro Example):**
```json
{
  "type": "record",
  "name": "UserEvent",
  "namespace": "com.company.ml.events",
  "fields": [
    {"name": "event_id", "type": "string"},
    {"name": "user_id", "type": "string"},
    {"name": "session_id", "type": "string"},
    {"name": "event_type", "type": "string"},
    {"name": "timestamp", "type": "long", "logicalType": "timestamp-millis"},
    {"name": "device_type", "type": ["null", "string"], "default": null},
    {"name": "metadata", "type": {"type": "map", "values": "string"}},
    {"name": "schema_version", "type": "string", "default": "1.0"}
  ]
}
```

**Key Challenge:** Handling out-of-order and late-arriving events requires watermarking and windowing strategies (covered in stream processing chapter).

---

#### C. Third-Party APIs

**Description:** External data providers enriching ML models with contextual information

**Characteristics:**
- Rate-limited (API quotas and throttling)
- Varying data quality and completeness
- Schema changes without notification
- Network latency and potential downtime
- Cost per API call

**Common Data Sources:**
- Weather data (OpenWeatherMap, Weather.com)
- Financial data (stock prices, exchange rates)
- Geolocation and maps (Google Maps, Mapbox)
- Social media (Twitter API, LinkedIn)
- Demographics (Clearbit, ZoomInfo)

**Ingestion Pattern with Retry Logic:**
```python
import requests
import time
from datetime import datetime
from functools import wraps
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def retry_with_backoff(retries=3, backoff_in_seconds=1):
    """Decorator for exponential backoff retry logic"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            x = 0
            while True:
                try:
                    return func(*args, **kwargs)
                except requests.exceptions.RequestException as e:
                    if x == retries:
                        logger.error(f"Failed after {retries} retries: {e}")
                        raise

                    wait = backoff_in_seconds * (2 ** x)
                    logger.warning(f"Request failed, retrying in {wait}s: {e}")
                    time.sleep(wait)
                    x += 1
        return wrapper
    return decorator

class ThirdPartyAPICollector:
    """Resilient API data collection for ML pipelines"""

    def __init__(self, base_url: str, api_key: str, rate_limit_per_minute: int = 60):
        self.base_url = base_url
        self.api_key = api_key
        self.rate_limit = rate_limit_per_minute
        self.request_times = []

    def _enforce_rate_limit(self):
        """Simple rate limiting implementation"""
        now = time.time()
        # Remove requests older than 1 minute
        self.request_times = [t for t in self.request_times if now - t < 60]

        if len(self.request_times) >= self.rate_limit:
            # Sleep until oldest request is >60s old
            sleep_time = 60 - (now - self.request_times[0])
            logger.info(f"Rate limit reached, sleeping for {sleep_time:.2f}s")
            time.sleep(sleep_time)

        self.request_times.append(now)

    @retry_with_backoff(retries=3, backoff_in_seconds=2)
    def fetch_weather_data(self, location: str, date: str):
        """
        Fetch weather data with retry and error handling

        Args:
            location: City or coordinates
            date: Date for historical weather (YYYY-MM-DD)

        Returns:
            Dict with weather data and metadata
        """
        self._enforce_rate_limit()

        url = f"{self.base_url}/weather/history"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Accept": "application/json"
        }
        params = {
            "location": location,
            "date": date,
            "units": "metric"
        }

        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()

        data = response.json()

        # Add metadata for ML pipeline
        enriched_data = {
            **data,
            "_source": "weather_api",
            "_location": location,
            "_date_requested": date,
            "_ingestion_timestamp": datetime.now().isoformat(),
            "_api_latency_ms": response.elapsed.total_seconds() * 1000
        }

        logger.info(f"Successfully fetched weather data for {location} on {date}")
        return enriched_data

# Usage in ML pipeline
api_collector = ThirdPartyAPICollector(
    base_url="https://api.weatherprovider.com",
    api_key="your_api_key_here",
    rate_limit_per_minute=60
)

weather_data = api_collector.fetch_weather_data(
    location="New York, NY",
    date="2025-10-18"
)
```

**Best Practice:**
- Cache API responses to reduce costs and improve reliability
- Store raw API responses before transformation (enables re-processing if schema changes)
- Monitor API health and costs
- Have fallback strategies for API outages

---

#### D. Log Files and System Metrics

**Description:** Application logs, system metrics, and model inference logs

**Characteristics:**
- Unstructured or semi-structured text
- High volume (GB to TB per day)
- Contains valuable signals for monitoring and debugging
- Requires parsing and transformation

**Common Data:**
- Application error logs
- Performance metrics (CPU, memory, latency)
- Model prediction logs (inputs, outputs, latency)
- User behavior traces

**Ingestion Tools:**
- Log aggregators: Fluentd, Logstash, Vector
- Metrics collectors: Prometheus, StatsD, Telegraf
- Cloud-native: AWS CloudWatch, GCP Cloud Logging, Azure Monitor

**ML Use Cases:**
- Model performance monitoring
- Feature drift detection
- Error analysis and debugging
- Anomaly detection in system behavior

**Structured Logging for ML:**
```python
import logging
import json
from datetime import datetime

class MLStructuredLogger:
    """
    Structured logger for ML pipelines
    Outputs JSON logs for easy parsing and analysis
    """

    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)

        handler = logging.StreamHandler()
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)

    def log_prediction(self, model_name: str, features: dict, prediction: any,
                      latency_ms: float, metadata: dict = None):
        """Log model predictions for monitoring and analysis"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "log_type": "model_prediction",
            "model_name": model_name,
            "prediction": prediction,
            "latency_ms": latency_ms,
            "feature_summary": {
                "num_features": len(features),
                "feature_names": list(features.keys())
            },
            **(metadata or {})
        }

        self.logger.info(json.dumps(log_entry))

    def log_data_quality_check(self, dataset: str, check_name: str,
                               passed: bool, metrics: dict):
        """Log data quality check results"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "log_type": "data_quality",
            "dataset": dataset,
            "check_name": check_name,
            "passed": passed,
            "metrics": metrics
        }

        self.logger.info(json.dumps(log_entry))

# Usage
ml_logger = MLStructuredLogger("ml_pipeline")

ml_logger.log_prediction(
    model_name="churn_model_v3",
    features={"ltv": 1000, "recency_days": 7, "frequency": 12},
    prediction={"churn_probability": 0.23, "segment": "low_risk"},
    latency_ms=45.2,
    metadata={"user_id": "user_12345", "inference_id": "inf_abc123"}
)
```

---

### 2. Batch vs. Streaming Ingestion

One of the most critical architectural decisions in ML data pipelines is choosing between batch and streaming ingestion‚Äîor increasingly, implementing hybrid approaches.

#### When to Use Batch Ingestion

**Optimal Scenarios:**
- Training data preparation with historical features
- Daily/hourly feature updates are sufficient for use case
- Large data volumes requiring distributed processing
- Complex transformations (multi-table joins, aggregations) benefit from batch optimization
- Inference can tolerate staleness (e.g., daily recommendation updates)

**Characteristics:**
- Processes data in scheduled intervals (hourly, daily, weekly)
- Higher latency (minutes to hours) but higher throughput
- Simpler to implement and debug
- Cost-effective for large volumes
- Easier to handle data quality issues (can reprocess)

**Architecture Pattern:**
```
Source DB ‚Üí Scheduler (Airflow/Dagster) ‚Üí Batch Extract ‚Üí Transform (Spark) ‚Üí Data Lake
                                                                              ‚Üì
                                                                      Offline Feature Store
                                                                              ‚Üì
                                                                      Model Training
```

**Implementation Example (PySpark):**
```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from datetime import datetime, timedelta

def batch_ingest_user_features(date: str):
    """
    Daily batch ingestion of user behavioral features

    This pipeline computes historical features from transaction data,
    ensuring point-in-time correctness for ML training.

    Args:
        date: Processing date in YYYY-MM-DD format
    """
    spark = SparkSession.builder \
        .appName(f"UserFeatureIngestion_{date}") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()

    # Read from multiple sources
    transactions = spark.read.parquet(f"s3://raw-data/transactions/date={date}")

    user_profiles = spark.read \
        .format("jdbc") \
        .option("url", "jdbc:postgresql://db.prod:5432/users") \
        .option("dbtable", "user_profiles") \
        .option("user", "ml_reader") \
        .option("password", "${DB_PASSWORD}") \
        .load()

    product_catalog = spark.read.parquet("s3://raw-data/products/latest/")

    # Join and compute features
    enriched = transactions \
        .join(user_profiles, "user_id", "left") \
        .join(product_catalog, "product_id", "left")

    # Aggregate features per user
    user_features = enriched.groupBy("user_id").agg(
        # Transaction counts
        F.count("transaction_id").alias("daily_transactions"),
        F.countDistinct("product_id").alias("unique_products"),

        # Monetary features
        F.sum("amount").alias("daily_spend"),
        F.avg("amount").alias("avg_transaction_value"),
        F.max("amount").alias("max_transaction"),

        # Categorical features
        F.collect_set("category").alias("categories_purchased"),
        F.mode("category").alias("favorite_category"),

        # Temporal features
        F.min("transaction_timestamp").alias("first_transaction_today"),
        F.max("transaction_timestamp").alias("last_transaction_today"),

        # User context
        F.first("user_segment").alias("segment"),
        F.first("signup_date").alias("signup_date")
    )

    # Compute derived features
    user_features = user_features \
        .withColumn("categories_count", F.size("categories_purchased")) \
        .withColumn("days_since_signup",
                   F.datediff(F.lit(date), F.col("signup_date"))) \
        .withColumn("processing_date", F.lit(date)) \
        .withColumn("ingestion_timestamp", F.current_timestamp())

    # Partition by date and write to feature store
    user_features.write \
        .mode("overwrite") \
        .partitionBy("processing_date") \
        .parquet(f"s3://feature-store/user_daily_features/")

    num_users = user_features.count()
    print(f"‚úÖ Ingested features for {num_users} users on {date}")

    return num_users

# Orchestration (Airflow DAG example)
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

with DAG(
    'user_feature_ingestion',
    default_args={
        'owner': 'ml_team',
        'retries': 2,
        'retry_delay': timedelta(minutes=5)
    },
    schedule_interval='0 2 * * *',  # Run daily at 2 AM
    start_date=datetime(2025, 1, 1),
    catchup=False
) as dag:

    ingest_task = PythonOperator(
        task_id='ingest_user_features',
        python_callable=batch_ingest_user_features,
        op_kwargs={'date': '{{ ds }}'}  # Airflow execution date
    )
```

**Best Practices for Batch:**
- Use incremental processing when possible (only process new/changed data)
- Implement idempotent writes (safe to re-run)
- Partition by date for efficient querying and retention management
- Add data quality checks before writing features
- Monitor batch job duration and set SLA alerts

---

#### When to Use Streaming Ingestion

**Optimal Scenarios:**
- Real-time ML inference requires fresh features (<1 minute latency)
- Detecting rapidly changing patterns (fraud detection, trending content)
- Event-driven applications (user just clicked, immediate recommendation)
- Continuous model monitoring and drift detection

**Characteristics:**
- Processes data continuously as it arrives
- Low latency (milliseconds to seconds)
- Higher complexity (stateful processing, exactly-once semantics)
- Requires robust error handling (can't easily reprocess)

**Architecture Pattern:**
```
App Events ‚Üí Kafka ‚Üí Flink/Spark Streaming ‚Üí Feature Transform ‚Üí Online Store (Redis/DynamoDB)
                                                                 ‚Üì
                                                          Archive to Data Lake
```

**Implementation Example (Spark Structured Streaming):**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import json

def setup_streaming_feature_pipeline():
    """
    Real-time ingestion of user behavior events for ML features
    Computes windowed aggregations and updates online feature store
    """
    spark = SparkSession.builder \
        .appName("RealtimeFeaturePipeline") \
        .config("spark.sql.shuffle.partitions", "20") \
        .getOrCreate()

    # Define schema for user events
    event_schema = StructType([
        StructField("user_id", StringType(), False),
        StructField("event_type", StringType(), False),
        StructField("product_id", StringType(), True),
        StructField("timestamp", TimestampType(), False),
        StructField("session_id", StringType(), False),
        StructField("device_type", StringType(), True),
        StructField("amount", DoubleType(), True)
    ])

    # Read from Kafka
    raw_stream = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:9092") \
        .option("subscribe", "user_events") \
        .option("startingOffsets", "latest") \
        .option("maxOffsetsPerTrigger", "10000") \
        .load()

    # Parse JSON and extract event data
    events = raw_stream \
        .selectExpr("CAST(value AS STRING) as json_data", "timestamp as kafka_timestamp") \
        .select(
            from_json("json_data", event_schema).alias("data"),
            "kafka_timestamp"
        ) \
        .select("data.*", "kafka_timestamp")

    # Compute streaming features with multiple time windows
    streaming_features = events \
        .withWatermark("timestamp", "10 minutes") \
        .groupBy(
            window("timestamp", "5 minutes", "1 minute"),  # 5-min window, 1-min slide
            "user_id"
        ) \
        .agg(
            # Event counts
            count("*").alias("events_last_5min"),
            countDistinct("product_id").alias("unique_products_viewed"),
            countDistinct("session_id").alias("unique_sessions"),

            # Behavioral signals
            sum(when(col("event_type") == "add_to_cart", 1).otherwise(0)).alias("cart_adds"),
            sum(when(col("event_type") == "purchase", 1).otherwise(0)).alias("purchases"),
            sum(when(col("event_type") == "product_view", 1).otherwise(0)).alias("product_views"),

            # Monetary features
            sum("amount").alias("total_spent_5min"),
            avg("amount").alias("avg_spend_5min"),

            # Temporal features
            min("timestamp").alias("window_start_time"),
            max("timestamp").alias("window_end_time"),

            # Device context
            mode("device_type").alias("primary_device")
        ) \
        .withColumn("feature_timestamp", current_timestamp()) \
        .withColumn("window_start", col("window.start")) \
        .withColumn("window_end", col("window.end")) \
        .drop("window")

    # Write to online feature store (Redis) for low-latency serving
    def write_to_redis(batch_df, batch_id):
        """
        Write features to Redis for real-time serving
        Each user gets a hash with all their recent features
        """
        import redis
        import json

        redis_client = redis.Redis(
            host='redis.prod',
            port=6379,
            db=0,
            decode_responses=True
        )

        for row in batch_df.collect():
            redis_key = f"user_features:{row['user_id']}"

            feature_dict = {
                "events_last_5min": row["events_last_5min"],
                "unique_products_viewed": row["unique_products_viewed"],
                "cart_adds": row["cart_adds"],
                "purchases": row["purchases"],
                "product_views": row["product_views"],
                "total_spent_5min": float(row["total_spent_5min"] or 0),
                "avg_spend_5min": float(row["avg_spend_5min"] or 0),
                "primary_device": row["primary_device"],
                "last_update": row["feature_timestamp"].isoformat()
            }

            # Set with TTL of 30 minutes (features expire if user inactive)
            pipeline = redis_client.pipeline()
            pipeline.hset(redis_key, mapping=feature_dict)
            pipeline.expire(redis_key, 1800)  # 30 min TTL
            pipeline.execute()

        print(f"‚úÖ Batch {batch_id}: Updated features for {batch_df.count()} users in Redis")

    # Archive to Data Lake for historical analysis
    archive_query = streaming_features.writeStream \
        .format("parquet") \
        .option("path", "s3://feature-store/streaming_features/") \
        .option("checkpointLocation", "s3://checkpoints/streaming_features/") \
        .partitionBy("window_start") \
        .outputMode("append") \
        .trigger(processingTime="1 minute") \
        .start()

    # Update online feature store
    online_query = streaming_features.writeStream \
        .foreachBatch(write_to_redis) \
        .outputMode("update") \
        .option("checkpointLocation", "s3://checkpoints/redis_features/") \
        .trigger(processingTime="30 seconds") \
        .start()

    # Wait for streams to finish (runs indefinitely)
    spark.streams.awaitAnyTermination()

if __name__ == "__main__":
    setup_streaming_feature_pipeline()
```

**Best Practices for Streaming:**
- Use watermarks to handle late-arriving data
- Implement exactly-once processing semantics
- Monitor lag (processing time vs. event time)
- Set appropriate checkpoint intervals
- Design for failure recovery (stateful processing needs checkpointing)

---

#### Hybrid Batch-Streaming (Lambda Architecture)

Most production ML systems require **both** batch and streaming ingestion to balance freshness, completeness, and cost.

**Lambda Architecture Pattern:**
```
                         ‚îå‚îÄ‚îÄ> Batch Layer (Spark) ‚îÄ‚îÄ> Offline Features (S3/Snowflake)
                         ‚îÇ         ‚Üì
Data Sources ‚îÄ‚îÄ> Kafka ‚îÄ‚îÄ‚î§    Historical, complete, high-latency
                         ‚îÇ
                         ‚îî‚îÄ‚îÄ> Speed Layer (Flink) ‚îÄ‚îÄ> Online Features (Redis/DynamoDB)
                                  ‚Üì
                            Real-time, partial, low-latency
                                  ‚Üì
                          Serving Layer merges both views
```

**Use Case: E-commerce Recommendation System**

**Batch Layer (runs daily):**
- User's historical purchase patterns (last 90 days)
- Collaborative filtering matrices (all users √ó all products)
- Product popularity trends (weekly aggregations)
- User lifetime value and churn probability

**Speed Layer (real-time):**
- Items viewed in current session (last 30 minutes)
- Recent add-to-cart events (last hour)
- Current cart contents
- Real-time trending products (last 4 hours)

**Serving Layer:**
```python
def get_recommendation_features(user_id: str):
    """Merge batch and streaming features for recommendation model"""

    # Get batch features (historical, complete)
    batch_features = offline_store.get_features(
        feature_group="user_historical_features",
        entity_id=user_id,
        as_of_date=datetime.now()
    )

    # Get streaming features (real-time, partial)
    streaming_features = redis_client.hgetall(f"user_features:{user_id}")

    # Merge with defaults for missing streaming features
    merged = {
        **batch_features,
        **streaming_features,
        "merged_timestamp": datetime.now().isoformat()
    }

    return merged
```

**Trade-offs:**

| Aspect | Batch | Streaming | Hybrid |
|--------|-------|-----------|--------|
| **Latency** | Hours to days | Milliseconds to seconds | Best of both |
| **Completeness** | 100% of data | May miss late arrivals | Complete + fresh |
| **Complexity** | Low | High | Very high |
| **Cost** | Lower | Higher | Highest |
| **Use Case** | Training, reports | Real-time inference | Production ML |

---

### 3. Data Collection Challenges for ML

ML systems introduce unique challenges beyond traditional data pipelines.

#### Challenge 1: Temporal Consistency (Point-in-Time Correctness)

**The Problem:**
Features must reflect what was *actually known* at prediction time, not future information that wasn't available yet. This is critical for training data‚Äîif you leak future information, your model will perform well in development but fail in production.

**Example of Leakage:**
```python
# ‚ùå WRONG: Using all-time data creates leakage
df['user_lifetime_value'] = df.groupby('user_id')['amount'].transform('sum')

# At training time (2024-01-15), this includes purchases through 2024-12-31
# At prediction time (2024-01-15), you only know purchases through 2024-01-14
# Result: Model sees future information during training
```

**Correct Implementation:**
```python
# ‚úÖ CORRECT: Point-in-time feature computation
def compute_features_as_of(transactions_df, prediction_date):
    """
    Only use data available BEFORE prediction_date
    Critical for preventing temporal leakage

    Args:
        transactions_df: All transactions with timestamps
        prediction_date: Date we're making prediction for

    Returns:
        DataFrame with point-in-time correct features
    """
    # Only include historical data
    historical = transactions_df[
        transactions_df['transaction_date'] < prediction_date
    ]

    # Compute features from historical data only
    features = historical.groupby('user_id').agg({
        'amount': ['sum', 'mean', 'count'],
        'transaction_date': 'max'
    }).reset_index()

    features.columns = ['user_id', 'lifetime_value', 'avg_order_value',
                       'num_purchases', 'last_purchase_date']

    # Add temporal features
    features['days_since_last_purchase'] = (
        pd.to_datetime(prediction_date) -
        pd.to_datetime(features['last_purchase_date'])
    ).dt.days

    features['prediction_date'] = prediction_date

    return features

# Usage for training
train_dates = pd.date_range('2024-01-01', '2024-06-01', freq='D')

training_data = pd.concat([
    compute_features_as_of(transactions, date)
    for date in train_dates
])
```

**Feature Store Solution:**
Modern feature stores (Feast, Tecton, SageMaker) enforce point-in-time correctness automatically:

```python
# Feature store handles temporal joins automatically
features = feature_store.get_historical_features(
    entity_df=prediction_requests,  # Has user_id and event_timestamp
    features=[
        "user_features:lifetime_value",
        "user_features:days_since_last_purchase"
    ]
)
# Returns features as they existed at each event_timestamp
```

---

#### Challenge 2: Data Freshness Requirements

Different ML applications have vastly different freshness needs. The latency requirement drives your architecture.

**Freshness Categories:**

| Latency | Use Cases | Architecture |
|---------|-----------|-------------|
| **<100ms** | Fraud detection, ad bidding | Streaming + precomputed features |
| **1-10 min** | Real-time recommendations | Micro-batch streaming |
| **1-6 hours** | Demand forecasting, inventory optimization | Hourly batch |
| **Daily** | Churn prediction, customer segmentation | Daily batch |
| **Weekly** | Cohort analysis, long-term trends | Weekly batch |

**Example: Fraud Detection (<100ms)**
```python
# Features must be computed in near-real-time
# Architecture: Flink stateful processing + Redis
features = {
    "transactions_last_5min": 3,
    "total_amount_last_hour": 1523.45,
    "distance_from_last_transaction_km": 15.3,
    "is_new_merchant": True,
    "merchant_risk_score": 0.67
}
# Computed and served in <50ms
```

**Example: Churn Prediction (Daily)**
```python
# Features can be batch-computed overnight
# Architecture: Spark batch + S3 + daily model scoring
features = {
    "days_since_last_login": 14,
    "num_logins_last_30_days": 2,
    "feature_usage_score": 0.23,
    "support_tickets_last_month": 3
}
# Computed once per day, acceptable staleness
```

---

#### Challenge 3: Handling Schema Evolution

Data sources change over time‚Äînew fields added, types changed, columns removed. ML pipelines must handle this gracefully.

**Schema Evolution Strategies:**

**1. Schema Registry (Recommended for Streaming)**
```python
from confluent_kafka import avro
from confluent_kafka.avro import AvroProducer, AvroConsumer

# Define schema with backward compatibility
user_event_schema_v1 = """
{
  "type": "record",
  "name": "UserEvent",
  "fields": [
    {"name": "user_id", "type": "string"},
    {"name": "event_type", "type": "string"},
    {"name": "timestamp", "type": "long"}
  ]
}
"""

user_event_schema_v2 = """
{
  "type": "record",
  "name": "UserEvent",
  "fields": [
    {"name": "user_id", "type": "string"},
    {"name": "event_type", "type": "string"},
    {"name": "timestamp", "type": "long"},
    {"name": "device_type", "type": ["null", "string"], "default": null},
    {"name": "session_id", "type": ["null", "string"], "default": null}
  ]
}
"""
# New fields have defaults, backward compatible
```

**2. Schema Alignment (For Batch Processing)**
```python
class SchemaEvolutionHandler:
    """
    Gracefully handle schema changes in batch data pipelines
    Ensures features remain consistent across schema versions
    """

    def __init__(self, expected_schema: Dict[str, str]):
        """
        Args:
            expected_schema: Dict of {column_name: data_type}
        """
        self.expected_schema = expected_schema

    def align_schema(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Add missing columns, remove unexpected columns, cast types

        Args:
            df: Input DataFrame with potentially different schema

        Returns:
            DataFrame aligned to expected schema
        """
        expected_cols = set(self.expected_schema.keys())
        actual_cols = set(df.columns)

        # Add missing columns with appropriate defaults
        for col_name in expected_cols - actual_cols:
            dtype = self.expected_schema[col_name]
            default_value = self._get_default_for_type(dtype)
            df[col_name] = default_value
            print(f"‚ÑπÔ∏è  Added missing column '{col_name}' with default {default_value}")

        # Drop unexpected columns (log warning)
        for col_name in actual_cols - expected_cols:
            df = df.drop(columns=[col_name])
            print(f"‚ö†Ô∏è  Dropped unexpected column '{col_name}'")

        # Cast to expected types
        for col_name, expected_type in self.expected_schema.items():
            if col_name in df.columns:
                df[col_name] = df[col_name].astype(expected_type, errors='ignore')

        # Reorder columns to match expected schema
        df = df[list(self.expected_schema.keys())]

        return df

    def _get_default_for_type(self, dtype: str):
        """Return appropriate default value for data type"""
        defaults = {
            'int64': 0,
            'float64': 0.0,
            'object': None,
            'bool': False,
            'datetime64[ns]': pd.NaT
        }
        return defaults.get(dtype, None)

# Usage
expected_schema = {
    'user_id': 'object',
    'event_type': 'object',
    'timestamp': 'datetime64[ns]',
    'amount': 'float64',
    'device_type': 'object'
}

handler = SchemaEvolutionHandler(expected_schema)

# Data from old source (missing 'device_type')
old_data = pd.DataFrame({
    'user_id': ['u1', 'u2'],
    'event_type': ['click', 'purchase'],
    'timestamp': pd.to_datetime(['2025-10-18', '2025-10-18']),
    'amount': [0.0, 49.99]
})

# Align to expected schema
aligned_data = handler.align_schema(old_data)
# device_type column added with None values
```

**3. Schema Monitoring**
```python
def monitor_schema_drift(current_df: pd.DataFrame,
                        reference_schema: Dict[str, str],
                        alert_channel: str):
    """
    Detect and alert on schema changes
    """
    current_schema = dict(current_df.dtypes.astype(str))

    # Check for schema differences
    added_cols = set(current_schema.keys()) - set(reference_schema.keys())
    removed_cols = set(reference_schema.keys()) - set(current_schema.keys())
    type_changes = {
        col: (reference_schema[col], current_schema[col])
        for col in set(current_schema.keys()) & set(reference_schema.keys())
        if current_schema[col] != reference_schema[col]
    }

    if added_cols or removed_cols or type_changes:
        alert_message = f"""
        ‚ö†Ô∏è SCHEMA DRIFT DETECTED

        Added columns: {added_cols}
        Removed columns: {removed_cols}
        Type changes: {type_changes}

        Action required: Update feature pipeline
        """
        send_alert(alert_channel, alert_message)
        return False

    return True
```

---

#### Challenge 4: Dealing with Duplicates

Duplicate data is a common problem in ML pipelines that can skew model training and inflate metrics.

**Sources of Duplicates:**
- Retry logic in event producers (at-least-once delivery)
- Multiple data sources with overlapping data
- CDC capturing the same update multiple times
- Network issues causing message redelivery
- Human error (re-running batch jobs)

**Impact on ML:**
- **Training:** Duplicates give certain examples more weight, biasing the model
- **Evaluation:** Inflated metrics if test data contains duplicates
- **Features:** Incorrect aggregations (counts, sums)
- **Costs:** Wasted storage and compute resources

**Deduplication Strategies:**

**1. Event-Level Deduplication (Using Event ID)**
```python
from pyspark.sql import Window
from pyspark.sql import functions as F

def deduplicate_events(df, partition_col='user_id', order_col='timestamp', id_col='event_id'):
    """
    Remove duplicate events, keeping the most recent occurrence

    Args:
        df: Input DataFrame with potential duplicates
        partition_col: Column to partition by (e.g., 'user_id')
        order_col: Column to order by (e.g., 'timestamp')
        id_col: Unique identifier column (e.g., 'event_id')

    Returns:
        Deduplicated DataFrame
    """
    # Strategy 1: Simple distinct on ID (if events are truly identical)
    if id_col and id_col in df.columns:
        return df.dropDuplicates([id_col])

    # Strategy 2: Keep latest occurrence using window function
    window = Window.partitionBy(partition_col).orderBy(F.desc(order_col))

    deduplicated = (
        df.withColumn("row_num", F.row_number().over(window))
        .filter(F.col("row_num") == 1)
        .drop("row_num")
    )

    original_count = df.count()
    deduplicated_count = deduplicated.count()
    duplicates_removed = original_count - deduplicated_count

    print(f"Deduplication: {original_count} ‚Üí {deduplicated_count} rows ({duplicates_removed} duplicates removed)")

    return deduplicated

# Usage
clean_events = deduplicate_events(
    raw_events,
    partition_col='user_id',
    order_col='event_timestamp',
    id_col='event_id'
)
```

**2. Idempotent Processing (Upsert Pattern)**
```python
from delta.tables import DeltaTable

def upsert_to_delta(new_data_df, target_path, merge_keys):
    """
    Upsert (merge) new data into Delta table, handling duplicates gracefully

    Args:
        new_data_df: DataFrame with new data
        target_path: Path to Delta table
        merge_keys: List of columns that uniquely identify a row
    """
    # Check if table exists
    if DeltaTable.isDeltaTable(spark, target_path):
        delta_table = DeltaTable.forPath(spark, target_path)

        # Build merge condition
        merge_condition = " AND ".join([f"target.{key} = source.{key}" for key in merge_keys])

        # Upsert: update if exists, insert if new
        delta_table.alias("target").merge(
            new_data_df.alias("source"),
            merge_condition
        ).whenMatchedUpdateAll() \
         .whenNotMatchedInsertAll() \
         .execute()

        print(f"‚úÖ Upserted {new_data_df.count()} rows to {target_path}")
    else:
        # First run: create table
        new_data_df.write.format("delta").save(target_path)
        print(f"‚úÖ Created new Delta table at {target_path} with {new_data_df.count()} rows")

# Usage: idempotent daily user feature updates
upsert_to_delta(
    new_data_df=daily_features,
    target_path="s3://feature-store/user_features/",
    merge_keys=['user_id', 'feature_date']
)
```

**3. Bloom Filters for Streaming Deduplication**
```python
from pyspark.sql.functions import bloom_filter_agg, might_contain

def streaming_dedup_with_bloom(streaming_df, checkpoint_path):
    """
    Use Bloom filters for approximate deduplication in streaming
    More memory-efficient than storing all seen IDs
    """
    # Aggregate Bloom filter of seen event IDs
    bloom_df = (
        streaming_df
        .groupBy()
        .agg(bloom_filter_agg("event_id", expectedNumItems=10000000).alias("bloom"))
    )

    # Filter out likely duplicates
    deduplicated = streaming_df.join(
        broadcast(bloom_df),
        might_contain(bloom_df.bloom, streaming_df.event_id) == False
    )

    return deduplicated
```

---

### Additional Practical Examples

#### Example 3: IoT Sensor Data Collection

**Scenario:** Collecting telemetry from 100,000 IoT devices for predictive maintenance ML model.

**Requirements:**
- Sensor readings every 5 seconds
- 10 metrics per device (temperature, vibration, pressure, etc.)
- ~1.2M events per minute
- Predict equipment failure 24 hours in advance

**Data Collection Architecture:**
```
IoT Devices ‚Üí AWS IoT Core ‚Üí Kinesis Data Streams ‚Üí Lambda (Parse) ‚Üí S3 + Timestream
                                                                        ‚Üì
                                                                  Feature Engineering
                                                                        ‚Üì
                                                              Train Anomaly Detection Model
```

**Implementation:**

**Step 1: Device Data Schema**
```python
from dataclasses import dataclass
from datetime import datetime
from typing import Dict

@dataclass
class SensorReading:
    device_id: str
    timestamp: datetime
    temperature: float
    vibration: float
    pressure: float
    rpm: int
    power_consumption: float
    operating_hours: int
    maintenance_flag: bool
    location: str

    def to_dict(self) -> Dict:
        return {
            'device_id': self.device_id,
            'timestamp': self.timestamp.isoformat(),
            'temperature': self.temperature,
            'vibration': self.vibration,
            'pressure': self.pressure,
            'rpm': self.rpm,
            'power_consumption': self.power_consumption,
            'operating_hours': self.operating_hours,
            'maintenance_flag': self.maintenance_flag,
            'location': self.location
        }
```

**Step 2: Lambda Processing Function**
```python
import json
import boto3
from datetime import datetime

s3_client = boto3.client('s3')
timestream_client = boto3.client('timestream-write')

def lambda_handler(event, context):
    """
    Process IoT sensor data from Kinesis
    - Write raw data to S3 for ML training
    - Write metrics to Timestream for real-time monitoring
    """
    records_processed = 0
    s3_buffer = []

    for record in event['Records']:
        # Decode Kinesis record
        payload = json.loads(base64.b64decode(record['kinesis']['data']))

        # Validate data quality
        if validate_sensor_reading(payload):
            # Buffer for S3 batch write
            s3_buffer.append(payload)

            # Write to Timestream for real-time queries
            write_to_timestream(payload)

            records_processed += 1

    # Batch write to S3
    if s3_buffer:
        write_batch_to_s3(s3_buffer)

    return {
        'statusCode': 200,
        'body': f'Processed {records_processed} sensor readings'
    }

def validate_sensor_reading(reading: dict) -> bool:
    """Data quality checks"""
    required_fields = ['device_id', 'timestamp', 'temperature', 'vibration']

    # Check required fields
    if not all(field in reading for field in required_fields):
        return False

    # Check value ranges
    if not (0 <= reading['temperature'] <= 200):  # ¬∞C
        return False

    if not (0 <= reading['vibration'] <= 100):  # mm/s
        return False

    return True

def write_batch_to_s3(records: list):
    """Write sensor data to S3 for ML training"""
    timestamp = datetime.now()
    key = f"sensor-data/year={timestamp.year}/month={timestamp.month:02d}/day={timestamp.day:02d}/hour={timestamp.hour:02d}/batch_{timestamp.isoformat()}.json"

    s3_client.put_object(
        Bucket='iot-ml-data-lake',
        Key=key,
        Body=json.dumps(records).encode('utf-8')
    )
```

**Step 3: Feature Engineering from Sensor Data**
```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

def create_predictive_maintenance_features(sensor_data_path, output_path):
    """
    Create time-series features for predictive maintenance
    """
    spark = SparkSession.builder.appName("IoT-Features").getOrCreate()

    # Read sensor data
    sensors = spark.read.json(sensor_data_path)

    # Define time windows for rolling aggregations
    window_spec_1h = Window.partitionBy("device_id").orderBy("timestamp").rangeBetween(-3600, 0)
    window_spec_24h = Window.partitionBy("device_id").orderBy("timestamp").rangeBetween(-86400, 0)

    # Compute features
    features = sensors.withColumn(
        # Rolling statistics (1 hour)
        "temp_mean_1h", F.avg("temperature").over(window_spec_1h)
    ).withColumn(
        "temp_std_1h", F.stddev("temperature").over(window_spec_1h)
    ).withColumn(
        "vibration_max_1h", F.max("vibration").over(window_spec_1h)
    ).withColumn(
        "vibration_mean_1h", F.avg("vibration").over(window_spec_1h)
    ).withColumn(
        # Rolling statistics (24 hours)
        "temp_mean_24h", F.avg("temperature").over(window_spec_24h)
    ).withColumn(
        "power_sum_24h", F.sum("power_consumption").over(window_spec_24h)
    ).withColumn(
        # Rate of change features
        "temp_rate_of_change",
        (F.col("temperature") - F.lag("temperature", 1).over(window_spec_1h)) / 300  # per 5 min
    ).withColumn(
        # Anomaly indicators
        "temp_anomaly", F.abs(F.col("temperature") - F.col("temp_mean_24h")) > 3 * F.col("temp_std_1h")
    ).withColumn(
        "vibration_spike", F.col("vibration") > 2 * F.col("vibration_mean_1h")
    )

    # Write features
    features.write.mode("overwrite").parquet(output_path)

    print(f"‚úÖ Created features for {features.select('device_id').distinct().count()} devices")
```

**Key Challenges Solved:**
- **Volume:** Kinesis handles 1.2M events/min, Lambda processes in parallel
- **Real-time + Historical:** Timestream for monitoring, S3 for ML training
- **Data Quality:** Validation at ingestion prevents bad data downstream
- **Feature Engineering:** Time-series windowing for predictive signals

---

## üîß Additional Code Examples

### Example: Multi-Source Data Collection Orchestrator

```python
"""
Production-grade multi-source data collection orchestrator
Coordinates ingestion from multiple sources with error handling and monitoring
"""

from typing import Dict, List, Any
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from dataclasses import dataclass
from enum import Enum

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IngestionStatus(Enum):
    SUCCESS = "success"
    FAILED = "failed"
    PARTIAL = "partial"

@dataclass
class IngestionResult:
    source_name: str
    status: IngestionStatus
    rows_collected: int
    duration_seconds: float
    error_message: str = None
    metadata: Dict[str, Any] = None

class DataCollectionOrchestrator:
    """
    Orchestrates data collection from multiple sources in parallel
    Handles failures gracefully and provides comprehensive reporting
    """

    def __init__(self, sources: Dict[str, 'DataSource'], max_workers: int = 5):
        self.sources = sources
        self.max_workers = max_workers
        self.results: List[IngestionResult] = []

    def collect_all(self, collection_date: str, fail_fast: bool = False) -> Dict[str, Any]:
        """
        Collect data from all sources in parallel

        Args:
            collection_date: Date to collect data for (YYYY-MM-DD)
            fail_fast: If True, stop all ingestion on first failure

        Returns:
            Summary of collection results
        """
        logger.info(f"Starting data collection for {len(self.sources)} sources")
        start_time = datetime.now()

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all ingestion tasks
            future_to_source = {
                executor.submit(self._collect_from_source, name, source, collection_date): name
                for name, source in self.sources.items()
            }

            # Process completed tasks
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]

                try:
                    result = future.result()
                    self.results.append(result)

                    if result.status == IngestionStatus.SUCCESS:
                        logger.info(f"‚úÖ {source_name}: Collected {result.rows_collected} rows in {result.duration_seconds:.2f}s")
                    else:
                        logger.error(f"‚ùå {source_name}: {result.error_message}")

                        if fail_fast:
                            # Cancel remaining tasks
                            for f in future_to_source:
                                f.cancel()
                            raise Exception(f"Ingestion failed for {source_name}, stopping all tasks")

                except Exception as e:
                    logger.error(f"‚ùå {source_name}: Unexpected error - {e}")
                    self.results.append(IngestionResult(
                        source_name=source_name,
                        status=IngestionStatus.FAILED,
                        rows_collected=0,
                        duration_seconds=0,
                        error_message=str(e)
                    ))

        # Generate summary
        total_duration = (datetime.now() - start_time).total_seconds()
        summary = self._generate_summary(total_duration)

        logger.info(f"Collection completed in {total_duration:.2f}s")
        logger.info(f"Success: {summary['successful_sources']}/{summary['total_sources']}")

        return summary

    def _collect_from_source(self, name: str, source: 'DataSource', date: str) -> IngestionResult:
        """Collect data from a single source with error handling"""
        start_time = datetime.now()

        try:
            # Fetch data
            df = source.fetch(date=date)

            # Validate
            if not source.validate(df):
                raise ValueError("Data validation failed")

            # Save
            output_path = f"s3://ml-data-lake/raw/{name}/date={date}/"
            df.to_parquet(output_path, index=False)

            duration = (datetime.now() - start_time).total_seconds()

            return IngestionResult(
                source_name=name,
                status=IngestionStatus.SUCCESS,
                rows_collected=len(df),
                duration_seconds=duration,
                metadata={
                    'output_path': output_path,
                    'columns': list(df.columns),
                    'memory_mb': df.memory_usage(deep=True).sum() / 1024 / 1024
                }
            )

        except Exception as e:
            duration = (datetime.now() - start_time).total_seconds()

            return IngestionResult(
                source_name=name,
                status=IngestionStatus.FAILED,
                rows_collected=0,
                duration_seconds=duration,
                error_message=str(e)
            )

    def _generate_summary(self, total_duration: float) -> Dict[str, Any]:
        """Generate collection summary report"""
        successful = [r for r in self.results if r.status == IngestionStatus.SUCCESS]
        failed = [r for r in self.results if r.status == IngestionStatus.FAILED]

        return {
            'total_sources': len(self.sources),
            'successful_sources': len(successful),
            'failed_sources': len(failed),
            'total_rows_collected': sum(r.rows_collected for r in successful),
            'total_duration_seconds': total_duration,
            'results': [
                {
                    'source': r.source_name,
                    'status': r.status.value,
                    'rows': r.rows_collected,
                    'duration': r.duration_seconds,
                    'error': r.error_message
                }
                for r in self.results
            ],
            'failed_sources_list': [r.source_name for r in failed]
        }

# Usage example
if __name__ == "__main__":
    from data_sources import DatabaseSource, APISource  # Your implementations

    sources = {
        'users': DatabaseSource(
            connection_string='postgresql://...',
            query="SELECT * FROM users WHERE created_date = '{date}'"
        ),
        'transactions': DatabaseSource(
            connection_string='postgresql://...',
            query="SELECT * FROM transactions WHERE transaction_date = '{date}'"
        ),
        'weather': APISource(
            base_url='https://api.weather.com',
            endpoint='v1/forecast',
            api_key='YOUR_KEY'
        )
    }

    orchestrator = DataCollectionOrchestrator(sources, max_workers=3)
    summary = orchestrator.collect_all(collection_date='2025-10-18')

    print(json.dumps(summary, indent=2))
```

---

## ‚úÖ Best Practices

### 1. Design for Idempotency

**Why:** Data pipelines may run multiple times (retries, backfills, debugging). Idempotent operations prevent data duplication and corruption.

**How:**
- Use unique identifiers for deduplication
- Implement upsert operations instead of append-only
- Maintain processing checkpoints
- Make writes deterministic (same input ‚Üí same output)

**Example:**
```python
# Idempotent write pattern with Delta Lake
def idempotent_batch_write(df, target_path, merge_keys):
    """
    Write data idempotently - safe to run multiple times
    """
    from delta.tables import DeltaTable

    if DeltaTable.isDeltaTable(spark, target_path):
        # Table exists: merge (upsert)
        delta_table = DeltaTable.forPath(spark, target_path)
        merge_condition = " AND ".join([f"target.{k} = source.{k}" for k in merge_keys])

        delta_table.alias("target").merge(
            df.alias("source"),
            merge_condition
        ).whenMatchedUpdateAll() \
         .whenNotMatchedInsertAll() \
         .execute()
    else:
        # First run: create table
        df.write.format("delta").save(target_path)
```

---

### 2. Implement Data Quality Checks at Ingestion

**Why:** Catch bad data early before it propagates downstream and corrupts models.

**What to Check:**
- **Schema validation:** Expected columns present, correct types
- **Null rates:** Within acceptable thresholds
- **Value ranges:** Numeric values in valid ranges
- **Data type correctness:** Dates are valid, strings match patterns
- **Referential integrity:** Foreign keys exist
- **Business rules:** Domain-specific validations

**Implementation:**
```python
from great_expectations.core import ExpectationSuite, ExpectationConfiguration
from great_expectations.dataset import PandasDataset

def validate_ingested_data(df, dataset_name):
    """
    Comprehensive data quality validation at ingestion
    """
    # Convert to GE dataset
    ge_df = PandasDataset(df)

    # Define expectations
    validations = []

    # Schema checks
    validations.append(ge_df.expect_table_columns_to_match_set(
        column_set=['user_id', 'timestamp', 'event_type', 'amount'],
        exact_match=True
    ))

    # Null checks
    validations.append(ge_df.expect_column_values_to_not_be_null(
        column='user_id',
        mostly=0.99  # Allow 1% nulls
    ))

    # Value range checks
    validations.append(ge_df.expect_column_values_to_be_between(
        column='amount',
        min_value=0,
        max_value=100000
    ))

    # Pattern matching
    validations.append(ge_df.expect_column_values_to_match_regex(
        column='user_id',
        regex=r'^user_[0-9a-f]{8}$'
    ))

    # Check results
    failed_checks = [v for v in validations if not v['success']]

    if failed_checks:
        error_msg = f"Data quality checks failed for {dataset_name}:\n"
        for check in failed_checks:
            error_msg += f"  - {check['expectation_config']['expectation_type']}\n"
        raise ValueError(error_msg)

    logger.info(f"‚úÖ All quality checks passed for {dataset_name}")
    return True
```

---

### 3. Partition Data by Time

**Why:** Enables efficient querying, backfilling, and retention management.

**Best Practices:**
- Partition by date for time-series data (most common)
- Use Hive-style partitioning: `date=2025-10-18`
- Consider additional partitions for high-volume data: `date=2025-10-18/hour=14`
- Balance partition size (aim for 100MB-1GB per partition)

**Directory Structure:**
```
s3://ml-data-lake/
    ‚îî‚îÄ‚îÄ raw/
        ‚îî‚îÄ‚îÄ user_events/
            ‚îî‚îÄ‚îÄ date=2025-10-18/
                ‚îî‚îÄ‚îÄ hour=00/
                    ‚îú‚îÄ‚îÄ part-00000.parquet
                    ‚îú‚îÄ‚îÄ part-00001.parquet
                    ‚îî‚îÄ‚îÄ part-00002.parquet
                ‚îî‚îÄ‚îÄ hour=01/
                    ‚îî‚îÄ‚îÄ ...
```

**Query Optimization:**
```python
# Bad: Full table scan
df = spark.read.parquet("s3://ml-data-lake/raw/user_events/")
recent_data = df.filter(F.col("date") >= "2025-10-01")

# Good: Partition pruning
df = spark.read.parquet("s3://ml-data-lake/raw/user_events/") \
    .filter(F.col("date") >= "2025-10-01")  # Spark only reads Oct partitions
```

---

### 4. Separate Raw and Processed Data

**Why:**
- Raw data is immutable source of truth
- Enables reprocessing with new logic
- Supports debugging and auditing

**Layer Structure:**
```
s3://ml-data-lake/
    ‚îú‚îÄ‚îÄ raw/           # Immutable, as ingested
    ‚îÇ   ‚îî‚îÄ‚îÄ user_events/
    ‚îú‚îÄ‚îÄ processed/     # Cleaned, validated
    ‚îÇ   ‚îî‚îÄ‚îÄ user_events_clean/
    ‚îî‚îÄ‚îÄ curated/       # Business logic applied, ML-ready
        ‚îî‚îÄ‚îÄ user_features/
```

---

### 5. Monitor Data Collection SLAs

**Metrics to Track:**
- **Ingestion latency:** Time from data generation to availability
- **Data volume:** Rows/bytes ingested per time window
- **Error rates:** Failed ingestions / total attempts
- **Data freshness:** Age of most recent data
- **Schema evolution events:** Unexpected schema changes

**Monitoring Implementation:**
```python
from prometheus_client import Counter, Histogram, Gauge
from datetime import datetime

# Define metrics
ingestion_duration = Histogram(
    'data_ingestion_seconds',
    'Time to ingest data',
    ['source', 'date']
)

ingestion_rows = Counter(
    'data_ingestion_rows_total',
    'Total rows ingested',
    ['source', 'date']
)

ingestion_errors = Counter(
    'data_ingestion_errors_total',
    'Ingestion errors',
    ['source', 'error_type']
)

data_freshness = Gauge(
    'data_freshness_seconds',
    'Age of most recent data',
    ['source']
)

@ingestion_duration.labels(source='user_events', date='2025-10-18').time()
def ingest_data(source, date):
    try:
        df = fetch_from_source(source, date)
        ingestion_rows.labels(source=source, date=date).inc(len(df))
        save_data(df)

        # Update freshness
        max_timestamp = df['timestamp'].max()
        freshness_seconds = (datetime.now() - max_timestamp).total_seconds()
        data_freshness.labels(source=source).set(freshness_seconds)

    except Exception as e:
        ingestion_errors.labels(source=source, error_type=type(e).__name__).inc()
        raise
```

---

## ‚ö†Ô∏è Common Pitfalls

### 1. Ignoring Time Zones

**Problem:** Mixing UTC and local times causes temporal inconsistencies and data leakage.

**Impact:**
- Features computed at wrong time boundaries
- Train/test splits leak future information
- Daylight saving time creates duplicates/gaps

**Solution:**
```python
# ‚ùå WRONG: Mixing time zones
df['timestamp'] = pd.to_datetime(df['timestamp'])  # Timezone-naive

# ‚úÖ CORRECT: Always use UTC
df['timestamp_utc'] = pd.to_datetime(df['timestamp']).dt.tz_localize('UTC')

# Or if already has timezone, convert to UTC
df['timestamp_utc'] = pd.to_datetime(df['timestamp']).dt.tz_convert('UTC')

# Store timestamps as UTC in data lake
df.to_parquet('s3://data/events.parquet',
              coerce_timestamps='us',  # Microsecond precision
              allow_truncated_timestamps=False)
```

---

### 2. Not Handling Late-Arriving Data

**Problem:** Events arrive out of order or delayed, causing incomplete aggregations.

**Example:**
- Mobile app sends event at 10:00 AM
- Network issues delay it
- Event arrives at pipeline at 10:15 AM
- Hourly aggregation for 10:00-11:00 already completed and published

**Solutions:**

**A. Watermarks (Streaming)**
```python
# Allow 10 minutes for late data
streaming_df.withWatermark("event_time", "10 minutes") \
    .groupBy(window("event_time", "1 hour")) \
    .count()
```

**B. Batch Reprocessing Windows**
```python
# Reprocess last 24 hours daily to catch late data
def process_with_lookback(current_date, lookback_days=1):
    """Process current date + reprocess recent days for late arrivals"""
    start_date = (pd.to_datetime(current_date) - pd.Timedelta(days=lookback_days)).strftime('%Y-%m-%d')

    # Read data from lookback window
    df = spark.read.parquet(f"s3://data/events/date>={start_date}")

    # Recompute features
    features = compute_features(df)

    # Upsert to feature store (handles updates)
    upsert_features(features)
```

---

### 3. Hard-Coding Credentials

**Problem:** Security risk, inflexible, violates least privilege.

**Solution:** Use secret management services
```python
import boto3
from botocore.exceptions import ClientError

def get_secret(secret_name, region_name="us-east-1"):
    """Retrieve secret from AWS Secrets Manager"""
    session = boto3.session.Session()
    client = session.client(
        service_name='secretsmanager',
        region_name=region_name
    )

    try:
        get_secret_value_response = client.get_secret_value(SecretId=secret_name)
        secret = json.loads(get_secret_value_response['SecretString'])
        return secret
    except ClientError as e:
        logger.error(f"Failed to retrieve secret: {e}")
        raise

# Usage
db_creds = get_secret("ml-pipeline/db-credentials")
connection_string = f"postgresql://{db_creds['username']}:{db_creds['password']}@{db_creds['host']}/db"
```

---

### 4. Insufficient Error Handling

**Problem:** Pipeline fails silently or catastrophically on minor issues.

**Solution:** Implement graceful degradation and alerting
```python
from enum import Enum

class DataQuality(Enum):
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    FAILED = "failed"

def robust_ingestion(source_name, date):
    """
    Ingest with error handling and quality assessment
    """
    try:
        # Attempt ingestion
        df = fetch_from_source(source_name, date)

        # Assess quality
        quality = assess_quality(df)

        if quality == DataQuality.HIGH:
            save_data(df, f"s3://data/{source_name}/high_quality/")
            logger.info(f"‚úÖ High quality data ingested from {source_name}")

        elif quality == DataQuality.MEDIUM:
            save_data(df, f"s3://data/{source_name}/medium_quality/")
            send_alert(f"‚ö†Ô∏è Medium quality data from {source_name}", severity="warning")

        elif quality == DataQuality.LOW:
            save_data(df, f"s3://data/{source_name}/quarantine/")
            send_alert(f"‚ö†Ô∏è Low quality data quarantined from {source_name}", severity="error")
            # Continue pipeline with previous day's data
            df = load_fallback_data(source_name)

        else:  # FAILED
            raise ValueError(f"Data quality check failed for {source_name}")

        return df

    except Exception as e:
        logger.error(f"‚ùå Ingestion failed for {source_name}: {e}")
        send_alert(f"üö® Ingestion failure: {source_name}", severity="critical")

        # Attempt to use cached/fallback data
        try:
            df = load_fallback_data(source_name)
            logger.info(f"Using fallback data for {source_name}")
            return df
        except:
            raise  # Re-raise if no fallback available
```

---

### 5. Not Planning for Schema Evolution

**Problem:** Source systems add/remove/change columns, breaking pipelines.

**Solution:** Flexible schema handling
```python
class SchemaEvolutionStrategy(Enum):
    STRICT = "strict"          # Fail on any change
    ADD_ONLY = "add_only"      # Allow new columns only
    FLEXIBLE = "flexible"      # Handle all changes gracefully

def handle_schema_evolution(df, expected_schema, strategy=SchemaEvolutionStrategy.FLEXIBLE):
    """
    Handle schema changes based on strategy
    """
    actual_cols = set(df.columns)
    expected_cols = set(expected_schema.keys())

    added = actual_cols - expected_cols
    removed = expected_cols - actual_cols

    if strategy == SchemaEvolutionStrategy.STRICT:
        if added or removed:
            raise ValueError(f"Schema changed! Added: {added}, Removed: {removed}")

    elif strategy == SchemaEvolutionStrategy.ADD_ONLY:
        if removed:
            raise ValueError(f"Columns removed: {removed}")
        # Drop new columns
        df = df[list(expected_cols & actual_cols)]

    elif strategy == SchemaEvolutionStrategy.FLEXIBLE:
        # Add missing columns with defaults
        for col in removed:
            df[col] = None
        # Drop unexpected columns
        df = df[list(expected_cols)]
        # Reorder to match expected schema
        df = df[list(expected_schema.keys())]

    return df
```

---

## üèãÔ∏è Hands-On Exercises

### Exercise 1: Build a Multi-Source Data Collection Pipeline

**Objective:** Implement a production-ready data collection pipeline that ingests from multiple sources.

**Instructions:**

1. **Set Up Data Sources**
   - Create a PostgreSQL database with sample user and transaction tables
   - Set up a simple REST API that returns JSON data (use mocky.io or JSONPlaceholder)
   - Generate sample CSV files with time-series data

2. **Implement Collection Logic**
   - Write a Python script to ingest from all three sources
   - Handle errors gracefully (retry logic, logging)
   - Save data to local Parquet files with date partitioning

3. **Add Data Quality Checks**
   - Validate schema matches expected structure
   - Check for null values in critical columns
   - Detect duplicates

4. **Implement Idempotency**
   - Ensure running the script multiple times doesn't create duplicates
   - Use deduplication logic

**Deliverable:**
- Working Python script(s)
- Sample output data files
- Log file showing successful ingestion

**Time Estimate:** 4-6 hours

---

### Exercise 2: Streaming Data Pipeline with Kafka

**Objective:** Build a real-time streaming pipeline to process events.

**Setup:**
```bash
# Start Kafka locally
docker-compose up -d kafka zookeeper
```

**Tasks:**

1. **Event Producer**
   - Write Python script to generate simulated user events (clicks, purchases)
   - Publish 100 events/second to Kafka topic
   - Include proper event schema (user_id, timestamp, event_type, metadata)

2. **Event Consumer**
   - Create consumer that reads from Kafka
   - Perform basic aggregations (events per minute, unique users)
   - Write results to local storage

3. **Handle Late Data**
   - Simulate late-arriving events (events with past timestamps)
   - Implement watermarking logic
   - Verify aggregations are correct

**Deliverable:**
- Producer script
- Consumer script
- Sample output showing correct aggregations

**Time Estimate:** 5-7 hours

---

### Exercise 3: Data Leakage Detection Challenge

**Objective:** Identify and fix data leakage in feature engineering code.

**Scenario:** You're given buggy feature engineering code that contains data leakage.

**Buggy Code:**
```python
import pandas as pd
from sklearn.preprocessing import StandardScaler

def create_features(df):
    # Calculate lifetime value
    df['ltv'] = df.groupby('user_id')['amount'].transform('sum')

    # Normalize features using all data
    scaler = StandardScaler()
    df[['ltv', 'amount']] = scaler.fit_transform(df[['ltv', 'amount']])

    # Create target
    df['will_churn_30d'] = (df['last_active_date'] < df['current_date'] - pd.Timedelta(days=30))

    return df
```

**Tasks:**

1. **Identify All Leakage**
   - List every instance of data leakage in the code
   - Explain why each is problematic

2. **Fix the Code**
   - Implement point-in-time correct feature computation
   - Ensure scaling uses only training data statistics
   - Fix target variable definition

3. **Validate**
   - Create test data that proves your fix works
   - Show that features are different with leakage vs. without

**Deliverable:**
- Document listing all leakage issues
- Fixed code
- Test results

**Time Estimate:** 2-3 hours

---

### Exercise 4: Schema Evolution Handling

**Objective:** Build a pipeline that gracefully handles schema changes.

**Setup:**
- Create sample data files representing different schema versions
  - v1: `user_id, timestamp, event_type`
  - v2: Adds `device_type`
  - v3: Adds `session_id`, removes `event_type`, renames `device_type` to `device`

**Tasks:**

1. **Implement Schema Handler**
   - Write code that detects schema version
   - Aligns all versions to a common schema
   - Fills missing columns with appropriate defaults

2. **Test All Scenarios**
   - Test with each schema version
   - Test with mixed versions in same batch
   - Verify output schema is consistent

3. **Add Monitoring**
   - Log when schema changes detected
   - Alert on unexpected changes
   - Track schema version distribution

**Deliverable:**
- Schema handler code
- Test files for all versions
- Test results showing successful handling

**Time Estimate:** 3-4 hours

---

## üîó Related Concepts

- [[02. Data Storage and Versioning Strategies.md|Next: Data Storage and Versioning]]
- [[../01. Introduction to DE for AI-ML/01. The Role of Data Engineering in ML.md|DE Role in ML]]
- [[../04. Data Storage for ML/README.md|Chapter 4: Data Storage]]
- [[../08. Stream Processing & Real-Time ML/README.md|Chapter 8: Stream Processing]]

---

## üìö Further Reading

### Essential Books

1. **"Streaming Systems" by Tyler Akidau, Slava Chernyak, Reuven Lax** (O'Reilly, 2018)
   - Authoritative guide to stream processing
   - Covers watermarking, windowing, and late data handling
   - Essential for understanding real-time ML data pipelines

2. **"Designing Data-Intensive Applications" by Martin Kleppmann** (O'Reilly, 2017)
   - Covers data system fundamentals
   - Batch vs. streaming processing
   - Data integration patterns

3. **"Data Pipelines Pocket Reference" by James Densmore** (O'Reilly, 2021)
   - Practical guide to building data pipelines
   - ETL/ELT patterns
   - Modern data stack

### Key Papers & Articles

1. **"The Dataflow Model" - Google (2015)**
   - https://research.google/pubs/pub43864/
   - Foundation for Apache Beam and Google Cloud Dataflow
   - Unified batch and streaming processing model

2. **"Lakehouse: A New Generation of Open Platforms" - Databricks (2020)**
   - https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf
   - Delta Lake architecture
   - Combining data lakes and warehouses for ML

3. **"Data Management Challenges in Production Machine Learning" - Google (2017)**
   - https://research.google/pubs/pub46178/
   - Real-world data challenges at scale
   - Data validation and monitoring

### Blogs & Online Resources

1. **Confluent Blog - Kafka and Streaming**
   - https://www.confluent.io/blog/
   - Kafka patterns and best practices
   - Event-driven architectures

2. **AWS Big Data Blog**
   - https://aws.amazon.com/blogs/big-data/
   - Real-world AWS data pipeline architectures
   - Kinesis, Glue, EMR patterns

3. **The Netflix Tech Blog - Data Platform**
   - https://netflixtechblog.com/tagged/data-platform
   - How Netflix handles massive data scale
   - Real-time and batch processing

4. **Uber Engineering Blog - Data Infrastructure**
   - https://eng.uber.com/category/articles/data/
   - Uber's data platform evolution
   - Real-time ML at scale

### Documentation to Bookmark

1. **Apache Kafka Documentation**
   - https://kafka.apache.org/documentation/
   - Essential for streaming ML pipelines

2. **Apache Spark Structured Streaming**
   - https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
   - Unified batch and streaming API

3. **AWS Kinesis Documentation**
   - https://docs.aws.amazon.com/kinesis/
   - Managed streaming service

4. **Debezium Documentation**
   - https://debezium.io/documentation/
   - CDC for various databases

---

## üìù Key Takeaways

### 1. Data Collection is the Foundation of ML
All ML starts with data collection. Poor collection strategies lead to incomplete, biased, or stale data that dooms models from the start. Invest heavily in robust, scalable collection infrastructure.

### 2. Batch and Streaming are Complementary
Most production ML systems require both:
- **Batch** for complete historical features and training data
- **Streaming** for real-time features and low-latency serving
- **Hybrid architectures** (Lambda) balance these needs

### 3. Temporal Consistency is Critical
Data leakage from future information is a silent killer of ML projects:
- Always implement point-in-time correct feature computation
- Use feature stores that enforce temporal consistency
- Validate that training and serving use the same time horizons

### 4. Data Quality Must Be Checked at Ingestion
Catching bad data early prevents cascading failures:
- Implement schema validation at the source
- Check for nulls, outliers, and value ranges
- Monitor data freshness and volume
- Alert on quality degradations immediately

### 5. Plan for Schema Evolution
Data sources change‚Äîfields are added, renamed, removed:
- Build flexible schema handling into pipelines
- Monitor for schema drift
- Have strategies for backward compatibility
- Document schema versions

### 6. Deduplication is Harder Than It Looks
Duplicates creep in from many sources:
- Event producers with retry logic
- Multiple ingestion paths
- Network issues
- Use event IDs, timestamps, and windowing for deduplication
- Consider idempotent processing patterns

### 7. Different ML Use Cases Require Different Architectures

| Use Case | Latency | Architecture |
|----------|---------|-------------|
| Fraud detection | <100ms | Streaming + pre-computed features |
| Recommendations | 1-10 min | Micro-batch or streaming |
| Churn prediction | Daily | Batch processing |
| Demand forecasting | Hourly | Scheduled batch |

Design your collection strategy based on actual requirements, not assumptions.

### 8. Monitoring is Non-Negotiable
You must actively monitor:
- Ingestion latency (data freshness)
- Data volume (detect anomalies)
- Error rates (pipeline health)
- Schema changes (prevent breaking changes)
- Data quality metrics (null rates, distributions)

Set up alerts before going to production.

---

## ‚úèÔ∏è Notes Section

**Use this space to document your personal insights, questions, and learnings as you work through this material.**

### My Key Insights:
-
-
-

### Questions to Explore Further:
-
-
-

### How This Applies to My Work:
-
-
-

### Tools to Investigate:
-
-
-

### Action Items:
- [ ] Set up a test Kafka cluster to practice streaming ingestion
- [ ] Implement schema validation for my current data pipelines
- [ ] Research and compare feature store solutions (Feast vs. Tecton)
- [ ] Create a deduplication strategy for my event streams
-

### Real-World Examples to Study:
- Uber's Michelangelo platform (end-to-end ML)
- Netflix's data pipeline architecture
- Airbnb's Zipline (feature engineering)
-

---

## üîó Related Concepts

- [[02. Data Storage and Versioning Strategies.md|Next: Data Storage and Versioning]]
- [[../01. Introduction to DE for AI-ML/01. The Role of Data Engineering in ML.md|DE Role in ML]]
- [[../04. Data Storage for ML/README.md|Chapter 4: Data Storage]]

---

*Created: October 18, 2025*
*Last Updated: October 18, 2025*
*Status: ‚úÖ Completed - Ready for study*
