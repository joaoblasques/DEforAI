# 02. Data Storage and Versioning Strategies

**Chapter:** ML Data Pipeline Lifecycle
**Topic:** Managing ML datasets with versioning, reproducibility, and lineage

---

## üìã Overview

Data versioning is foundational to reproducible ML systems. Unlike traditional software where code versioning suffices, ML requires versioning data, features, models, and their relationships. This subchapter explores storage patterns and versioning strategies that enable reproducible experiments, debugging, and compliance.

**Key Insight from Industry (2024-2025):** Model and data versioning tools like DVC and LakeFS help manage datasets and model versions, ensuring reproducibility and traceability in ML workflows. By integrating these tools with MLflow, you can create a scalable, reproducible, and traceable ML pipeline.

---

## üéØ Learning Objectives

After completing this subchapter, you will be able to:
- Design appropriate storage architectures for ML data (data lakes, lakehouses, feature stores)
- Implement data versioning using DVC, LakeFS, or Git-LFS
- Establish data lineage tracking for compliance and debugging
- Choose optimal data formats for ML workloads (Parquet, Avro, Delta Lake)
- Manage dataset lifecycles (retention, archival, deletion)
- Integrate versioning with experiment tracking (MLflow, Weights & Biases)

---

## üìö Core Concepts

### 1. Storage Architectures for ML

#### A. Data Lake

**Definition:** Centralized repository storing raw data in native format at scale.

**Characteristics:**
- Schema-on-read (no predefined schema)
- Stores structured, semi-structured, and unstructured data
- Object storage (S3, GCS, Azure Blob)
- Cost-effective for large volumes
- Separation of compute and storage

**ML Use Cases:**
- Raw training data archive
- Historical feature computation
- Model artifacts and experiment logs
- Data science exploration

**Architecture:**
```
Raw Data ‚Üí Data Lake (S3/GCS) ‚Üí Processing (Spark) ‚Üí Curated Data ‚Üí Feature Store
                ‚Üì
         Data Catalog (Glue/Hive Metastore)
```

**Implementation Example (AWS):**
```python
import boto3
from datetime import datetime

class MLDataLake:
    """
    Manage ML data in S3-based data lake with organized structure
    """

    def __init__(self, bucket_name: str, region: str = 'us-east-1'):
        self.bucket = bucket_name
        self.s3_client = boto3.client('s3', region_name=region)

    def write_raw_data(self, data: bytes, source: str, date: str, file_name: str):
        """
        Write raw ingested data to landing zone

        Path structure: s3://bucket/raw/{source}/date={date}/{file_name}
        """
        key = f"raw/{source}/date={date}/{file_name}"

        self.s3_client.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=data,
            Metadata={
                'source': source,
                'ingestion_timestamp': datetime.now().isoformat(),
                'data_type': 'raw'
            }
        )

        return f"s3://{self.bucket}/{key}"

    def write_processed_data(self, df, dataset_name: str, version: str, partition_cols: list = None):
        """
        Write processed/cleaned data to curated zone with versioning

        Path: s3://bucket/processed/{dataset_name}/version={version}/
        """
        import pyarrow.parquet as pq
        import pyarrow as pa

        # Convert to PyArrow Table
        table = pa.Table.from_pandas(df)

        # Define partitioning
        partition_cols = partition_cols or []

        # Write partitioned parquet
        output_path = f"s3://{self.bucket}/processed/{dataset_name}/version={version}/"

        pq.write_to_dataset(
            table,
            root_path=output_path,
            partition_cols=partition_cols,
            compression='snappy',
            use_dictionary=True,
            write_statistics=True
        )

        print(f"‚úÖ Wrote {len(df)} rows to {output_path}")
        return output_path

# Usage
data_lake = MLDataLake(bucket_name='ml-data-lake-prod')

# Write raw data
data_lake.write_raw_data(
    data=raw_json_bytes,
    source='user_events',
    date='2025-10-18',
    file_name='events_batch_001.json'
)

# Write processed data with versioning
data_lake.write_processed_data(
    df=cleaned_dataframe,
    dataset_name='user_features',
    version='v2.3',
    partition_cols=['date', 'region']
)
```

**Best Practices:**
- Organize by data lifecycle stage: `raw/` ‚Üí `processed/` ‚Üí `curated/`
- Partition by date for efficient querying and retention
- Use data catalogs (AWS Glue, Hive Metastore) for discoverability
- Implement lifecycle policies for cost optimization

---

#### B. Data Lakehouse (Modern Approach)

**Definition:** Combines data lake flexibility with data warehouse reliability using table formats like Delta Lake, Iceberg, or Hudi.

**Key Features:**
- ACID transactions on data lakes
- Schema enforcement and evolution
- Time travel (query historical versions)
- Upserts and deletes (unlike append-only data lakes)
- Unified batch and streaming

**Popular Technologies:**
- **Delta Lake** (Databricks)
- **Apache Iceberg** (Netflix, Apple)
- **Apache Hudi** (Uber)

**Delta Lake Example:**
```python
from delta import *
from pyspark.sql import SparkSession

# Create Spark with Delta support
builder = SparkSession.builder.appName("DeltaLakeML") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = configure_spark_with_delta_pip(builder).getOrCreate()

# Write data as Delta table
df.write.format("delta") \
    .mode("overwrite") \
    .partitionBy("date") \
    .option("overwriteSchema", "true") \
    .save("s3://ml-lakehouse/training_data/user_features/")

# Read specific version (time travel)
df_v1 = spark.read.format("delta") \
    .option("versionAsOf", 1) \
    .load("s3://ml-lakehouse/training_data/user_features/")

# Read as of specific timestamp
df_historical = spark.read.format("delta") \
    .option("timestampAsOf", "2025-09-01") \
    .load("s3://ml-lakehouse/training_data/user_features/")

# Update specific records (ACID transaction)
from delta.tables import *

deltaTable = DeltaTable.forPath(spark, "s3://ml-lakehouse/training_data/user_features/")

deltaTable.update(
    condition = "user_segment = 'high_value'",
    set = {"priority_score": "priority_score * 1.2"}
)

# View table history
deltaTable.history().show()

# Vacuum old files (remove versions older than 30 days)
deltaTable.vacuum(retentionHours=720)  # 30 days
```

**Why Lakehouse for ML:**
1. **Time Travel:** Reproduce exact training data from any past date
2. **Schema Evolution:** Add features without breaking existing pipelines
3. **ACID:** Ensure consistency when multiple pipelines write to same table
4. **Unified:** Single format for batch training and streaming inference

---

#### C. Feature Store

**Definition:** Specialized storage for ML features, serving both offline (training) and online (inference) workloads.

**Architecture:**
```
                     ‚îå‚îÄ‚îÄ> Offline Store (S3/Snowflake)
Feature Pipelines ‚îÄ‚îÄ‚î§        - Historical features for training
                     ‚îÇ        - Columnar format (Parquet)
                     ‚îî‚îÄ‚îÄ> Online Store (Redis/DynamoDB)
                              - Latest features for inference
                              - Key-value, low-latency (<10ms)
```

**Key Capabilities:**
- **Dual Storage:** Offline (historical) + Online (real-time)
- **Feature Registry:** Catalog with metadata, ownership, SLAs
- **Point-in-Time Joins:** Prevent data leakage
- **Monitoring:** Track feature distributions and drift

**Popular Solutions:**
- **Feast** (Open-source)
- **Tecton**
- **AWS SageMaker Feature Store**
- **Databricks Feature Store**
- **Hopsworks**

**Feature Store Example (Feast):**
```python
from feast import FeatureStore, Entity, FeatureView, Field
from feast.types import Float32, Int64, String
from feast.infra.offline_stores.file_source import FileSource
from datetime import timedelta

# Define entity (what features are about)
user = Entity(
    name="user",
    join_keys=["user_id"],
    description="User entity for ML models"
)

# Define feature source (offline)
user_features_source = FileSource(
    path="s3://feature-data/user_features/",
    timestamp_field="event_timestamp"
)

# Define feature view (logical grouping of features)
user_features_view = FeatureView(
    name="user_features",
    entities=[user],
    ttl=timedelta(days=365),
    schema=[
        Field(name="lifetime_value", dtype=Float32),
        Field(name="num_purchases", dtype=Int64),
        Field(name="avg_order_value", dtype=Float32),
        Field(name="days_since_last_purchase", dtype=Int64),
        Field(name="favorite_category", dtype=String)
    ],
    source=user_features_source,
    online=True  # Enable online serving
)

# Initialize feature store
store = FeatureStore(repo_path=".")

# Register features
store.apply([user, user_features_view])

# Get historical features for training (point-in-time correct)
training_df = store.get_historical_features(
    entity_df=entity_df,  # DataFrame with user_id and event_timestamp
    features=[
        "user_features:lifetime_value",
        "user_features:num_purchases",
        "user_features:avg_order_value"
    ]
).to_df()

# Materialize to online store for serving
store.materialize_incremental(end_date=datetime.now())

# Get online features for inference (low-latency)
online_features = store.get_online_features(
    features=[
        "user_features:lifetime_value",
        "user_features:num_purchases"
    ],
    entity_rows=[{"user_id": "user_123"}]
).to_dict()
```

---

### 2. Data Versioning Strategies

#### Why Version Data?

1. **Reproducibility:** Re-run experiments with exact same data
2. **Debugging:** Identify when model degradation started
3. **Compliance:** Audit trail for regulatory requirements
4. **Collaboration:** Teams can work on different dataset versions
5. **Rollback:** Revert to previous version if issues found

**What to Version:**
- Raw datasets
- Processed/cleaned datasets
- Feature sets
- Train/val/test splits
- Model inputs and outputs

---

#### A. DVC (Data Version Control)

**Best For:** Dataset versioning integrated with Git, local and cloud storage support.

**How It Works:**
- Metadata stored in Git (`.dvc` files)
- Actual data stored in remote storage (S3, GCS, Azure)
- Git-like commands: `dvc add`, `dvc push`, `dvc pull`, `dvc checkout`

**Setup and Usage:**
```bash
# Install DVC
pip install dvc[s3]

# Initialize DVC in your repo
git init
dvc init

# Configure remote storage
dvc remote add -d myremote s3://my-dvc-storage/datasets

# Add dataset to DVC
dvc add data/training_set.csv

# This creates:
# - data/training_set.csv.dvc (metadata, tracked by Git)
# - data/.gitignore (prevents Git from tracking large file)

# Commit metadata to Git
git add data/training_set.csv.dvc data/.gitignore
git commit -m "Add training dataset v1"

# Push data to remote storage
dvc push

# On another machine, pull data
git pull
dvc pull

# Switch to specific version
git checkout v1.0
dvc checkout  # Downloads data for that version
```

**DVC Pipeline (Reproducible ML Workflows):**
```yaml
# dvc.yaml - defines ML pipeline stages
stages:
  prepare_data:
    cmd: python src/prepare.py
    deps:
      - src/prepare.py
      - data/raw/
    outs:
      - data/processed/

  train:
    cmd: python src/train.py
    deps:
      - src/train.py
      - data/processed/
    params:
      - train.learning_rate
      - train.epochs
    outs:
      - models/model.pkl
    metrics:
      - metrics/train_metrics.json:
          cache: false

# Run entire pipeline
# dvc repro

# View metrics across experiments
# dvc metrics show
```

**Integration with MLflow:**
```python
import mlflow
import dvc.api

# Track dataset version in MLflow
with mlflow.start_run():
    # Get DVC-tracked data version
    data_version = dvc.api.get_url('data/training_set.csv', rev='HEAD')

    # Log dataset version
    mlflow.log_param("dataset_version", data_version)
    mlflow.log_param("dvc_commit", subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip())

    # Train model
    model = train_model(data)

    # Log model
    mlflow.sklearn.log_model(model, "model")
```

---

#### B. LakeFS

**Best For:** Git-like versioning for entire data lakes, branch-merge workflows for data.

**Key Features:**
- Isolated branches for experiments (no data copying)
- Atomic commits across multiple files
- Zero-copy branching (efficient for TB-scale data)
- Merge branches after validation

**Architecture:**
```
Data Lake (S3)
      ‚Üì
   LakeFS
      ‚Üì
 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 ‚Üì         ‚Üì
main    experiment-branch
         (isolated view)
```

**LakeFS CLI Usage:**
```bash
# Create branch for experiment
lakefs branch create lakefs://my-repo/experiment-1 --source lakefs://my-repo/main

# Write data to branch (isolated from main)
aws s3 cp new_features.parquet s3://my-bucket/my-repo/experiment-1/features/

# Commit changes
lakefs commit lakefs://my-repo/experiment-1 -m "Add new user engagement features"

# If experiment successful, merge to main
lakefs merge lakefs://my-repo/experiment-1 lakefs://my-repo/main

# If experiment failed, delete branch (no impact on main)
lakefs branch delete lakefs://my-repo/experiment-1
```

**Python SDK:**
```python
import lakefs_client
from lakefs_client.client import LakeFSClient

# Initialize client
client = LakeFSClient(
    configuration=lakefs_client.Configuration(
        host='https://lakefs.company.com',
        username='admin',
        password='password'
    )
)

# Create branch for new experiment
client.branches.create_branch(
    repository='ml-datasets',
    branch_creation=lakefs_client.models.BranchCreation(
        name='feature-engineering-v2',
        source='main'
    )
)

# Read from specific branch
df = spark.read.parquet('s3://my-bucket/ml-datasets/feature-engineering-v2/user_features/')

# Commit changes
client.commits.commit(
    repository='ml-datasets',
    branch='feature-engineering-v2',
    commit_creation=lakefs_client.models.CommitCreation(
        message='Add behavioral features',
        metadata={'author': 'data-eng-team', 'experiment_id': 'exp-42'}
    )
)

# Compare branches
diff = client.refs.diff_refs(
    repository='ml-datasets',
    left_ref='main',
    right_ref='feature-engineering-v2'
)
```

---

#### C. MLflow Dataset Tracking

**Best For:** Lightweight dataset tracking integrated with experiment management.

**Approach:**
- Log dataset hash/signature with each experiment
- Track dataset location and version
- Link datasets to model artifacts

**Implementation:**
```python
import mlflow
import hashlib
import pandas as pd

def compute_dataset_hash(df: pd.DataFrame) -> str:
    """Compute hash of dataset for versioning"""
    return hashlib.md5(pd.util.hash_pandas_object(df).values).hexdigest()

def log_dataset_with_mlflow(df: pd.DataFrame, dataset_name: str, dataset_path: str):
    """
    Log dataset metadata with MLflow experiment
    """
    # Compute dataset signature
    dataset_hash = compute_dataset_hash(df)

    # Log dataset info
    mlflow.log_param(f"{dataset_name}_hash", dataset_hash)
    mlflow.log_param(f"{dataset_name}_path", dataset_path)
    mlflow.log_param(f"{dataset_name}_rows", len(df))
    mlflow.log_param(f"{dataset_name}_cols", len(df.columns))

    # Log dataset schema
    schema = {col: str(dtype) for col, dtype in df.dtypes.items()}
    mlflow.log_dict(schema, f"{dataset_name}_schema.json")

    # Log sample of data
    df.head(100).to_csv(f"{dataset_name}_sample.csv", index=False)
    mlflow.log_artifact(f"{dataset_name}_sample.csv")

    return dataset_hash

# Usage in training pipeline
with mlflow.start_run():
    # Load data
    train_df = pd.read_parquet('s3://data/train.parquet')
    val_df = pd.read_parquet('s3://data/val.parquet')

    # Log datasets
    train_hash = log_dataset_with_mlflow(train_df, "train", "s3://data/train.parquet")
    val_hash = log_dataset_with_mlflow(val_df, "val", "s3://data/val.parquet")

    # Train model
    model = train(train_df)

    # Log model with dataset lineage
    mlflow.sklearn.log_model(
        model,
        "model",
        metadata={"train_dataset_hash": train_hash, "val_dataset_hash": val_hash}
    )
```

---

### 3. Data Lineage and Metadata Management

**Data Lineage:** Track data flow from sources ‚Üí transformations ‚Üí models ‚Üí predictions.

**Why It Matters:**
- **Root Cause Analysis:** Trace errors back to source
- **Impact Analysis:** Understand downstream effects of data changes
- **Compliance:** Demonstrate data governance (GDPR, CCPA)
- **Trust:** Understand data provenance

**Lineage Tracking Tools:**
- **Apache Atlas**
- **DataHub (LinkedIn)**
- **Amundsen (Lyft)**
- **OpenMetadata**
- **Cloud-native:** AWS Glue Data Catalog, GCP Data Catalog

**Example: OpenLineage Integration:**
```python
from openlineage.client import OpenLineageClient
from openlineage.client.run import RunEvent, RunState, Run, Job
from openlineage.client.facet import DataSourceDatasetFacet

client = OpenLineageClient(url="http://openlineage-server:5000")

# Define job
job = Job(namespace="ml-pipelines", name="user-feature-engineering")

# Define run
run = Run(runId="feature-eng-2025-10-18-001")

# Track start of job
start_event = RunEvent(
    eventType=RunState.START,
    eventTime="2025-10-18T10:00:00Z",
    run=run,
    job=job,
    inputs=[
        Dataset(
            namespace="s3://raw-data",
            name="transactions",
            facets={
                "dataSource": DataSourceDatasetFacet(
                    name="production-db",
                    uri="postgresql://prod-db:5432/transactions"
                )
            }
        )
    ],
    outputs=[]
)

client.emit(start_event)

# ... run feature engineering ...

# Track completion
complete_event = RunEvent(
    eventType=RunState.COMPLETE,
    eventTime="2025-10-18T10:15:00Z",
    run=run,
    job=job,
    inputs=[...],  # Same as start
    outputs=[
        Dataset(
            namespace="s3://feature-store",
            name="user_features",
            facets={
                "schema": SchemaDatasetFacet(
                    fields=[
                        SchemaField(name="user_id", type="STRING"),
                        SchemaField(name="lifetime_value", type="DOUBLE")
                    ]
                ),
                "dataQuality": DataQualityMetricsDatasetFacet(
                    rowCount=1000000,
                    columnMetrics={
                        "lifetime_value": {"mean": 1234.56, "std": 567.89}
                    }
                )
            }
        )
    ]
)

client.emit(complete_event)
```

---

### 4. Optimal Data Formats for ML

| Format | Best For | Pros | Cons |
|--------|----------|------|------|
| **Parquet** | Analytics, training data | Columnar, compressed, splittable | Not human-readable |
| **Avro** | Streaming, schema evolution | Row-based, compact, schema included | Slower for analytics |
| **CSV** | Small datasets, exploration | Human-readable, universal | No schema, inefficient, no compression |
| **TFRecord** | TensorFlow training | Optimized for TF, streaming | TensorFlow-specific |
| **Petastorm** | Distributed DL training | Parquet + DL optimizations | Complex setup |
| **Delta/Iceberg** | Lakehouse, ACID needed | Time travel, updates, schema evolution | Requires ecosystem support |

**Parquet Example (Optimized for ML):**
```python
import pyarrow as pa
import pyarrow.parquet as pq

# Write with ML-friendly settings
table = pa.Table.from_pandas(df)

pq.write_table(
    table,
    'training_data.parquet',
    compression='snappy',          # Good compression/speed tradeoff
    use_dictionary=True,           # Dict encoding for string columns
    write_statistics=True,         # Min/max stats for pruning
    row_group_size=100000,         # Optimize for parallel reading
    data_page_size=1024*1024       # 1MB pages
)

# Read with predicate pushdown
filtered_table = pq.read_table(
    'training_data.parquet',
    columns=['user_id', 'features', 'label'],  # Column pruning
    filters=[('signup_date', '>=', '2025-01-01')]  # Row filtering
)
```

---

## üí° Practical Examples

### Example 1: E-Commerce Recommendation System Data Management

**Scenario:** Online retailer building personalized product recommendations

**Data Architecture:**
```
Raw Data Lake (S3)
‚îú‚îÄ‚îÄ user_events/ (clickstream, purchases)
‚îú‚îÄ‚îÄ product_catalog/ (inventory, descriptions)
‚îî‚îÄ‚îÄ user_profiles/ (demographics, preferences)
    ‚Üì
Delta Lake (Lakehouse)
‚îú‚îÄ‚îÄ curated_events/ (cleaned, deduplicated)
‚îî‚îÄ‚îÄ aggregated_features/ (user behaviors, product stats)
    ‚Üì
Feature Store (Feast)
‚îú‚îÄ‚îÄ Offline: Historical features for training
‚îî‚îÄ‚îÄ Online: Real-time features for inference
```

**Versioning Strategy:**
- **DVC** for training datasets (snapshots tied to model versions)
- **Delta Lake** time travel for feature tables
- **Git** for feature definitions and transformation code

**Data Lifecycle:**
```python
# 1. Ingest raw events to Delta Lake
spark.readStream \
    .format("kafka") \
    .option("subscribe", "user-events") \
    .load() \
    .writeStream \
    .format("delta") \
    .option("checkpointLocation", "s3://checkpoints/events") \
    .start("s3://lakehouse/raw_events")

# 2. Create versioned training dataset with DVC
# Extract features for model v2.0
df_train = create_features(start_date='2025-01-01', end_date='2025-09-01')
df_train.to_parquet('data/train_v2.0.parquet')

# Version with DVC
!dvc add data/train_v2.0.parquet
!git add data/train_v2.0.parquet.dvc
!git commit -m "Training data v2.0 - added behavioral features"
!dvc push

# 3. Materialize features to online store
feature_store.materialize(
    start_date=datetime(2025, 1, 1),
    end_date=datetime.now()
)
```

---

### Example 2: Fraud Detection with Audit Trail

**Scenario:** Financial institution detecting fraudulent transactions with regulatory compliance requirements

**Requirements:**
- Complete audit trail of all data changes
- Ability to reproduce any model decision
- Point-in-time feature retrieval

**Implementation:**
```python
# Use LakeFS for full data lineage
import lakefs

# Create branch for new fraud features
lakefs.branch_create(
    repo='fraud-detection',
    branch='new-velocity-features',
    source='main'
)

# Develop features in isolation
df_velocity = compute_velocity_features(
    transactions_path='lakefs://fraud-detection/new-velocity-features/transactions/'
)

# Commit with audit metadata
lakefs.commit(
    repo='fraud-detection',
    branch='new-velocity-features',
    message='Add transaction velocity features',
    metadata={
        'author': 'fraud-team',
        'jira_ticket': 'FRAUD-123',
        'approved_by': 'compliance-team',
        'model_version': 'v3.2',
        'audit_timestamp': datetime.now().isoformat()
    }
)

# Track dataset lineage with every model prediction
def predict_with_lineage(transaction_id: str, model_version: str):
    # Retrieve features
    features = feature_store.get_online_features(
        entity_rows=[{"transaction_id": transaction_id}]
    )

    # Make prediction
    prediction = model.predict(features)

    # Log lineage
    lineage_db.insert({
        'transaction_id': transaction_id,
        'prediction': prediction,
        'model_version': model_version,
        'feature_version': features.metadata['version'],
        'data_commit': lakefs.get_commit_id('fraud-detection', 'main'),
        'timestamp': datetime.now().isoformat()
    })

    return prediction
```

---

### Example 3: Multi-Region ML System with Data Consistency

**Scenario:** Global service requiring low-latency predictions across regions while maintaining consistent feature computation

**Architecture:**
```
                    Global Feature Pipeline (Airflow)
                              ‚Üì
                      Central Data Lake (S3)
                              ‚Üì
                         Delta Lake
                    (versioned, ACID updates)
                              ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì                     ‚Üì                     ‚Üì
   US-East              EU-West              APAC-Southeast
Feature Store       Feature Store          Feature Store
(Redis replica)    (Redis replica)       (Redis replica)
```

**Version Synchronization:**
```python
from delta.tables import DeltaTable

# Central feature computation
def compute_and_publish_features(date: str):
    # Compute features
    df_features = spark.sql(f"""
        SELECT user_id,
               AVG(purchase_amount) as avg_purchase,
               COUNT(*) as num_purchases
        FROM transactions
        WHERE date = '{date}'
        GROUP BY user_id
    """)

    # Write to Delta Lake with version tag
    df_features.write.format("delta") \
        .mode("overwrite") \
        .option("replaceWhere", f"date = '{date}'") \
        .save("s3://global-features/user_metrics/")

    # Get version number
    delta_table = DeltaTable.forPath(spark, "s3://global-features/user_metrics/")
    version = delta_table.history(1).select("version").collect()[0][0]

    # Publish version to all regions
    for region in ['us-east', 'eu-west', 'apac-southeast']:
        publish_to_feature_store(
            region=region,
            data_path=f"s3://global-features/user_metrics/",
            version=version,
            date=date
        )

    return version

# Each region serves same version
def get_features_with_version_check(user_id: str, region: str):
    features = redis_client[region].hgetall(f"user:{user_id}")

    # Verify version consistency
    expected_version = config.get_latest_version()
    actual_version = features.get('_version')

    if actual_version != expected_version:
        logger.warning(f"Version mismatch in {region}: {actual_version} != {expected_version}")
        # Fallback to batch features
        return get_batch_features(user_id)

    return features
```

---

## üîß Code Examples

### Complete Data Versioning Workflow

```python
import mlflow
import dvc.api
from delta.tables import DeltaTable
import hashlib
from datetime import datetime

class VersionedMLPipeline:
    """
    End-to-end ML pipeline with comprehensive versioning
    """

    def __init__(self, project_name: str):
        self.project_name = project_name
        self.spark = self._init_spark()

    def create_training_dataset(
        self,
        source_tables: list,
        version: str,
        description: str
    ):
        """
        Create versioned training dataset with full lineage
        """
        with mlflow.start_run(run_name=f"dataset-{version}"):
            # Extract data from Delta Lake with time travel
            datasets = {}
            for table_name, table_config in source_tables:
                df = self.spark.read.format("delta") \
                    .option("versionAsOf", table_config.get("version")) \
                    .load(table_config["path"])

                datasets[table_name] = df

                # Log source metadata
                mlflow.log_param(f"{table_name}_version", table_config.get("version"))
                mlflow.log_param(f"{table_name}_path", table_config["path"])

            # Join and transform
            df_train = self._join_and_transform(datasets)

            # Compute dataset hash
            dataset_hash = self._compute_dataset_hash(df_train)

            # Write to Delta Lake
            output_path = f"s3://ml-datasets/{self.project_name}/training/{version}/"
            df_train.write.format("delta") \
                .mode("overwrite") \
                .save(output_path)

            # Track with DVC
            self._track_with_dvc(output_path, version)

            # Log comprehensive metadata
            mlflow.log_param("dataset_version", version)
            mlflow.log_param("dataset_hash", dataset_hash)
            mlflow.log_param("dataset_rows", df_train.count())
            mlflow.log_param("dataset_path", output_path)
            mlflow.log_param("description", description)
            mlflow.log_param("created_at", datetime.now().isoformat())

            # Log data quality metrics
            quality_metrics = self._compute_quality_metrics(df_train)
            mlflow.log_metrics(quality_metrics)

            return {
                "version": version,
                "path": output_path,
                "hash": dataset_hash,
                "mlflow_run_id": mlflow.active_run().info.run_id
            }

    def train_with_versioned_data(
        self,
        dataset_version: str,
        model_params: dict
    ):
        """
        Train model with specific dataset version
        """
        with mlflow.start_run(run_name=f"train-{dataset_version}"):
            # Load versioned dataset
            dataset_info = self._get_dataset_metadata(dataset_version)
            df_train = self.spark.read.format("delta") \
                .load(dataset_info["path"])

            # Log dataset lineage
            mlflow.log_param("dataset_version", dataset_version)
            mlflow.log_param("dataset_hash", dataset_info["hash"])
            mlflow.log_param("dataset_mlflow_run", dataset_info["mlflow_run_id"])

            # Train model
            model = self._train_model(df_train, model_params)

            # Version model with dataset reference
            mlflow.sklearn.log_model(
                model,
                "model",
                metadata={
                    "dataset_version": dataset_version,
                    "dataset_hash": dataset_info["hash"]
                }
            )

            return model

    def _compute_dataset_hash(self, df) -> str:
        """Compute deterministic hash of Spark DataFrame"""
        # Sort for determinism
        df_sorted = df.orderBy(*df.columns)

        # Compute hash of schema + sample
        schema_hash = hashlib.md5(str(df.schema).encode()).hexdigest()
        sample_hash = hashlib.md5(
            str(df_sorted.limit(1000).collect()).encode()
        ).hexdigest()

        return f"{schema_hash}-{sample_hash}"

    def _track_with_dvc(self, data_path: str, version: str):
        """Track dataset with DVC"""
        import subprocess

        # Add to DVC
        subprocess.run(["dvc", "add", data_path], check=True)

        # Commit to Git
        subprocess.run(["git", "add", f"{data_path}.dvc"], check=True)
        subprocess.run([
            "git", "commit", "-m",
            f"Dataset version {version}"
        ], check=True)

        # Tag version
        subprocess.run(["git", "tag", f"data-{version}"], check=True)

        # Push DVC
        subprocess.run(["dvc", "push"], check=True)

# Usage
pipeline = VersionedMLPipeline(project_name="fraud-detection")

# Create versioned dataset
dataset_info = pipeline.create_training_dataset(
    source_tables=[
        ("transactions", {"path": "s3://raw/transactions", "version": 42}),
        ("users", {"path": "s3://raw/users", "version": 15})
    ],
    version="v2.1",
    description="Added transaction velocity features"
)

# Train with exact dataset version
model = pipeline.train_with_versioned_data(
    dataset_version="v2.1",
    model_params={"max_depth": 10, "n_estimators": 100}
)
```

---

## ‚úÖ Best Practices

### 1. Version Control Everything
- Data, code, models, configurations
- Use semantic versioning (v1.0.0, v1.1.0, v2.0.0)
- Document what changed between versions

### 2. Immutable Data Principle
- Never modify existing datasets in place
- Create new versions instead
- Enables reproducibility and rollback

### 3. Partition for Performance and Retention
- Partition by date for time-series data
- Enables efficient pruning and deletion
- Supports data retention policies

### 4. Implement Data Contracts
- Define schema, SLAs, ownership
- Version contracts with data
- Alert on contract violations

### 5. Separate Raw and Processed Data
- Raw: immutable source of truth
- Processed: curated, validated, versioned
- Enables reprocessing with new logic

### 6. Use Lakehouse for Production ML
- Leverage ACID transactions for consistency
- Time travel for reproducing exact training data
- Schema evolution for adding features safely

### 7. Integrate Versioning Tools
- DVC for dataset snapshots
- LakeFS for branch-based experimentation
- MLflow for linking datasets to models

### 8. Monitor Data Versions in Production
- Track which data version serves which model
- Alert on version mismatches
- Implement graceful degradation

---

## ‚ö†Ô∏è Common Pitfalls

### 1. Versioning Code But Not Data
**Problem:** Training a model with different data than originally used, leading to irreproducible results.

**Solution:** Always version datasets alongside code. Use DVC or LakeFS to track data versions with Git commits.

**Example:**
```python
# ‚ùå BAD: No data versioning
df = spark.read.parquet("s3://data/training_set.parquet")
model = train(df)
mlflow.log_model(model, "model")

# ‚úÖ GOOD: Explicit data versioning
dataset_version = "v2.1"
df = spark.read.format("delta") \
    .option("versionAsOf", 42) \
    .load("s3://data/training_set/")
mlflow.log_param("dataset_version", dataset_version)
mlflow.log_param("delta_version", 42)
model = train(df)
```

---

### 2. Not Handling Schema Evolution
**Problem:** Breaking downstream consumers when adding/removing/renaming columns.

**Solution:** Use schema evolution features of Delta Lake/Iceberg. Version schemas explicitly.

**Example:**
```python
# ‚ùå BAD: Overwriting schema without compatibility check
df_new.write.format("delta").mode("overwrite").save("s3://features/user_data/")

# ‚úÖ GOOD: Schema evolution with safety checks
df_new.write.format("delta") \
    .option("mergeSchema", "true") \  # Allow new columns
    .mode("append") \
    .save("s3://features/user_data/")

# Validate schema compatibility
old_schema = DeltaTable.forPath(spark, "s3://features/user_data/").toDF().schema
new_schema = df_new.schema
assert is_schema_compatible(old_schema, new_schema), "Breaking schema change detected!"
```

---

### 3. Ignoring Data Retention Costs
**Problem:** Storing all historical versions indefinitely, leading to exponential storage costs.

**Solution:** Implement retention policies. Use vacuum for old versions.

**Example:**
```python
# Define retention policy
TRAINING_DATA_RETENTION_DAYS = 365  # 1 year
MODEL_ARTIFACT_RETENTION_DAYS = 90  # 3 months

# Vacuum old Delta versions
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "s3://training-data/user_features/")

# Remove files older than retention period
delta_table.vacuum(retentionHours=TRAINING_DATA_RETENTION_DAYS * 24)

# Archive important versions to cheaper storage before vacuum
archive_versions = [10, 20, 30]  # Milestone versions
for version in archive_versions:
    df = spark.read.format("delta").option("versionAsOf", version).load("...")
    df.write.parquet(f"s3://archive/user_features/v{version}/")
```

---

### 4. Incomplete Data Lineage
**Problem:** Unable to trace where data came from or how it was transformed, making debugging impossible.

**Solution:** Implement comprehensive lineage tracking with OpenLineage or cloud-native tools.

**Example:**
```python
# ‚úÖ GOOD: Complete lineage tracking
from openlineage.client import OpenLineageClient

client = OpenLineageClient(url="http://lineage-server:5000")

def transform_with_lineage(input_path: str, output_path: str, transformation: str):
    # Emit start event
    client.emit(RunEvent(
        eventType=RunState.START,
        run=Run(runId=f"transform-{uuid.uuid4()}"),
        job=Job(namespace="ml-pipelines", name="feature-transform"),
        inputs=[Dataset(namespace="s3", name=input_path)],
        outputs=[]
    ))

    # Perform transformation
    df = spark.read.parquet(input_path)
    df_transformed = apply_transformation(df, transformation)
    df_transformed.write.parquet(output_path)

    # Emit complete event
    client.emit(RunEvent(
        eventType=RunState.COMPLETE,
        run=Run(runId=f"transform-{uuid.uuid4()}"),
        job=Job(namespace="ml-pipelines", name="feature-transform"),
        inputs=[Dataset(namespace="s3", name=input_path)],
        outputs=[Dataset(namespace="s3", name=output_path, facets={
            "schema": extract_schema(df_transformed),
            "stats": compute_stats(df_transformed)
        })]
    ))
```

---

### 5. Training-Serving Version Skew
**Problem:** Model trained on data version X but served with features from version Y.

**Solution:** Explicitly version feature definitions and enforce consistency.

**Example:**
```python
# Feature definition versioning
class FeatureDefinition:
    def __init__(self, name: str, version: str, computation_logic: callable):
        self.name = name
        self.version = version
        self.computation = computation_logic
        self.hash = hashlib.md5(inspect.getsource(computation_logic).encode()).hexdigest()

# Training
feature_def_v2 = FeatureDefinition(
    name="user_lifetime_value",
    version="v2",
    computation_logic=compute_ltv_v2
)

mlflow.log_param("feature_ltv_version", feature_def_v2.version)
mlflow.log_param("feature_ltv_hash", feature_def_v2.hash)

# Serving - verify feature version matches
def predict(user_id: str):
    model_metadata = mlflow.get_model_metadata(model_uri)
    required_feature_version = model_metadata.metadata["feature_ltv_version"]

    # Ensure we use exact same feature computation
    if feature_def_v2.version != required_feature_version:
        raise ValueError(f"Feature version mismatch: {feature_def_v2.version} != {required_feature_version}")

    features = feature_def_v2.computation(user_id)
    return model.predict(features)
```

---

### 6. Not Testing Data Pipelines
**Problem:** Broken data pipelines silently produce corrupt datasets.

**Solution:** Implement data validation tests and CI/CD for data pipelines.

**Example:**
```python
import great_expectations as ge

def test_training_dataset_quality(dataset_path: str):
    """Test data quality before training"""
    df = ge.read_parquet(dataset_path)

    # Schema tests
    df.expect_table_columns_to_match_ordered_list([
        "user_id", "features", "label", "timestamp"
    ])

    # Data quality tests
    df.expect_column_values_to_not_be_null("user_id")
    df.expect_column_values_to_be_between("label", min_value=0, max_value=1)
    df.expect_column_values_to_be_unique("user_id")

    # Distribution tests
    df.expect_column_mean_to_be_between("features_mean", min_value=0, max_value=10)

    # Validate and raise on failure
    results = df.validate()
    if not results.success:
        raise ValueError(f"Data quality checks failed: {results}")
```

---

## üèãÔ∏è Hands-On Exercises

### Exercise 1: Build a Complete Data Versioning System

**Difficulty:** Advanced
**Time:** 6-8 hours
**Objective:** Implement end-to-end data versioning with DVC, Delta Lake, and MLflow

**Tasks:**
1. **Set up infrastructure:**
   - Initialize DVC in a Git repository
   - Configure S3 as remote storage
   - Set up Delta Lake with Spark
   - Configure MLflow tracking server

2. **Create versioned datasets:**
   - Generate synthetic user behavior data (100K rows)
   - Create v1.0 with basic features
   - Version with DVC and tag in Git
   - Create v1.1 with additional features
   - Version and track changes

3. **Implement Delta Lake time travel:**
   - Write data to Delta table with daily partitions
   - Update 10% of records (simulate corrections)
   - Query historical versions
   - Restore to previous version

4. **Integrate with MLflow:**
   - Train two models (one on each dataset version)
   - Log dataset version, hash, and path with each model
   - Create visualization showing model performance vs dataset version

**Validation:**
```python
# Test 1: Reproducibility
git checkout data-v1.0
dvc checkout
df_v1 = spark.read.parquet("data/features.parquet")
assert df_v1.count() == expected_count_v1

# Test 2: Time travel
df_yesterday = spark.read.format("delta") \
    .option("timestampAsOf", yesterday) \
    .load("delta/user_features")
assert df_yesterday.count() > 0

# Test 3: Lineage
run_data = mlflow.get_run(run_id)
assert "dataset_version" in run_data.data.params
```

---

### Exercise 2: Schema Evolution Challenge

**Difficulty:** Intermediate
**Time:** 3-4 hours
**Objective:** Handle schema changes without breaking existing consumers

**Scenario:**
You have a Delta Lake table `user_features` consumed by 3 downstream ML models. You need to:
- Add 2 new columns (behavioral features)
- Rename 1 column (for clarity)
- Remove 1 deprecated column

**Tasks:**
1. **Implement safe schema evolution:**
   - Add new columns with `mergeSchema=true`
   - Create view with old column name mapping
   - Mark deprecated column with metadata

2. **Version the schema:**
   - Store schema versions in a registry
   - Validate compatibility before writes
   - Create migration scripts

3. **Update consumers:**
   - Modify model training code to handle both schemas
   - Implement feature selection that works with v1 and v2

**Validation Criteria:**
- All 3 existing models continue working
- New models can use new features
- Schema history is tracked
- Rollback is possible

---

### Exercise 3: LakeFS Branch-Based Development

**Difficulty:** Advanced
**Time:** 5-6 hours
**Objective:** Use LakeFS for isolated feature engineering experimentation

**Tasks:**
1. **Set up LakeFS:**
   - Deploy LakeFS locally or on cloud
   - Connect to existing S3 bucket
   - Initialize repository

2. **Branching workflow:**
   - Create `main` branch with production features
   - Create `experiment-velocity-features` branch
   - Compute new features in isolation
   - Test model performance on branch
   - Merge to main if successful

3. **CI/CD integration:**
   - Automated tests run on commits
   - Data quality gates before merge
   - Rollback on test failures

**Success Criteria:**
```python
# Main branch remains unchanged during experiment
assert lakefs.diff("main", "experiment-velocity-features") shows only new files

# Merge only if improvement
if new_model_auc > production_model_auc + 0.02:
    lakefs.merge("experiment-velocity-features", "main")
else:
    lakefs.delete_branch("experiment-velocity-features")
```

---

### Exercise 4: Data Retention Policy Implementation

**Difficulty:** Intermediate
**Time:** 4-5 hours
**Objective:** Implement automated data lifecycle management

**Requirements:**
- Raw data: retain 90 days
- Training datasets: retain 1 year
- Model artifacts: retain important versions indefinitely
- Feature tables: retain 6 months, vacuum older versions

**Tasks:**
1. **Create retention policies:**
```python
class DataRetentionPolicy:
    def __init__(self):
        self.policies = {
            "raw_data": timedelta(days=90),
            "training_data": timedelta(days=365),
            "features": timedelta(days=180)
        }

    def apply_policy(self, data_type: str, table_path: str):
        # Implement vacuum logic for Delta tables
        # Implement S3 lifecycle rules for raw data
        # Archive important versions before deletion
        pass
```

2. **Implement archival:**
   - Archive milestone dataset versions to Glacier
   - Maintain metadata catalog of archived datasets
   - Create restore functionality

3. **Monitoring:**
   - Track storage costs by data type
   - Alert when retention policy violations occur

**Validation:**
- Storage costs reduced by >30%
- All active models still reproducible
- Restore from archive works

---

## üìö Further Reading

### Essential Books
1. **"The Enterprise Big Data Lake" by Alex Gorelik**
   - Comprehensive guide to data lake architectures
   - Best practices for organizing and governing data

2. **"Fundamentals of Data Engineering" by Joe Reis & Matt Housley**
   - Modern data engineering patterns
   - Chapter 6: Data Storage (particularly relevant)

3. **"Delta Lake: The Definitive Guide" by Denny Lee, Tristen Wentling**
   - Deep dive into lakehouse architecture
   - Advanced time travel and ACID transactions

### Key Papers
1. **"Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics"** (Armbrust et al., 2021)
   - [Paper](https://www.cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)
   - Foundational lakehouse architecture concepts

2. **"Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores"** (Armbrust et al., 2020)
   - [Paper](https://databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf)
   - Technical details of Delta Lake implementation

3. **"Apache Iceberg: The Definitive Guide"** (Iceberg community)
   - [Documentation](https://iceberg.apache.org/)
   - Alternative lakehouse table format

### Blogs and Documentation
1. **Netflix Technology Blog**
   - ["Data Mesh: A Data Movement and Processing Platform @ Netflix"](https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873)
   - ["Iceberg at Netflix"](https://netflixtechblog.com/iceberg-tables-powering-data-analytics-at-netflix-870b12c4ead6)

2. **Uber Engineering**
   - ["Hudi: Upserts, Deletes in Cloud Data Lakes"](https://www.uber.com/blog/hoodie/)
   - Data lake best practices at scale

3. **DVC Documentation**
   - [Official DVC Docs](https://dvc.org/doc)
   - [DVC for ML workflow](https://dvc.org/doc/use-cases/versioning-data-and-model-files)

4. **LakeFS Documentation**
   - [Official LakeFS Docs](https://docs.lakefs.io/)
   - [LakeFS for ML](https://lakefs.io/blog/ml-data-versioning/)

5. **MLflow Dataset Tracking**
   - [MLflow Datasets](https://mlflow.org/docs/latest/tracking.html#datasets)

### Video Courses
1. **"Data Lakehouse Fundamentals"** (Databricks Academy)
   - Free certification course
   - Hands-on Delta Lake exercises

2. **"Version Control for Data Science"** (DataCamp)
   - Practical DVC tutorial
   - Git-based data workflows

---

## üìù Key Takeaways

1. **Data Versioning is Non-Negotiable for Reproducible ML**
   - Without versioning, you cannot reproduce experiments or debug model regressions
   - Version data, features, models, and code together as a unit

2. **Lakehouse Architecture is the Modern Standard**
   - Combines flexibility of data lakes with reliability of data warehouses
   - Delta Lake, Iceberg, and Hudi provide ACID transactions and time travel
   - Essential for ML workloads requiring both batch and streaming

3. **Feature Stores Solve Training-Serving Consistency**
   - Dual storage (offline + online) ensures feature consistency
   - Point-in-time correctness prevents data leakage
   - Centralized feature registry enables reuse and governance

4. **Choose Versioning Tools Based on Scale and Workflow**
   - **DVC:** Best for small-to-medium datasets, Git-centric workflows
   - **LakeFS:** Best for large data lakes, branch-based experimentation
   - **MLflow:** Best for lightweight tracking integrated with experiments

5. **Implement Immutability Principle**
   - Never modify data in place; create new versions instead
   - Enables rollback, debugging, and compliance
   - Use append-only patterns with Delta Lake/Iceberg

6. **Data Lineage is Critical for Production ML**
   - Track data provenance from source to model predictions
   - Essential for debugging, compliance (GDPR), and impact analysis
   - Use OpenLineage or cloud-native catalog tools

7. **Optimize Data Formats for ML Workloads**
   - Parquet for analytical/training workloads (columnar, compressed)
   - Delta/Iceberg for ACID requirements and time travel
   - Avro for streaming and schema evolution

8. **Manage Data Lifecycle Proactively**
   - Implement retention policies to control storage costs
   - Archive important versions before vacuum
   - Monitor storage costs by data type and age

9. **Test Data Pipelines Like Code**
   - Validate schema, data quality, and distributions
   - Implement CI/CD for data pipelines
   - Use Great Expectations or similar frameworks

10. **Version Schema Alongside Data**
    - Schema evolution is inevitable in production ML systems
    - Use compatible changes only (add columns, not remove/rename)
    - Maintain backward compatibility for existing consumers

---

## üìù Notes Section

### My Key Insights:
-

### Questions to Explore Further:
-

### How This Applies to My Work:
-

### Tools to Investigate:
-

### Action Items:
-

---

## üîó Related Concepts

- [[01. Data Generation and Collection for ML.md|Previous: Data Collection]]
- [[03. Feature Engineering Workflows.md|Next: Feature Engineering]]
- [[../04. Data Storage for ML/README.md|Chapter 4: Data Storage Deep Dive]]

---

*Created: October 18, 2025*
*Last Updated: October 18, 2025*
*Status: ‚úÖ Completed - Ready for study*
