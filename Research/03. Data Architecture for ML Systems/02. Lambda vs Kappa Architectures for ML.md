# 02. Lambda vs Kappa Architectures for ML

**Chapter:** Data Architecture for ML Systems
**Topic:** Hybrid and unified approaches for batch and stream processing in ML systems

---

## üìã Overview

ML systems must often serve both real-time predictions (requiring fresh data) and batch training (requiring historical data). Lambda and Kappa architectures represent two fundamental approaches to this dual requirement: Lambda combines separate batch and streaming layers, while Kappa unifies everything as streams. Understanding when to use each‚Äîor hybrid approaches‚Äîis critical for building efficient, maintainable ML data architectures.

**Key Industry Reality (2024-2025):** Kappa architecture is gaining dominance for ML systems as stream processing technologies (Flink, Kafka Streams, Spark Structured Streaming) have matured to handle both real-time and historical data reliably. The trend toward "Streamhouse" (streaming + lakehouse) eliminates the complexity of maintaining dual pipelines while preserving Lambda's benefits.

**Critical Challenge:** Choosing between architectural simplicity (Kappa) and separation of concerns (Lambda). Lambda offers reliability through redundancy but doubles maintenance burden. Kappa offers elegance but requires robust stream processing infrastructure.

---

## üéØ Learning Objectives

After completing this subchapter, you will be able to:
- Understand the architecture, components, and trade-offs of Lambda and Kappa patterns
- Design Lambda architecture with batch, speed, and serving layers for ML systems
- Implement Kappa architecture using unified stream processing for ML workloads
- Choose the appropriate architecture based on latency, accuracy, and complexity requirements
- Build hybrid architectures that combine Lambda and Kappa principles
- Migrate from Lambda to Kappa (or vice versa) based on evolving requirements
- Implement Streamhouse architecture for real-time ML on lakehouses
- Evaluate cost, operational complexity, and business value of each approach

---

## üìö Core Concepts

### 1. Lambda Architecture

#### Overview

**Definition:** Data processing architecture with three layers‚Äîbatch, speed, and serving‚Äîproviding both comprehensive batch processing and real-time stream processing.

**Invented by:** Nathan Marz (creator of Apache Storm), 2011

**Core Principle:** "If batch processing provides complete and accurate views but with high latency, and stream processing provides fast views but with potential inaccuracies, why not have both?"

**Architecture:**
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Data Source        ‚îÇ
                    ‚îÇ  (Events, Logs, DB)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚Üì                                 ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  BATCH LAYER    ‚îÇ             ‚îÇ   SPEED LAYER    ‚îÇ
    ‚îÇ                 ‚îÇ             ‚îÇ  (Stream Proc.)  ‚îÇ
    ‚îÇ - Hadoop/Spark  ‚îÇ             ‚îÇ  - Flink/Storm   ‚îÇ
    ‚îÇ - Complete data ‚îÇ             ‚îÇ  - Recent data   ‚îÇ
    ‚îÇ - High latency  ‚îÇ             ‚îÇ  - Low latency   ‚îÇ
    ‚îÇ - High accuracy ‚îÇ             ‚îÇ  - Approx result ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                                ‚îÇ
             ‚îÇ  Batch Views                   ‚îÇ  Real-time Views
             ‚îÇ  (Precomputed)                 ‚îÇ  (Incremental)
             ‚Üì                                ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ            SERVING LAYER                       ‚îÇ
    ‚îÇ  (Merge batch + real-time views)               ‚îÇ
    ‚îÇ  - Druid / Cassandra / HBase                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
                   Query Results
                  (Complete + Fresh)
```

---

#### The Three Layers

**1. Batch Layer (Master Dataset)**

**Purpose:** Store complete, immutable history and recompute batch views periodically.

**Characteristics:**
- Processes entire dataset (hours/days of data)
- High latency (hourly, daily recomputation)
- High accuracy (can iterate multiple times)
- Fault-tolerant (can recompute from source)

**Technologies:**
- Apache Hadoop MapReduce
- Apache Spark (batch mode)
- AWS Glue, Azure Data Factory

**ML Example:**
```python
# Batch layer: Compute user features from all historical data

from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.appName("BatchLayer").getOrCreate()

# Read entire history (months/years of data)
transactions = spark.read.parquet("s3://data/transactions/")
user_profiles = spark.read.parquet("s3://data/user_profiles/")

# Compute batch features (comprehensive, accurate)
user_features_batch = transactions.groupBy("user_id").agg(
    # Lifetime features (all history)
    F.count("*").alias("lifetime_transactions"),
    F.sum("amount").alias("lifetime_value"),
    F.avg("amount").alias("avg_transaction_size"),
    F.min("timestamp").alias("first_transaction_date"),
    F.max("timestamp").alias("last_transaction_date"),
    F.countDistinct("merchant_id").alias("unique_merchants"),

    # Statistical features (entire distribution)
    F.stddev("amount").alias("transaction_amount_stddev"),
    F.expr("percentile_approx(amount, 0.5)").alias("median_transaction"),
    F.expr("percentile_approx(amount, 0.95)").alias("p95_transaction")
).join(user_profiles, "user_id", "left")

# Write batch views (replace daily)
user_features_batch.write.mode("overwrite") \
    .partitionBy("date") \
    .parquet("s3://features/batch_views/user_features/")

print(f"‚úÖ Batch layer complete: {user_features_batch.count()} users")
```

---

**2. Speed Layer (Real-time Processing)**

**Purpose:** Compensate for batch layer's high latency by processing recent data in real-time.

**Characteristics:**
- Processes only recent data (last few hours)
- Low latency (seconds to minutes)
- Approximate results (limited compute per record)
- Updates incremental views

**Technologies:**
- Apache Flink
- Apache Storm
- Spark Structured Streaming
- Kafka Streams

**ML Example:**
```python
# Speed layer: Compute real-time features from streaming data

from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.appName("SpeedLayer").getOrCreate()

# Read from Kafka stream
transactions_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "transactions") \
    .load()

# Parse JSON
parsed_stream = transactions_stream.select(
    F.from_json(F.col("value").cast("string"), transaction_schema).alias("data")
).select("data.*")

# Compute real-time features (last 1 hour window)
real_time_features = parsed_stream \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        F.window("timestamp", "1 hour", "5 minutes"),  # Sliding window
        "user_id"
    ).agg(
        F.count("*").alias("transactions_last_hour"),
        F.sum("amount").alias("amount_last_hour"),
        F.countDistinct("merchant_id").alias("unique_merchants_last_hour")
    )

# Write to serving layer (incremental updates)
query = real_time_features.writeStream \
    .outputMode("update") \
    .format("delta") \  # Or Redis/Cassandra for low latency
    .option("checkpointLocation", "s3://checkpoints/speed_layer/") \
    .start("s3://features/speed_views/user_features/")

query.awaitTermination()
```

---

**3. Serving Layer (Query Interface)**

**Purpose:** Merge batch and speed views to answer queries with both completeness (batch) and freshness (speed).

**Characteristics:**
- Low-latency reads
- Merges batch + real-time views
- Handles query load (user-facing)

**Technologies:**
- Apache Druid
- Cassandra
- HBase
- ElasticSearch
- Redis (caching)

**ML Example:**
```python
# Serving layer: Merge batch and speed views for predictions

class LambdaServingLayer:
    """
    Serve features by merging batch and real-time views
    """

    def __init__(self):
        # Batch view (updated daily)
        self.batch_store = connect_to_s3("s3://features/batch_views/")

        # Speed view (updated every 5 min)
        self.speed_store = connect_to_redis("redis://localhost:6379")

    def get_features(self, user_id: str) -> dict:
        """
        Get features by merging batch + real-time views

        Query: features(user_id) = batch_view(user_id) ‚à™ speed_view(user_id)
        """
        # Get batch features (comprehensive, slightly stale)
        batch_features = self.batch_store.query(
            f"SELECT * FROM user_features WHERE user_id = '{user_id}'"
        )

        # Get real-time features (recent activity)
        speed_features = self.speed_store.hgetall(f"user:{user_id}:realtime")

        # Merge (speed layer overrides batch for recency)
        merged_features = {
            # Batch features (lifetime stats)
            'lifetime_transactions': batch_features['lifetime_transactions'],
            'lifetime_value': batch_features['lifetime_value'],
            'first_transaction_date': batch_features['first_transaction_date'],

            # Speed features (recent activity - override batch)
            'transactions_last_hour': speed_features.get('transactions_last_hour', 0),
            'amount_last_hour': speed_features.get('amount_last_hour', 0.0),

            # Derived features
            'velocity_ratio': speed_features.get('amount_last_hour', 0) / batch_features['avg_transaction_size']
        }

        return merged_features


# Usage in prediction service
serving = LambdaServingLayer()

def predict(user_id: str):
    """Generate prediction with merged features"""
    features = serving.get_features(user_id)
    prediction = model.predict([features])
    return prediction
```

---

#### Lambda Architecture Pros & Cons

**Pros:**
- ‚úÖ **Robustness:** Batch layer can recompute if speed layer fails
- ‚úÖ **Accuracy:** Batch processing ensures eventual consistency
- ‚úÖ **Human fault-tolerance:** Mistakes in speed layer corrected by batch recomputation
- ‚úÖ **Separation of concerns:** Optimize batch and speed independently
- ‚úÖ **Technology choice flexibility:** Different tech stacks for batch vs stream

**Cons:**
- ‚ùå **Complexity:** Maintain two processing pipelines (2x code, 2x infrastructure)
- ‚ùå **Code duplication:** Same logic implemented twice (batch & streaming)
- ‚ùå **Consistency challenges:** Batch and speed views may diverge
- ‚ùå **Operational overhead:** Two systems to monitor, debug, deploy
- ‚ùå **Higher cost:** More infrastructure components

---

#### When to Use Lambda Architecture (ML Context)

**Ideal For:**

1. **Model Training Requires Complete History**
   - Training on years of data (batch layer)
   - Serving with fresh features (speed layer)
   - Example: Credit risk models, churn prediction

2. **Different Algorithms for Batch vs Stream**
   - Complex batch processing (joins, aggregations)
   - Simple real-time calculations (counts, sums)
   - Example: Recommendation systems (collaborative filtering batch, popularity counts real-time)

3. **Regulatory Compliance Needs**
   - Immutable audit trail (batch layer)
   - Real-time monitoring (speed layer)
   - Example: Financial fraud detection

4. **High Accuracy Requirements**
   - Can tolerate complexity for correctness
   - Batch recomputation as safety net
   - Example: Medical diagnosis, autonomous vehicles

**Not Ideal For:**
- Simple real-time analytics
- Small datasets (<100GB)
- Limited engineering resources

---

### 2. Kappa Architecture

#### Overview

**Definition:** Simplified data processing architecture that treats all data‚Äîboth historical and real-time‚Äîas streams, using a single stream processing engine.

**Invented by:** Jay Kreps (LinkedIn co-founder, creator of Kafka), 2014

**Core Principle:** "Everything is a stream. If your stream processor is powerful enough, why maintain two systems?"

**Architecture:**
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Data Source        ‚îÇ
                    ‚îÇ  (Events, Logs, DB)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Message Broker     ‚îÇ
                    ‚îÇ   (Kafka / Pulsar)   ‚îÇ
                    ‚îÇ   - Replay-able      ‚îÇ
                    ‚îÇ   - Long retention   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ STREAM PROCESSING    ‚îÇ
                    ‚îÇ  (Flink / Spark)     ‚îÇ
                    ‚îÇ                      ‚îÇ
                    ‚îÇ - Real-time data ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚Üí Real-time Views
                    ‚îÇ - Historical data ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚Üí Batch Views
                    ‚îÇ   (replay from Kafka)‚îÇ    (recompute by replaying)
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   SERVING LAYER      ‚îÇ
                    ‚îÇ  (Same as Lambda)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

#### Key Concept: Stream Reprocessing

**How Kappa Handles "Batch" Processing:**

Instead of a separate batch layer, Kappa **replays historical data through the stream processor**.

**Example:**
```python
# Kappa architecture: Single stream processor handles both real-time and historical

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("KappaArchitecture").getOrCreate()

def process_stream(data_stream, output_path: str):
    """
    Same processing logic for both:
    - Real-time data (Kafka stream)
    - Historical data (Kafka replay from offset 0)
    """

    # Parse events
    events = data_stream.select(
        F.from_json(F.col("value").cast("string"), event_schema).alias("data")
    ).select("data.*")

    # Compute features (same logic for real-time and historical)
    features = events \
        .withWatermark("timestamp", "10 minutes") \
        .groupBy(
            F.window("timestamp", "1 hour"),
            "user_id"
        ).agg(
            F.count("*").alias("event_count"),
            F.sum("amount").alias("total_amount"),
            F.avg("amount").alias("avg_amount")
        )

    # Write to serving layer
    query = features.writeStream \
        .outputMode("update") \
        .format("delta") \
        .option("checkpointLocation", f"{output_path}/checkpoint") \
        .start(output_path)

    return query


# Real-time processing
realtime_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .option("startingOffsets", "latest") \  # Process new data
    .load()

query_realtime = process_stream(realtime_stream, "s3://features/realtime/")


# "Batch" processing = Replay historical data through same stream processor
historical_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "events") \
    .option("startingOffsets", "earliest") \  # Replay from beginning
    .load()

query_historical = process_stream(historical_stream, "s3://features/batch/")

# Same code, different starting offset!
```

**Key Insight:** Kafka's log retention (days, weeks, months) allows "batch" reprocessing by replaying from `offset=0`.

---

#### Kappa Architecture Pros & Cons

**Pros:**
- ‚úÖ **Simplicity:** Single codebase, single processing paradigm
- ‚úÖ **No code duplication:** Write logic once, use for real-time and historical
- ‚úÖ **Easier maintenance:** One system to monitor, debug, optimize
- ‚úÖ **Lower cost:** Single infrastructure stack
- ‚úÖ **Consistency:** Real-time and batch use same computations
- ‚úÖ **Flexibility:** Reprocess historical data anytime by replaying streams

**Cons:**
- ‚ùå **Stream processor must be powerful:** Needs to handle batch-scale workloads
- ‚ùå **Replay complexity:** Reprocessing terabytes can be slow
- ‚ùå **Storage requirements:** Kafka must retain long history (high disk cost)
- ‚ùå **Not suitable for complex batch algorithms:** Stream semantics limit expressiveness
- ‚ùå **Ordering guarantees:** Complex for out-of-order data

---

#### When to Use Kappa Architecture (ML Context)

**Ideal For:**

1. **Real-time ML is Primary Requirement**
   - Low-latency predictions (<100ms)
   - Streaming features dominate
   - Example: Fraud detection, real-time bidding

2. **Simple, Consistent Processing Logic**
   - Same algorithm for historical and real-time data
   - Aggregations, filters, joins (stream-friendly)
   - Example: Click-through rate prediction, A/B testing analytics

3. **Event-Driven ML Systems**
   - Model retraining triggered by events
   - Continuous learning from streams
   - Example: Recommendation systems, personalization

4. **Resource Constraints**
   - Limited engineering bandwidth
   - Avoid maintaining dual systems
   - Example: Startups, small ML teams

**Not Ideal For:**
- Complex batch algorithms (graph processing, iterative ML)
- Very long historical windows (years of data)
- When batch and stream require different logic

---

### 3. Hybrid & Modern Approaches

#### Streamhouse Architecture (2024-2025 Trend)

**Definition:** Kappa architecture + Lakehouse (Delta Lake/Iceberg) = Best of both worlds.

**Architecture:**
```
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ  Event Streams  ‚îÇ
                ‚îÇ     (Kafka)     ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  Stream Processor    ‚îÇ
              ‚îÇ  (Flink / Spark)     ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚Üì
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ     LAKEHOUSE        ‚îÇ
              ‚îÇ   (Delta Lake)       ‚îÇ
              ‚îÇ                      ‚îÇ
              ‚îÇ - ACID transactions  ‚îÇ
              ‚îÇ - Time travel        ‚îÇ
              ‚îÇ - Stream + Batch     ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì                ‚Üì                ‚Üì
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ ML      ‚îÇ    ‚îÇ BI       ‚îÇ    ‚îÇ Real-time‚îÇ
   ‚îÇ Training‚îÇ    ‚îÇ Analytics‚îÇ    ‚îÇ Serving  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Innovation:** Delta Lake/Iceberg provide:
- Stream writes (low latency)
- Batch reads (time travel, full scans)
- ACID guarantees
- Unified storage

**Implementation:**
```python
# Streamhouse: Streaming into Delta Lake for both real-time and batch ML

from delta.tables import DeltaTable

# Write stream to Delta Lake
stream = spark.readStream.format("kafka") \
    .option("subscribe", "events") \
    .load()

# Real-time streaming writes (append mode)
query = stream.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "s3://checkpoints/events/") \
    .start("s3://lakehouse/events/")

# Batch reads for training (time travel)
training_data = spark.read.format("delta") \
    .option("versionAsOf", 42) \  # Specific version
    .load("s3://lakehouse/events/")

# Real-time reads for serving (latest data)
serving_data = spark.read.format("delta") \
    .load("s3://lakehouse/events/") \
    .filter(F.col("timestamp") > F.current_timestamp() - F.expr("INTERVAL 1 HOUR"))

# Reprocess historical data (time travel + streaming)
historical_reprocess = spark.readStream.format("delta") \
    .option("startingVersion", 0) \  # Replay from version 0
    .load("s3://lakehouse/events/")
```

**Benefits:**
- ‚úÖ Kappa simplicity + Lambda reliability
- ‚úÖ Single storage layer (reduced cost)
- ‚úÖ ACID transactions (data quality)
- ‚úÖ Time travel (reproducibility)

---

## üí° Practical Examples

### Example 1: Fraud Detection with Lambda Architecture

**Scenario:** Credit card fraud detection requiring both lifetime fraud history (batch) and real-time transaction velocity (speed).

```python
# Batch Layer: Compute user fraud risk from all history
def batch_layer_fraud_features(spark):
    """Daily batch job: Compute comprehensive fraud indicators"""

    # Load all transaction history (years of data)
    transactions = spark.read.parquet("s3://data/transactions/")

    user_fraud_features = transactions.groupBy("user_id").agg(
        # Lifetime stats
        F.count("*").alias("lifetime_transactions"),
        F.sum(F.when(F.col("fraud_flag") == 1, 1).otherwise(0)).alias("historical_fraud_count"),

        # Behavioral patterns
        F.avg("amount").alias("avg_transaction_size"),
        F.stddev("amount").alias("transaction_variance"),
        F.countDistinct("merchant_category").alias("merchant_diversity"),

        # Velocity over longer windows
        F.expr("count(*) / datediff(max(date), min(date))").alias("avg_daily_transaction_rate")
    )

    # Write batch view
    user_fraud_features.write.mode("overwrite") \
        .parquet("s3://features/batch/fraud_indicators/")


# Speed Layer: Compute real-time velocity features
def speed_layer_fraud_features(spark):
    """Streaming job: Compute real-time fraud indicators"""

    stream = spark.readStream.format("kafka") \
        .option("subscribe", "transactions") \
        .load()

    # Real-time features (last 5 minutes)
    real_time_velocity = stream \
        .withWatermark("timestamp", "10 minutes") \
        .groupBy(
            F.window("timestamp", "5 minutes"),
            "user_id"
        ).agg(
            F.count("*").alias("transactions_5min"),
            F.sum("amount").alias("amount_5min"),
            F.countDistinct("merchant_id").alias("unique_merchants_5min"),
            F.countDistinct("location").alias("unique_locations_5min")
        )

    # Write to Redis for low-latency serving
    real_time_velocity.writeStream \
        .foreachBatch(lambda df, epoch: write_to_redis(df)) \
        .start()


# Serving Layer: Merge batch + real-time for prediction
class FraudDetectionServing:
    def get_fraud_score(self, user_id: str, transaction: dict) -> float:
        """Score transaction using batch + real-time features"""

        # Batch features (comprehensive history)
        batch = self.batch_store.query(f"user_id = '{user_id}'")

        # Real-time features (recent velocity)
        realtime = self.redis.hgetall(f"user:{user_id}:velocity")

        # Combined features
        features = {
            **batch,
            **realtime,
            'velocity_vs_average': realtime['transactions_5min'] / (batch['avg_daily_transaction_rate'] / 288)
        }

        # Predict
        fraud_score = self.model.predict_proba([features])[0, 1]

        return fraud_score
```

---

### Example 2: Recommendation System with Kappa Architecture

**Scenario:** Real-time product recommendations based on user behavior streams.

```python
# Kappa Architecture: Single stream processor for all recommendation logic

class KappaRecommendationSystem:
    """
    Process both real-time and historical user behavior with single stream processor
    """

    def compute_user_preferences(self, spark, mode: str = "realtime"):
        """
        Same logic for real-time and batch reprocessing

        Args:
            mode: "realtime" (process new events) or "reprocess" (replay from start)
        """

        # Configure stream source
        starting_offsets = "latest" if mode == "realtime" else "earliest"

        events_stream = spark.readStream \
            .format("kafka") \
            .option("subscribe", "user_events") \
            .option("startingOffsets", starting_offsets) \
            .load()

        # Parse events
        events = events_stream.select(
            F.from_json(F.col("value").cast("string"), schema).alias("data")
        ).select("data.*")

        # Compute user preferences (same for real-time and historical)
        user_preferences = events \
            .filter(F.col("event_type").isin(["view", "click", "purchase"])) \
            .groupBy("user_id", "product_category") \
            .agg(
                F.sum(F.when(F.col("event_type") == "view", 1).otherwise(0)).alias("views"),
                F.sum(F.when(F.col("event_type") == "click", 1).otherwise(0)).alias("clicks"),
                F.sum(F.when(F.col("event_type") == "purchase", 1).otherwise(0)).alias("purchases")
            ).withColumn(
                "preference_score",
                F.col("purchases") * 10 + F.col("clicks") * 2 + F.col("views")
            )

        # Write to Delta Lake (unified storage)
        query = user_preferences.writeStream \
            .format("delta") \
            .outputMode("update") \
            .option("checkpointLocation", f"s3://checkpoints/{mode}/") \
            .start(f"s3://lakehouse/user_preferences/")

        return query


# Real-time processing
system = KappaRecommendationSystem()
realtime_query = system.compute_user_preferences(spark, mode="realtime")

# Reprocess historical data (triggered when algorithm changes)
# Same code, just replay from beginning!
reprocess_query = system.compute_user_preferences(spark, mode="reprocess")
```

---

## üîß Code Examples

### Migration from Lambda to Kappa

```python
# Before (Lambda): Separate batch and stream code

# Batch job (Spark)
def batch_job():
    df = spark.read.parquet("s3://data/")
    features = df.groupBy("user_id").agg(F.count("*").alias("count"))
    features.write.parquet("s3://features/batch/")

# Stream job (Flink - different framework!)
def stream_job():
    stream = env.add_source(FlinkKafkaConsumer("topic", ...))
    features = stream.key_by("user_id").reduce(lambda a,b: a+b)  # Different syntax!
    features.add_sink(...)


# After (Kappa): Unified stream processing

def unified_stream_processing(starting_offset: str):
    """
    Same code for batch (replay) and real-time

    Args:
        starting_offset: "earliest" for batch, "latest" for real-time
    """
    stream = spark.readStream.format("kafka") \
        .option("startingOffsets", starting_offset) \
        .load()

    features = stream.groupBy("user_id").agg(F.count("*").alias("count"))

    features.writeStream.format("delta") \
        .option("checkpointLocation", f"s3://checkpoints/{starting_offset}/") \
        .start("s3://features/unified/")


# Real-time processing
unified_stream_processing("latest")

# Batch reprocessing (when needed)
unified_stream_processing("earliest")
```

---

## ‚úÖ Best Practices

### 1. Choose Based on Requirements, Not Trends
- Lambda if: Complex batch algorithms, compliance needs
- Kappa if: Simple logic, resource constraints
- Hybrid if: Need both simplicity and safety

### 2. Design for Reprocessing
- Version feature computation logic
- Make stream processing deterministic
- Store raw events (enable replay)

### 3. Use Lakehouse for Unified Storage
- Delta Lake / Iceberg for stream + batch
- Time travel for reproducibility
- ACID for correctness

### 4. Monitor Both Layers (Lambda)
- Compare batch vs speed results
- Alert on divergence
- Automate reconciliation

### 5. Test Stream Reprocessing (Kappa)
- Validate replay produces same results
- Test at scale (full historical replay)
- Measure replay time

### 6. Abstract Architecture Behind Interfaces
- Don't expose Lambda/Kappa to consumers
- Features API abstracts implementation
- Enables architecture evolution

---

## ‚ö†Ô∏è Common Pitfalls

### 1. Lambda: Batch and Speed Logic Diverge

**Problem:** Batch and stream implementations produce different results.

**Solution:** Share code between layers, automated testing for consistency.

---

### 2. Kappa: Stream Processor Can't Handle Batch Scale

**Problem:** Replay crashes or takes days.

**Solution:** Use robust stream processor (Flink, Spark), test at scale.

---

### 3. Not Planning for Reprocessing

**Problem:** Bug in features, no way to recompute.

**Solution:** Keep raw events, design for replay.

---

### 4. Over-complicating with Lambda

**Problem:** Using Lambda when simple stream would work.

**Solution:** Start with Kappa, move to Lambda only when necessary.

---

## üèãÔ∏è Hands-On Exercises

### Exercise 1: Implement Lambda Architecture

**Difficulty:** Advanced
**Time:** 10-12 hours

**Objective:** Build Lambda architecture for user features.

**Tasks:**
1. Batch layer: Spark job (all-time features)
2. Speed layer: Flink (last hour features)
3. Serving layer: Merge in API
4. Compare batch vs speed results

---

### Exercise 2: Build Kappa with Stream Replay

**Difficulty:** Advanced
**Time:** 8-10 hours

**Objective:** Implement Kappa with historical replay.

**Tasks:**
1. Set up Kafka with 30-day retention
2. Stream processor (Spark Structured Streaming)
3. Real-time mode (latest offset)
4. Batch mode (replay from offset 0)
5. Validate results match

---

### Exercise 3: Migrate Lambda to Kappa

**Difficulty:** Advanced
**Time:** 12-15 hours

**Objective:** Migrate existing Lambda system to Kappa.

**Tasks:**
1. Identify differences in batch/stream logic
2. Unify into single stream processor
3. Test replay matches batch layer
4. Deploy and validate
5. Decommission batch layer

---

### Exercise 4: Streamhouse Architecture

**Difficulty:** Advanced
**Time:** 10-12 hours

**Objective:** Implement Streamhouse with Delta Lake.

**Tasks:**
1. Stream writes to Delta Lake
2. Real-time reads (latest data)
3. Batch reads (time travel)
4. Historical replay (streaming from version 0)

---

## üìö Further Reading

### Essential Books
1. **"Designing Data-Intensive Applications" by Martin Kleppmann** - Chapter 11: Stream Processing
2. **"Streaming Systems" by Tyler Akidau et al.** - Comprehensive stream processing guide

### Key Papers
1. **"The Dataflow Model"** (Google, 2015) - Unified batch + streaming
2. **"Delta Lake: High-Performance ACID Table Storage"** - Lakehouse approach

### Blogs
1. **Jay Kreps' Blog** - "Questioning the Lambda Architecture" (original Kappa proposal)
2. **Uber Engineering** - "Kappa Architecture at Uber"
3. **Confluent Blog** - Kafka-based architectures

---

## üìù Key Takeaways

1. **Lambda = Robust, Complex; Kappa = Simple, Requires Powerful Streams**
2. **Lambda: Use when batch and stream need different algorithms**
3. **Kappa: Use when same logic applies to all data**
4. **Streamhouse: Modern approach combining Kappa + Lakehouse**
5. **Design for reprocessing: Keep raw events, version logic**
6. **Test at scale: Ensure stream replay works for full history**
7. **Monitor divergence in Lambda: Batch and speed should converge**
8. **Start simple (Kappa), evolve to Lambda if needed**
9. **Abstract architecture: Don't expose Lambda/Kappa to users**
10. **Future is streaming: Kappa gaining adoption as tools mature**

---

## üìù Notes Section

### My Key Insights:
-

### Questions to Explore Further:
-

### How This Applies to My Work:
-

### Tools to Investigate:
-

### Action Items:
-

---

## üîó Related Concepts

- [[01. Architectural Patterns for ML Systems.md|Previous: Architectural Patterns]]
- [[03. Batch vs Streaming Considerations.md|Next: Batch vs Streaming]]
- [[../02. ML Data Pipeline Lifecycle/README.md|Chapter 2: ML Pipeline Lifecycle]]

---

*Created: October 18, 2025*
*Last Updated: October 18, 2025*
*Status: ‚úÖ Completed - Ready for study*
