# 03. Batch vs Streaming Considerations

**Date:** 2025-10-19
**Status:** #research
**Tags:** #data-engineering #ml-systems #batch-processing #streaming #architecture #performance #latency

---

## üìã Overview

Choosing between batch and streaming processing architectures is one of the most critical architectural decisions in ML systems. This choice fundamentally impacts system complexity, latency, cost, and operational overhead.

**Batch processing** excels at processing large volumes of data efficiently with high throughput, making it ideal for model training, historical analysis, and use cases where latency is measured in hours or days. **Streaming processing** enables real-time or near-real-time decision-making with latencies in milliseconds to seconds, essential for fraud detection, recommendation systems, and dynamic pricing.

Modern ML systems increasingly adopt **hybrid architectures** that combine batch and streaming processing to balance cost-efficiency with low-latency requirements. Understanding the trade-offs between these approaches enables data engineers to design systems that meet business requirements while maintaining operational sustainability.

**Key Dimensions**:
- **Latency**: Batch (hours-days) vs Streaming (milliseconds-seconds)
- **Throughput**: Batch optimizes for volume, streaming for velocity
- **Complexity**: Batch is simpler, streaming requires sophisticated orchestration
- **Cost**: Batch leverages economies of scale, streaming incurs continuous resource costs
- **State Management**: Streaming requires complex windowing and state handling

---

## üéØ Learning Objectives

By the end of this subchapter, you will:

1. **Understand the fundamental trade-offs** between batch and streaming processing for ML systems
2. **Evaluate latency requirements** and map them to appropriate processing paradigms
3. **Design hybrid architectures** that combine batch and streaming where appropriate
4. **Calculate cost models** for batch vs streaming infrastructure
5. **Implement streaming windowing operations** for real-time feature computation
6. **Handle late-arriving data** in streaming pipelines with watermarks and triggers
7. **Optimize batch jobs** for throughput and resource utilization
8. **Select appropriate technologies** (Spark, Flink, Kafka, Beam) based on use case requirements
9. **Manage state** in stateful streaming applications for ML
10. **Test and validate** streaming pipelines for correctness and performance

---

## üìö Core Concepts

### 1. Batch Processing Fundamentals

**Definition**: Processing accumulated data in discrete, scheduled jobs (hourly, daily, weekly).

**Characteristics**:
- **High Throughput**: Optimized for processing large volumes efficiently
- **Resource Efficiency**: Leverages cluster resources fully during execution windows
- **Simplicity**: Easier to reason about, debug, and maintain
- **Deterministic**: Rerunning the same input produces the same output
- **Complete Data View**: Access to full dataset enables complex aggregations

**When to Use Batch**:
- Model training (retraining weekly/daily with historical data)
- Batch feature computation (computing features overnight for next-day serving)
- Historical analysis and reporting
- Data quality checks and validation
- ETL pipelines with SLA measured in hours

**Example Technologies**: Apache Spark (batch mode), Apache Beam (batch runner), dbt, Airflow

---

### 2. Streaming Processing Fundamentals

**Definition**: Continuously processing data as it arrives, often with subsecond latency.

**Characteristics**:
- **Low Latency**: Results available within milliseconds to seconds
- **Continuous Computation**: Always-on processing of infinite data streams
- **Event-Driven**: Reacts to events as they occur
- **State Management**: Maintains state across events for aggregations
- **Complexity**: Requires handling out-of-order data, late arrivals, exactly-once semantics

**When to Use Streaming**:
- Real-time fraud detection (transaction scoring in <100ms)
- Online feature computation (real-time user embeddings)
- Anomaly detection (detecting unusual patterns immediately)
- Real-time recommendations (personalized content based on current session)
- Monitoring and alerting (drift detection, SLA violations)

**Example Technologies**: Apache Flink, Apache Kafka Streams, Apache Beam (streaming runner), AWS Kinesis Data Analytics

---

### 3. Latency Requirements Spectrum

Understanding latency requirements helps select the right architecture:

| **Use Case** | **Latency Requirement** | **Architecture** | **Example** |
|-------------|------------------------|------------------|-------------|
| Model Retraining | Days to weeks | Batch | Weekly retrain on historical data |
| Daily Feature Updates | Hours | Batch | Nightly feature computation |
| Hourly Aggregations | Minutes to hours | Micro-batch | Hourly user activity features |
| Real-time Features | Seconds | Streaming | Session-based features |
| Online Inference | <100ms | Streaming + Precomputed | Fraud detection scoring |
| Ultra-low Latency | <10ms | In-memory + Cached | High-frequency trading ML |

**Latency vs Complexity Trade-off**: Lower latency requirements exponentially increase system complexity and operational costs.

---

### 4. Throughput vs Latency Trade-offs

**Batch Optimization** (maximize throughput):
- Process large chunks of data together
- Leverage parallelism across nodes
- Use columnar formats (Parquet, ORC) for I/O efficiency
- Trade latency for cost efficiency

**Streaming Optimization** (minimize latency):
- Process records individually or in micro-batches
- Maintain continuous pipeline execution
- Use in-memory state stores (RocksDB, Redis)
- Trade cost for responsiveness

**Example**: Processing 1 billion records
- **Batch**: 1 billion records in 1 hour = 277K records/sec throughput, 1-hour latency
- **Streaming**: Continuous processing at 10K records/sec = real-time latency, higher cost

---

### 5. Windowing in Streaming Systems

**Windowing** divides infinite streams into finite chunks for aggregation.

**Window Types**:

1. **Tumbling Windows**: Fixed-size, non-overlapping
   - Use case: Hourly aggregations (count transactions per hour)
   - Example: [0:00-1:00), [1:00-2:00), [2:00-3:00)

2. **Sliding Windows**: Fixed-size, overlapping
   - Use case: Moving averages (average over last 5 minutes, updated every minute)
   - Example: Window size = 5 min, slide = 1 min

3. **Session Windows**: Dynamic size based on inactivity gap
   - Use case: User session analysis (group events until 30 min of inactivity)
   - Example: User clicks until 30-min gap triggers session close

4. **Global Windows**: All data in one window (requires custom triggers)
   - Use case: Stateful processing where window boundaries are application-defined

**Watermarks**: Mechanism to track event-time progress in streaming systems
- **Purpose**: Determine when to close a window and emit results
- **Trade-off**: Late watermarks = more complete results but higher latency

---

### 6. State Management in Streaming

**Stateful Operations** require maintaining state across events:
- Aggregations (count, sum, average)
- Joins between streams
- Machine learning model serving with context
- Sessionization

**State Backend Options**:
1. **In-Memory** (Flink MemoryStateBackend): Fast but limited by memory
2. **RocksDB** (Flink RocksDBStateBackend): Disk-backed, scales to large state
3. **External State Store** (Redis, DynamoDB): Shared state across instances

**State Size Challenges**:
- Large state (>100GB) impacts checkpoint performance
- State recovery after failure increases downtime
- State growth over time requires compaction strategies

---

### 7. Cost Models

**Batch Processing Costs**:
- **Infrastructure**: Pay for cluster time during job execution (elastic scaling)
- **Storage**: Data lakes (S3, GCS) are cheap (~$0.023/GB/month)
- **Compute**: Spot/preemptible instances reduce costs by 70-90%
- **Example**: Daily 4-hour Spark job on 20 nodes = 80 node-hours/day

**Streaming Processing Costs**:
- **Infrastructure**: 24/7 running clusters (less elastic)
- **Kafka/Kinesis**: ~$0.015/hour per shard + data transfer costs
- **Flink Cluster**: Continuous TaskManager + JobManager resource usage
- **State Storage**: RocksDB state stored on disk, S3 for checkpoints
- **Example**: 3-node Flink cluster running 24/7 = 2,160 node-hours/month

**Cost Comparison** (1 TB/day processing):
- Batch (4 hours/day on 10 nodes): ~$300-500/month
- Streaming (3 nodes 24/7): ~$1,500-2,500/month

**Cost Optimization Strategies**:
- Use batch for non-latency-sensitive workloads
- Implement tiered architectures (hot/warm/cold data)
- Leverage auto-scaling for variable load patterns
- Consider managed services (Databricks, AWS Kinesis) for operational efficiency

---

### 8. Complexity Considerations

**Batch Complexity**:
- **Simpler Mental Model**: Finite input ‚Üí process ‚Üí finite output
- **Easier Debugging**: Rerun job with same input to reproduce issues
- **Testing**: Unit test transformations with sample datasets
- **Orchestration**: Airflow DAGs are straightforward

**Streaming Complexity**:
- **Infinite Data Model**: Requires thinking about unbounded streams
- **Out-of-Order Data**: Events arrive late, requiring watermarking
- **Exactly-Once Semantics**: Complex coordination (Kafka transactions, Flink checkpoints)
- **State Management**: Maintaining and recovering distributed state
- **Testing**: Harder to test time-based logic and windowing

**Operational Complexity**:
- Batch: Retry failed jobs, debug with logs
- Streaming: Monitor lag, handle backpressure, manage state size, recover from failures

---

### 9. Hybrid Architectures

**Combining Batch and Streaming**:

**Pattern 1: Speed Layer + Batch Layer (Lambda Architecture)**
- Batch: Compute comprehensive features overnight
- Streaming: Compute incremental updates in real-time
- Serving: Merge both layers at query time

**Pattern 2: Batch Training + Streaming Inference**
- Batch: Train models on historical data (weekly)
- Streaming: Serve predictions with real-time features

**Pattern 3: Streaming Aggregation + Batch Enrichment**
- Streaming: Real-time aggregations (click counts)
- Batch: Enrich with slow-changing dimensions (user demographics)

**Pattern 4: Near-Real-Time with Micro-Batching**
- Process small batches every 5-15 minutes
- Balances latency and throughput
- Example: Spark Structured Streaming with trigger intervals

---

### 10. Technology Selection Matrix

| **Requirement** | **Batch Technologies** | **Streaming Technologies** |
|----------------|------------------------|---------------------------|
| Large-scale batch training | Apache Spark, Databricks | N/A |
| Real-time aggregation | N/A | Apache Flink, Kafka Streams |
| Sub-second latency | N/A | Apache Flink, Kinesis Analytics |
| SQL interface | dbt, Spark SQL | Flink SQL, ksqlDB |
| Unified batch + streaming | Apache Beam, Spark Structured Streaming | Apache Beam, Spark Structured Streaming |
| Managed service | AWS Glue, Databricks | AWS Kinesis, GCP Dataflow |
| Complex stateful logic | Spark (limited) | Flink (excellent state management) |
| Exactly-once guarantees | Spark (batch mode) | Flink, Kafka Streams |

**Apache Flink vs Kafka Streams**:
- **Flink**: Better for complex event processing, advanced windowing, large state
- **Kafka Streams**: Simpler deployment (library, not cluster), good for Kafka-centric architectures

**Spark vs Flink**:
- **Spark**: Better batch performance, micro-batching streaming, unified API
- **Flink**: True streaming, better latency, superior state management

---

## üí° Practical Examples

### Example 1: Fraud Detection - Hybrid Batch + Streaming

**Scenario**: Credit card fraud detection requiring <100ms inference latency

**Batch Layer** (Daily):
```python
# Compute historical user features overnight (Spark batch job)
def compute_user_historical_features(transactions_df, date):
    """
    Batch compute features like:
    - Average transaction amount (30/90 days)
    - Transaction count by merchant category
    - Geographic diversity score
    """
    user_features = transactions_df.filter(
        (F.col("date") >= F.date_sub(F.lit(date), 90))
    ).groupBy("user_id").agg(
        F.avg("amount").alias("avg_amount_90d"),
        F.count("*").alias("transaction_count_90d"),
        F.countDistinct("merchant_category").alias("merchant_diversity"),
        F.countDistinct("city").alias("geo_diversity_90d"),
        F.stddev("amount").alias("amount_stddev_90d")
    )

    # Write to feature store (batch table)
    user_features.write.format("delta").mode("overwrite") \
        .partitionBy("date") \
        .save("s3://features/user_historical/")

    return user_features
```

**Streaming Layer** (Real-time):
```python
# Apache Flink streaming job for real-time features
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, EnvironmentSettings

env = StreamExecutionEnvironment.get_execution_environment()
settings = EnvironmentSettings.new_instance().in_streaming_mode().build()
table_env = StreamTableEnvironment.create(env, environment_settings=settings)

# Define Kafka source for transaction stream
table_env.execute_sql("""
    CREATE TABLE transactions (
        user_id STRING,
        amount DOUBLE,
        merchant_category STRING,
        city STRING,
        event_time TIMESTAMP(3),
        WATERMARK FOR event_time AS event_time - INTERVAL '30' SECOND
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'transactions',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# Compute real-time features (last 1 hour, updated every 1 min)
table_env.execute_sql("""
    CREATE TABLE user_realtime_features AS
    SELECT
        user_id,
        HOP_START(event_time, INTERVAL '1' MINUTE, INTERVAL '1' HOUR) AS window_start,
        COUNT(*) AS txn_count_1h,
        AVG(amount) AS avg_amount_1h,
        MAX(amount) AS max_amount_1h,
        COUNT(DISTINCT merchant_category) AS merchant_diversity_1h
    FROM transactions
    GROUP BY user_id, HOP(event_time, INTERVAL '1' MINUTE, INTERVAL '1' HOUR)
""")

# Sink to Redis for low-latency serving
table_env.execute_sql("""
    CREATE TABLE redis_sink (
        user_id STRING,
        window_start TIMESTAMP(3),
        txn_count_1h BIGINT,
        avg_amount_1h DOUBLE,
        max_amount_1h DOUBLE,
        merchant_diversity_1h BIGINT,
        PRIMARY KEY (user_id) NOT ENFORCED
    ) WITH (
        'connector' = 'redis',
        'host' = 'localhost',
        'port' = '6379',
        'key-pattern' = 'fraud:user:${user_id}'
    )
""")

table_env.execute_sql("""
    INSERT INTO redis_sink
    SELECT * FROM user_realtime_features
""")
```

**Serving Layer** (Merge at inference):
```python
import redis
from delta import DeltaTable

class FraudFeatureService:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379)
        self.delta_table = DeltaTable.forPath(spark, "s3://features/user_historical/")

    def get_features(self, user_id: str, current_date: str):
        # Get batch features (historical, computed daily)
        batch_features = self.delta_table.toDF().filter(
            (F.col("user_id") == user_id) & (F.col("date") == current_date)
        ).collect()[0].asDict()

        # Get streaming features (real-time, updated every minute)
        realtime_key = f"fraud:user:{user_id}"
        realtime_features = self.redis_client.hgetall(realtime_key)
        realtime_features = {k.decode(): float(v.decode()) for k, v in realtime_features.items()}

        # Merge features
        merged = {**batch_features, **realtime_features}

        return merged

# Usage in FastAPI inference endpoint
@app.post("/predict/fraud")
async def predict_fraud(user_id: str, transaction_data: dict):
    start = time.time()

    features = feature_service.get_features(user_id, current_date=today)
    features.update(transaction_data)  # Add current transaction details

    fraud_score = fraud_model.predict_proba([features])[0, 1]

    latency_ms = (time.time() - start) * 1000
    logger.info(f"Fraud prediction latency: {latency_ms:.2f}ms")

    return {"fraud_score": fraud_score, "latency_ms": latency_ms}
```

**Why Hybrid?**:
- Batch provides rich historical context (90-day patterns) at low cost
- Streaming provides immediate behavioral signals (last hour activity)
- Serving layer merges both for comprehensive fraud detection

---

### Example 2: Recommendation System - Streaming Session Features

**Scenario**: E-commerce recommendations based on current browsing session

**Streaming Processing** (Flink Session Windows):
```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.window import SessionWindowTimeGapExtractor
from pyflink.common import Types

env = StreamExecutionEnvironment.get_execution_environment()

# Define click stream
click_stream = env.from_source(
    kafka_source,  # Kafka source configuration
    WatermarkStrategy.for_bounded_out_of_orderness(Duration.of_seconds(30)),
    "click-events"
)

# Session window: Group clicks until 30 minutes of inactivity
def compute_session_features(click_events):
    return click_events \
        .key_by(lambda event: event['user_id']) \
        .window(SessionWindows.with_gap(Duration.of_minutes(30))) \
        .process(SessionFeatureProcessor())

class SessionFeatureProcessor(ProcessWindowFunction):
    def process(self, key, context, elements):
        clicks = list(elements)

        # Compute session-level features
        session_features = {
            'user_id': key,
            'session_start': min(c['timestamp'] for c in clicks),
            'session_end': max(c['timestamp'] for c in clicks),
            'session_duration_sec': (max(c['timestamp'] for c in clicks) -
                                      min(c['timestamp'] for c in clicks)).total_seconds(),
            'total_clicks': len(clicks),
            'unique_products_viewed': len(set(c['product_id'] for c in clicks)),
            'categories_browsed': list(set(c['category'] for c in clicks)),
            'add_to_cart_count': sum(1 for c in clicks if c['event_type'] == 'add_to_cart'),
            'avg_time_per_product': (max(c['timestamp'] for c in clicks) -
                                      min(c['timestamp'] for c in clicks)).total_seconds() / len(set(c['product_id'] for c in clicks))
        }

        # Write to Redis for real-time serving
        yield session_features

# Execute streaming job
session_features_stream = compute_session_features(click_stream)

# Sink to Redis
session_features_stream.add_sink(
    RedisSink(
        redis_config={'host': 'localhost', 'port': 6379},
        key_pattern='session:{user_id}'
    )
)

env.execute("Session Feature Computation")
```

**Serving** (Real-time recommendations):
```python
@app.get("/recommendations/{user_id}")
async def get_recommendations(user_id: str):
    # Get current session features from Redis (computed in real-time by Flink)
    session_key = f"session:{user_id}"
    session_features = redis_client.hgetall(session_key)

    if not session_features:
        # No active session, fall back to batch user profile
        return get_batch_recommendations(user_id)

    # Use session features for context-aware recommendations
    categories_browsed = session_features.get('categories_browsed', [])
    unique_products_viewed = int(session_features.get('unique_products_viewed', 0))

    # Real-time recommendation logic
    if unique_products_viewed > 5 and session_features.get('add_to_cart_count', 0) == 0:
        # User is browsing extensively but not adding to cart ‚Üí recommend popular items in browsed categories
        recommendations = recommend_popular_in_categories(categories_browsed)
    else:
        # Standard collaborative filtering
        recommendations = recommend_collaborative(user_id, session_features)

    return {"recommendations": recommendations, "session_based": True}
```

**Why Streaming?**:
- Session features capture immediate user intent
- Low latency (<1s) enables dynamic recommendation updates
- Session windows naturally align with user behavior patterns

---

### Example 3: Batch Model Training with Streaming Feature Updates

**Scenario**: Weekly model retraining + daily streaming feature refresh

**Batch Training** (Spark):
```python
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier

def train_model_weekly():
    """
    Train model every Sunday on full historical dataset
    """
    # Load last 6 months of training data
    training_data = spark.read.format("delta") \
        .load("s3://ml-data/training/") \
        .filter(F.col("date") >= F.date_sub(F.current_date(), 180))

    # Feature engineering (batch computation)
    features = training_data.groupBy("user_id").agg(
        F.avg("purchase_amount").alias("avg_purchase"),
        F.count("*").alias("purchase_count"),
        F.max("purchase_amount").alias("max_purchase"),
        F.datediff(F.current_date(), F.max("purchase_date")).alias("days_since_purchase")
    )

    # Join with labels
    train_df = features.join(labels, "user_id")

    # Train model
    rf = RandomForestClassifier(numTrees=100, featuresCol="features", labelCol="will_churn")
    model = rf.fit(train_df)

    # Save model
    model.write().overwrite().save("s3://ml-models/churn-predictor/")

    # Log to MLflow
    mlflow.log_model(model, "churn-predictor")
    mlflow.log_metric("training_samples", train_df.count())

# Schedule with Airflow
train_model_dag = DAG('weekly_model_training', schedule_interval='@weekly')
```

**Streaming Feature Updates** (Flink):
```python
# Update features daily as new purchases stream in
def update_features_streaming():
    """
    Incrementally update user features as purchases arrive
    """
    table_env.execute_sql("""
        CREATE TABLE purchase_stream (
            user_id STRING,
            purchase_amount DOUBLE,
            purchase_date TIMESTAMP(3),
            WATERMARK FOR purchase_date AS purchase_date - INTERVAL '1' HOUR
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'purchases',
            'properties.bootstrap.servers' = 'localhost:9092'
        )
    """)

    # Incremental aggregation (running totals)
    table_env.execute_sql("""
        CREATE TABLE user_features_incremental AS
        SELECT
            user_id,
            COUNT(*) AS purchase_count,
            AVG(purchase_amount) AS avg_purchase,
            MAX(purchase_amount) AS max_purchase,
            MAX(purchase_date) AS last_purchase_date
        FROM purchase_stream
        GROUP BY user_id
    """)

    # Upsert to Delta Lake (merge new aggregations with existing)
    table_env.execute_sql("""
        INSERT INTO delta_features
        SELECT * FROM user_features_incremental
        ON DUPLICATE KEY UPDATE
            purchase_count = user_features_incremental.purchase_count,
            avg_purchase = user_features_incremental.avg_purchase
    """)
```

**Benefits**:
- Weekly batch training processes full historical data for model quality
- Streaming feature updates keep user profiles fresh daily
- Reduces training frequency (cost savings) while maintaining feature freshness

---

## üîß Code Examples

### Code Example 1: Windowing Operations in Flink

```python
from pyflink.datastream import StreamExecutionEnvironment, TimeCharacteristic
from pyflink.table import StreamTableEnvironment, EnvironmentSettings
from pyflink.table.window import Tumble, Slide, Session

env = StreamExecutionEnvironment.get_execution_environment()
env.set_stream_time_characteristic(TimeCharacteristic.EventTime)
settings = EnvironmentSettings.new_instance().in_streaming_mode().build()
t_env = StreamTableEnvironment.create(env, environment_settings=settings)

# Create source table with event time
t_env.execute_sql("""
    CREATE TABLE sensor_readings (
        sensor_id STRING,
        temperature DOUBLE,
        humidity DOUBLE,
        event_time TIMESTAMP(3),
        WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'sensor-data',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json',
        'scan.startup.mode' = 'latest-offset'
    )
""")

# 1. Tumbling Window (5-minute non-overlapping windows)
tumbling_agg = t_env.sql_query("""
    SELECT
        sensor_id,
        TUMBLE_START(event_time, INTERVAL '5' MINUTE) AS window_start,
        TUMBLE_END(event_time, INTERVAL '5' MINUTE) AS window_end,
        AVG(temperature) AS avg_temp,
        MAX(temperature) AS max_temp,
        MIN(temperature) AS min_temp,
        COUNT(*) AS reading_count
    FROM sensor_readings
    GROUP BY sensor_id, TUMBLE(event_time, INTERVAL '5' MINUTE)
""")

# 2. Sliding Window (10-minute window, sliding every 2 minutes)
sliding_agg = t_env.sql_query("""
    SELECT
        sensor_id,
        HOP_START(event_time, INTERVAL '2' MINUTE, INTERVAL '10' MINUTE) AS window_start,
        HOP_END(event_time, INTERVAL '2' MINUTE, INTERVAL '10' MINUTE) AS window_end,
        AVG(temperature) AS avg_temp_10min
    FROM sensor_readings
    GROUP BY sensor_id, HOP(event_time, INTERVAL '2' MINUTE, INTERVAL '10' MINUTE)
""")

# 3. Session Window (dynamic windows based on 30-minute inactivity gap)
session_agg = t_env.sql_query("""
    SELECT
        sensor_id,
        SESSION_START(event_time, INTERVAL '30' MINUTE) AS session_start,
        SESSION_END(event_time, INTERVAL '30' MINUTE) AS session_end,
        COUNT(*) AS readings_in_session,
        AVG(temperature) AS avg_temp_session
    FROM sensor_readings
    GROUP BY sensor_id, SESSION(event_time, INTERVAL '30' MINUTE)
""")

# Sink results to separate Kafka topics
t_env.execute_sql("""
    CREATE TABLE tumbling_sink (
        sensor_id STRING,
        window_start TIMESTAMP(3),
        window_end TIMESTAMP(3),
        avg_temp DOUBLE,
        max_temp DOUBLE,
        min_temp DOUBLE,
        reading_count BIGINT
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'tumbling-aggregates',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

tumbling_agg.execute_insert("tumbling_sink")
```

**Explanation**:
- **Watermarks** (`WATERMARK FOR event_time AS event_time - INTERVAL '10' SECOND`): Flink waits up to 10 seconds for late events before closing a window
- **Tumbling**: Use for periodic snapshots (e.g., hourly metrics)
- **Sliding**: Use for moving averages (e.g., "average over last 10 minutes, updated every 2 minutes")
- **Session**: Use for user behavior analysis (e.g., group clicks until 30-min inactivity)

---

### Code Example 2: Batch Processing with Spark (Optimized)

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Initialize Spark with optimized configurations
spark = SparkSession.builder \
    .appName("Batch Feature Engineering") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.files.maxPartitionBytes", "128MB") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.dynamicAllocation.enabled", "true") \
    .getOrCreate()

def compute_user_features_batch(date: str):
    """
    Batch compute user features with optimizations:
    - Broadcast join for small dimension tables
    - Partition pruning for date-partitioned data
    - Caching intermediate results
    - Adaptive query execution
    """

    # Read transactions (partitioned by date for efficient pruning)
    transactions = spark.read.format("delta") \
        .load("s3://data/transactions/") \
        .filter(
            (F.col("date") >= F.date_sub(F.lit(date), 90)) &
            (F.col("date") <= F.lit(date))
        )

    # Read user dimension (small table, broadcast join)
    users = spark.read.format("delta").load("s3://data/users/")
    users_broadcast = F.broadcast(users)  # Broadcast hint for small table

    # Cache transactions since we'll use it multiple times
    transactions.cache()

    # 1. Time-based aggregations with window functions
    user_time_features = transactions.groupBy("user_id").agg(
        # Last 30 days
        F.sum(F.when(F.col("date") >= F.date_sub(F.lit(date), 30), F.col("amount")).otherwise(0)).alias("amount_30d"),
        F.count(F.when(F.col("date") >= F.date_sub(F.lit(date), 30), 1)).alias("txn_count_30d"),

        # Last 90 days
        F.sum(F.col("amount")).alias("amount_90d"),
        F.count("*").alias("txn_count_90d"),
        F.avg("amount").alias("avg_amount_90d"),
        F.stddev("amount").alias("stddev_amount_90d"),

        # Recency
        F.datediff(F.lit(date), F.max("date")).alias("days_since_last_txn"),

        # Diversity metrics
        F.countDistinct("merchant_id").alias("merchant_diversity"),
        F.countDistinct("category").alias("category_diversity")
    )

    # 2. Trend features (comparing recent vs historical behavior)
    user_trend_features = transactions.groupBy("user_id").agg(
        (F.sum(F.when(F.col("date") >= F.date_sub(F.lit(date), 30), F.col("amount")).otherwise(0)) /
         (F.sum(F.col("amount")) / 3)).alias("spending_trend_ratio")  # Last 30d vs avg monthly over 90d
    )

    # 3. Sequence features (using window functions)
    window_spec = Window.partitionBy("user_id").orderBy(F.col("date").desc())

    sequence_features = transactions.withColumn("rank", F.row_number().over(window_spec)) \
        .filter(F.col("rank") <= 5) \
        .groupBy("user_id").agg(
            F.collect_list("amount").alias("last_5_amounts"),
            F.collect_list("category").alias("last_5_categories")
        )

    # 4. Join all feature sets
    user_features = user_time_features \
        .join(user_trend_features, "user_id", "left") \
        .join(sequence_features, "user_id", "left") \
        .join(users_broadcast, "user_id", "left")  # Broadcast join

    # Add metadata
    user_features = user_features.withColumn("feature_date", F.lit(date)) \
        .withColumn("created_at", F.current_timestamp())

    # Write to Delta Lake with optimizations
    user_features.write.format("delta") \
        .mode("overwrite") \
        .partitionBy("feature_date") \
        .option("overwriteSchema", "true") \
        .option("optimizeWrite", "true") \
        .option("autoCompact", "true") \
        .save("s3://features/user-features/")

    # Unpersist cache
    transactions.unpersist()

    print(f"Computed features for {user_features.count()} users on {date}")

# Execute batch job
compute_user_features_batch("2025-10-19")
```

**Optimizations**:
- **Adaptive Query Execution**: Spark dynamically optimizes query plan based on runtime statistics
- **Broadcast Join**: Small dimension tables (<10MB) are broadcast to all executors
- **Partition Pruning**: Only read relevant date partitions
- **Caching**: Reuse intermediate DataFrames
- **Dynamic Allocation**: Cluster scales based on workload

---

### Code Example 3: Handling Late Data with Watermarks

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
from pyflink.table.expressions import col, lit
from datetime import datetime, timedelta

env = StreamExecutionEnvironment.get_execution_environment()
env.enable_checkpointing(60000)  # Checkpoint every 60 seconds
t_env = StreamTableEnvironment.create(env)

# Define table with watermark strategy
t_env.execute_sql("""
    CREATE TABLE events (
        user_id STRING,
        event_type STRING,
        amount DOUBLE,
        event_time TIMESTAMP(3),
        processing_time AS PROCTIME(),
        -- Watermark: Allow up to 1 minute of lateness
        WATERMARK FOR event_time AS event_time - INTERVAL '1' MINUTE
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'events',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json',
        'json.timestamp-format.standard' = 'ISO-8601'
    )
""")

# Windowed aggregation with late data handling
t_env.execute_sql("""
    CREATE TABLE event_aggregates (
        user_id STRING,
        window_start TIMESTAMP(3),
        window_end TIMESTAMP(3),
        event_count BIGINT,
        total_amount DOUBLE
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'event-aggregates',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# Compute aggregates with tumbling window
t_env.execute_sql("""
    INSERT INTO event_aggregates
    SELECT
        user_id,
        TUMBLE_START(event_time, INTERVAL '5' MINUTE) AS window_start,
        TUMBLE_END(event_time, INTERVAL '5' MINUTE) AS window_end,
        COUNT(*) AS event_count,
        SUM(amount) AS total_amount
    FROM events
    GROUP BY user_id, TUMBLE(event_time, INTERVAL '5' MINUTE)
""")

# Advanced: Side output for late events (events arriving after watermark)
t_env.execute_sql("""
    CREATE TABLE late_events (
        user_id STRING,
        event_type STRING,
        amount DOUBLE,
        event_time TIMESTAMP(3),
        processing_time TIMESTAMP(3),
        lateness_seconds BIGINT
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'late-events',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# Capture late events (arrive after watermark has passed their window)
t_env.execute_sql("""
    INSERT INTO late_events
    SELECT
        user_id,
        event_type,
        amount,
        event_time,
        PROCTIME() AS processing_time,
        TIMESTAMPDIFF(SECOND, event_time, PROCTIME()) AS lateness_seconds
    FROM events
    WHERE event_time < PROCTIME() - INTERVAL '1' MINUTE  -- Arrived late
""")
```

**Watermark Trade-offs**:
- **Short watermark (10s)**: Low latency but may drop late events
- **Long watermark (5min)**: More complete results but higher latency
- **Adaptive watermarks**: Adjust based on observed latency distribution (advanced Flink feature)

**Monitoring Late Events**:
```python
# Separate Flink job to monitor late event percentage
t_env.execute_sql("""
    CREATE VIEW late_event_metrics AS
    SELECT
        TUMBLE_START(PROCTIME(), INTERVAL '1' MINUTE) AS window_start,
        COUNT(*) AS late_event_count,
        AVG(lateness_seconds) AS avg_lateness_sec,
        MAX(lateness_seconds) AS max_lateness_sec
    FROM late_events
    GROUP BY TUMBLE(PROCTIME(), INTERVAL '1' MINUTE)
""")
```

**Decision Rule**: If late events exceed 1% of total events, increase watermark interval.

---

### Code Example 4: Cost-Optimized Hybrid Architecture

```python
class HybridFeatureArchitecture:
    """
    Cost-optimized hybrid architecture:
    - Batch: Compute expensive features daily (large-scale aggregations)
    - Streaming: Compute cheap real-time features (simple counts, sums)
    - Serving: Merge at query time with caching
    """

    def __init__(self):
        self.spark = SparkSession.builder.appName("BatchFeatures").getOrCreate()
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.feature_cache = {}  # In-memory cache for frequently accessed features
        self.cache_ttl = 300  # 5 minutes

    def compute_batch_features_daily(self, date: str):
        """
        Batch job: Compute expensive features overnight
        Cost: ~$50/day for 10-node Spark cluster (4 hours)
        """
        transactions = self.spark.read.format("delta") \
            .load("s3://data/transactions/") \
            .filter(F.col("date") >= F.date_sub(F.lit(date), 90))

        # Expensive aggregations (require shuffles, full dataset scans)
        batch_features = transactions.groupBy("user_id").agg(
            # Statistical features (requires full dataset)
            F.percentile_approx("amount", [0.25, 0.5, 0.75]).alias("amount_quartiles"),
            F.stddev("amount").alias("amount_stddev"),
            F.skewness("amount").alias("amount_skewness"),

            # Time-series features
            F.collect_list(F.struct("date", "amount")).alias("daily_spending_ts"),

            # Graph-based features (merchant co-occurrence)
            F.countDistinct("merchant_id").alias("merchant_count"),

            # Category preference distribution
            F.collect_list(F.struct("category", "amount")).alias("category_distribution")
        )

        # Write to Delta Lake (queryable by serving layer)
        batch_features.write.format("delta").mode("overwrite") \
            .partitionBy("date") \
            .save("s3://features/batch-features/")

        # Also cache in Redis for fast serving (expire after 25 hours)
        batch_features_pd = batch_features.toPandas()
        for _, row in batch_features_pd.iterrows():
            key = f"batch:user:{row['user_id']}"
            self.redis_client.hset(key, mapping=row.to_dict())
            self.redis_client.expire(key, 90000)  # 25 hours

    def compute_streaming_features_realtime(self):
        """
        Streaming job: Compute cheap real-time features
        Cost: ~$150/month for 3-node Flink cluster (24/7)
        """
        from pyflink.table import StreamTableEnvironment

        t_env = StreamTableEnvironment.create(env)

        # Simple aggregations (no expensive shuffles)
        t_env.execute_sql("""
            INSERT INTO redis_realtime_features
            SELECT
                user_id,
                COUNT(*) AS txn_count_1h,
                SUM(amount) AS total_amount_1h,
                MAX(event_time) AS last_event_time
            FROM transaction_stream
            GROUP BY user_id, TUMBLE(event_time, INTERVAL '1' HOUR)
        """)

    def get_features_with_caching(self, user_id: str, current_date: str) -> dict:
        """
        Serving layer: Merge batch + streaming with multi-tier caching
        Target latency: <10ms (p99)
        """
        cache_key = f"{user_id}:{current_date}"

        # Tier 1: In-memory cache (fastest, <1ms)
        if cache_key in self.feature_cache:
            cache_entry = self.feature_cache[cache_key]
            if time.time() - cache_entry['timestamp'] < self.cache_ttl:
                return cache_entry['features']

        # Tier 2: Redis (fast, 1-5ms)
        batch_key = f"batch:user:{user_id}"
        realtime_key = f"realtime:user:{user_id}"

        batch_features = self.redis_client.hgetall(batch_key)
        realtime_features = self.redis_client.hgetall(realtime_key)

        # Tier 3: Delta Lake fallback (slow, 50-200ms)
        if not batch_features:
            batch_features = self._query_delta_lake(user_id, current_date)

        # Merge features
        merged_features = {**batch_features, **realtime_features}

        # Update in-memory cache
        self.feature_cache[cache_key] = {
            'features': merged_features,
            'timestamp': time.time()
        }

        return merged_features

    def _query_delta_lake(self, user_id: str, date: str) -> dict:
        """Fallback: Query Delta Lake directly (slower)"""
        df = self.spark.read.format("delta") \
            .load("s3://features/batch-features/") \
            .filter((F.col("user_id") == user_id) & (F.col("date") == date)) \
            .limit(1)

        if df.count() == 0:
            return {}

        return df.collect()[0].asDict()

# Cost breakdown:
# - Batch (4 hours/day, 10 nodes): ~$50/day = $1,500/month
# - Streaming (24/7, 3 nodes): ~$150/month
# - Redis (cache.r5.large): ~$100/month
# - Total: ~$1,750/month
#
# Compare to pure streaming (24/7, 15 nodes for same throughput): ~$3,500/month
# Savings: ~50% cost reduction
```

---

## ‚úÖ Best Practices

### 1. Start with Batch, Add Streaming Only When Necessary

**Principle**: Batch processing is simpler, cheaper, and easier to maintain. Add streaming only when latency requirements justify the added complexity.

**Decision Framework**:
- **Latency requirement > 1 hour**: Use batch
- **Latency requirement 5-60 minutes**: Use micro-batching (Spark Structured Streaming)
- **Latency requirement < 5 minutes**: Use streaming (Flink, Kafka Streams)
- **Latency requirement < 1 second**: Use streaming + caching (Redis)

**Example**: Start with daily batch model retraining. If model staleness becomes an issue, move to hourly micro-batching before committing to full streaming.

---

### 2. Use Watermarks Carefully

**Principle**: Watermarks balance completeness vs latency. Too aggressive watermarks drop late data; too conservative watermarks increase latency.

**Best Practices**:
- **Monitor late event percentage**: Track how many events arrive after watermark
- **Set watermark based on observed latency distribution**: P99 latency is a good starting point
- **Use side outputs for late events**: Capture late events in separate stream for analysis
- **Adjust watermarks based on monitoring**: Increase if late events exceed threshold (e.g., 1%)

**Example**:
```python
# Good: Watermark based on observed P99 event latency (30 seconds)
WATERMARK FOR event_time AS event_time - INTERVAL '30' SECOND

# Bad: Overly conservative watermark (unnecessarily high latency)
WATERMARK FOR event_time AS event_time - INTERVAL '10' MINUTE
```

---

### 3. Design for Idempotency

**Principle**: Both batch and streaming jobs should be idempotent (rerunning produces same result).

**Batch Idempotency**:
- Use `mode("overwrite")` for full partition rewrites
- Include date/version in output paths
- Use Delta Lake MERGE for upserts

**Streaming Idempotency**:
- Enable exactly-once semantics (Kafka transactions, Flink checkpoints)
- Use idempotent sinks (upserts, not appends)
- Include deduplication logic for event sources without guarantees

**Example**:
```python
# Idempotent batch write (Delta Lake merge)
new_features.write.format("delta") \
    .mode("overwrite") \
    .option("replaceWhere", f"date = '{target_date}'") \
    .save("s3://features/")

# Idempotent streaming sink (upsert to database)
stream.add_sink(
    JDBCSink.sink(
        "INSERT INTO features (user_id, feature_value) VALUES (?, ?) "
        "ON CONFLICT (user_id) DO UPDATE SET feature_value = EXCLUDED.feature_value",
        jdbc_params
    )
)
```

---

### 4. Optimize State Size in Streaming

**Principle**: Unbounded state growth causes OOM errors and slow checkpoints.

**State Management Strategies**:
- **TTL (Time-To-Live)**: Expire old state automatically
```python
state_backend.set_ttl(StateTtlConfig.new_builder(Time.hours(24)).build())
```
- **Windowing**: Use windows to bound state size
- **State Compaction**: Use RocksDB compaction for large state
- **State Backend Selection**: In-memory for <1GB, RocksDB for >1GB

**Monitoring**:
- Track state size per operator
- Alert if state size grows unbounded
- Benchmark checkpoint duration (<1 minute ideal)

---

### 5. Implement Backpressure Handling

**Principle**: Streaming systems must handle variable load to avoid data loss or system crashes.

**Backpressure Strategies**:
- **Kafka Consumer Lag**: Monitor lag and scale consumers
- **Flink Backpressure Monitoring**: Built-in backpressure metrics
- **Rate Limiting**: Limit ingestion rate during high load
- **Auto-Scaling**: Scale Flink TaskManagers based on CPU/memory

**Example** (Kafka consumer lag monitoring):
```python
# Prometheus metrics for Kafka lag
kafka_consumer_lag = Gauge('kafka_consumer_lag', 'Consumer lag in messages')

def monitor_consumer_lag():
    lag = consumer.lag()  # Get lag from Kafka admin API
    kafka_consumer_lag.set(lag)

    if lag > 1000000:  # 1M message lag threshold
        logger.warning(f"High consumer lag: {lag}")
        # Trigger auto-scaling or alert
```

---

### 6. Separate Compute and Storage

**Principle**: Decouple processing engines from storage for flexibility and cost optimization.

**Architecture**:
- **Storage**: Delta Lake, Apache Iceberg on S3/GCS (cheap, durable)
- **Compute**: Ephemeral Spark/Flink clusters (scale up/down)

**Benefits**:
- Run multiple processing engines on same data (Spark, Flink, Presto)
- Scale compute independently of storage
- Reduce costs by shutting down compute when idle

**Example**:
```python
# Multiple engines reading from same Delta Lake table
spark_df = spark.read.format("delta").load("s3://data/features/")
flink_table = t_env.from_path("delta.`s3://data/features/`")
presto_query = "SELECT * FROM delta.s3://data/features/"
```

---

### 7. Test Streaming Pipelines Thoroughly

**Principle**: Streaming bugs are harder to debug than batch; invest in testing.

**Testing Strategies**:
- **Unit Tests**: Test windowing logic, aggregations with synthetic data
- **Integration Tests**: Test end-to-end with embedded Kafka/Flink
- **Time Travel Tests**: Simulate late events, out-of-order events
- **Performance Tests**: Benchmark throughput and latency

**Example** (Flink unit test):
```python
import unittest
from pyflink.table import EnvironmentSettings, TableEnvironment

class TestStreamingAggregation(unittest.TestCase):
    def setUp(self):
        self.t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())

    def test_tumbling_window_aggregation(self):
        # Create test data
        self.t_env.execute_sql("""
            CREATE TABLE test_events (
                user_id STRING,
                amount DOUBLE,
                event_time TIMESTAMP(3),
                WATERMARK FOR event_time AS event_time
            ) WITH (
                'connector' = 'datagen',
                'number-of-rows' = '100'
            )
        """)

        # Apply aggregation
        result = self.t_env.sql_query("""
            SELECT user_id, COUNT(*) AS cnt
            FROM test_events
            GROUP BY user_id, TUMBLE(event_time, INTERVAL '5' MINUTE)
        """)

        # Validate results
        self.assertGreater(result.execute().collect().__len__(), 0)
```

---

### 8. Monitor End-to-End Latency

**Principle**: Track latency from event generation to final output (not just processing time).

**Metrics to Monitor**:
- **Event-to-Processing Latency**: Time from event generation to processing
- **Processing Latency**: Time spent in processing logic
- **End-to-End Latency**: Event generation ‚Üí final output
- **Consumer Lag**: Messages waiting to be processed

**Implementation**:
```python
# Embed timestamp in events
event = {
    'user_id': 'user123',
    'amount': 100.0,
    'event_time': datetime.utcnow().isoformat(),  # Event generation time
    'ingestion_time': None  # Populated by ingestion service
}

# Calculate latency in processing
def process_event(event):
    event_time = datetime.fromisoformat(event['event_time'])
    processing_time = datetime.utcnow()
    latency_ms = (processing_time - event_time).total_seconds() * 1000

    # Log to monitoring system
    latency_histogram.observe(latency_ms)

    if latency_ms > 1000:  # Alert if >1 second latency
        logger.warning(f"High latency: {latency_ms}ms for event {event['user_id']}")
```

---

### 9. Use Managed Services for Operational Simplicity

**Principle**: Managed services reduce operational overhead at the cost of flexibility and potentially higher expense.

**Trade-offs**:

| **Self-Managed (EMR, EKS)** | **Managed (Databricks, Kinesis)** |
|----------------------------|-----------------------------------|
| Full control over configuration | Limited configuration options |
| Lower cost (if optimized) | Higher cost but predictable |
| Requires dedicated ops team | Minimal operational overhead |
| Custom integrations | Pre-built integrations |
| Cluster management overhead | Auto-scaling, monitoring included |

**Recommendation**:
- **Small teams (<5 engineers)**: Use managed services (Databricks, AWS Kinesis)
- **Large teams with dedicated ops**: Self-managed for cost optimization
- **Hybrid**: Managed for streaming (complex ops), self-managed for batch (simpler)

---

### 10. Document Latency SLAs

**Principle**: Clearly define and document latency requirements to guide architectural decisions.

**SLA Template**:
```yaml
feature_pipeline_sla:
  feature_type: user_real_time_features
  latency_target:
    p50: 500ms
    p95: 1000ms
    p99: 2000ms
  freshness_requirement: <5 minutes
  data_completeness: 99.9%
  allowed_late_events: 1%

  monitoring:
    metrics:
      - end_to_end_latency
      - consumer_lag
      - late_event_percentage
    alerts:
      - trigger: p99_latency > 3000ms
        severity: warning
      - trigger: consumer_lag > 1000000
        severity: critical
```

---

## ‚ö†Ô∏è Common Pitfalls

### 1. Overengineering with Streaming When Batch Suffices

**Pitfall**: Implementing complex streaming pipelines when business requirements allow hourly or daily updates.

**Example**:
- **Bad**: Building a Flink streaming pipeline for model retraining when model is retrained weekly
- **Good**: Use Airflow batch DAG to retrain weekly; switch to streaming only if business needs <1 hour staleness

**Solution**: Always validate latency requirements before choosing streaming. Ask: "What is the business impact of 1-hour delay vs real-time?"

---

### 2. Ignoring Late Events

**Pitfall**: Setting aggressive watermarks that drop late events without monitoring impact.

**Example**:
- **Bad**: `WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND` drops events delayed >5s
- **Impact**: 10% of mobile app events arrive late due to offline usage ‚Üí model training on incomplete data

**Solution**:
- Monitor late event percentage
- Use side outputs to capture and analyze late events
- Adjust watermarks based on observed latency distribution

---

### 3. Not Planning for State Growth

**Pitfall**: Stateful streaming jobs with unbounded state leading to OOM errors.

**Example**:
```python
# Bad: Unbounded global aggregation (state grows forever)
SELECT user_id, COUNT(*) FROM events GROUP BY user_id
```

**Solution**:
```python
# Good: Window-based aggregation (state bounded by window size)
SELECT user_id, COUNT(*)
FROM events
GROUP BY user_id, TUMBLE(event_time, INTERVAL '1' DAY)
```

---

### 4. Mixing Event Time and Processing Time

**Pitfall**: Confusing event time (when event occurred) with processing time (when event was processed).

**Example**:
- **Bad**: Using processing time for aggregations when analyzing user behavior
- **Impact**: Late-arriving events assigned to wrong time buckets

**Solution**: Always use event time for business logic; use processing time only for monitoring.

```python
# Good: Event time-based aggregation
GROUP BY TUMBLE(event_time, INTERVAL '1' HOUR)

# Bad: Processing time-based aggregation (non-deterministic)
GROUP BY TUMBLE(PROCTIME(), INTERVAL '1' HOUR)
```

---

### 5. Not Testing Backpressure Scenarios

**Pitfall**: Streaming pipelines fail under load spikes due to untested backpressure handling.

**Example**:
- **Scenario**: Black Friday traffic spike 10x normal load ‚Üí Flink job crashes
- **Cause**: Insufficient resources + no backpressure testing

**Solution**:
- Load test with 2-5x expected traffic
- Implement rate limiting and auto-scaling
- Monitor backpressure metrics in production

---

### 6. Inefficient Batch Jobs Due to Data Skew

**Pitfall**: Batch jobs with extreme data skew cause some partitions to take 10x longer.

**Example**:
```python
# Bad: Skewed join (some users have 1M transactions, others have 10)
user_features = transactions.groupBy("user_id").agg(...)
```

**Solution**:
```python
# Good: Salting to distribute skewed keys
from pyspark.sql.functions import rand

transactions_salted = transactions.withColumn("salt", (rand() * 10).cast("int"))
user_features = transactions_salted.groupBy("user_id", "salt").agg(...) \
    .groupBy("user_id").agg(...)  # Re-aggregate to remove salt
```

---

### 7. Not Implementing Idempotency

**Pitfall**: Rerunning streaming or batch jobs produces different results or duplicates data.

**Example**:
- **Bad**: Appending to output table on every run
- **Impact**: Duplicate features after job retry

**Solution**: Use upserts (Delta Lake MERGE, database UPSERT) instead of appends.

---

### 8. Underestimating Streaming Operational Complexity

**Pitfall**: Assuming streaming is "just faster batch" without accounting for operational overhead.

**Hidden Costs**:
- 24/7 on-call for streaming pipeline failures
- Complex debugging (checkpoints, state recovery)
- Kafka cluster management (rebalancing, partition management)
- Monitoring and alerting infrastructure

**Solution**: Start small with streaming (1-2 critical pipelines), build operational maturity before scaling.

---

### 9. Not Monitoring Consumer Lag

**Pitfall**: Kafka consumer lag grows unbounded, eventually causing pipeline failure or data loss.

**Example**:
- **Scenario**: Consumer processes 1K events/sec, producer sends 1.5K events/sec
- **Impact**: Lag grows by 500 events/sec ‚Üí 1.8M lag after 1 hour ‚Üí exceed Kafka retention ‚Üí data loss

**Solution**:
- Monitor consumer lag continuously
- Alert when lag exceeds threshold (e.g., 100K messages)
- Auto-scale consumers based on lag

---

### 10. Poor Cost Visibility

**Pitfall**: Lack of cost tracking leads to unexpected cloud bills (especially with always-on streaming).

**Example**:
- **Scenario**: Streaming cluster running 24/7 without monitoring ‚Üí $5K/month surprise bill
- **Cause**: Over-provisioned cluster (100 cores when 20 suffice)

**Solution**:
- Tag resources with cost center and project
- Set up billing alerts (AWS Budgets, GCP Budgets)
- Right-size clusters based on actual usage
- Consider spot/preemptible instances for cost savings

---

## üèãÔ∏è Hands-On Exercises

### Exercise 1: Implement Windowing in Flink (Intermediate, 8-10 hours)

**Objective**: Build a streaming aggregation pipeline with multiple window types and understand watermarking.

**Scenario**: You have a stream of e-commerce clickstream events. Implement:
1. **Tumbling window**: Count clicks per user every 5 minutes
2. **Sliding window**: Calculate average session duration (10-min window, 2-min slide)
3. **Session window**: Group clicks into sessions (30-min inactivity gap)

**Tasks**:

1. **Set up Flink environment**:
```bash
# Start local Flink cluster
./bin/start-cluster.sh

# Start Kafka
docker run -d -p 9092:9092 --name kafka wurstmeister/kafka
```

2. **Generate synthetic clickstream data** (Python script):
```python
from kafka import KafkaProducer
import json, time, random
from datetime import datetime, timedelta

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

users = [f"user_{i}" for i in range(100)]

while True:
    event = {
        'user_id': random.choice(users),
        'event_type': random.choice(['click', 'view', 'add_to_cart']),
        'product_id': f"product_{random.randint(1, 1000)}",
        'event_time': (datetime.utcnow() - timedelta(seconds=random.randint(0, 60))).isoformat()
    }
    producer.send('clickstream', value=event)
    time.sleep(random.uniform(0.01, 0.1))  # Variable rate
```

3. **Implement Flink job** with all three window types (see Code Example 1 for reference)

4. **Introduce late events** (modify data generator to inject events with 2-minute delay)

5. **Experiment with watermarks**:
   - Start with 10-second watermark
   - Monitor late event percentage
   - Adjust watermark to 30 seconds and observe difference

**Validation**:
- Verify tumbling windows are non-overlapping
- Confirm sliding windows overlap correctly
- Check session windows merge consecutive events
- Measure percentage of late events dropped

**Stretch Goal**: Implement side output for late events and analyze their distribution.

---

### Exercise 2: Optimize a Slow Batch Job (Intermediate, 6-8 hours)

**Objective**: Take an inefficient Spark batch job and apply optimizations to improve performance.

**Scenario**: You have a slow feature engineering job that processes 1TB of transaction data. Current runtime: 4 hours. Target: <1 hour.

**Initial (Slow) Code**:
```python
# Slow job (no optimizations)
transactions = spark.read.parquet("s3://data/transactions/")  # 1TB unpartitioned data

user_features = transactions.groupBy("user_id").agg(
    F.count("*").alias("txn_count"),
    F.sum("amount").alias("total_amount")
)

# Slow join (broadcast not used)
users = spark.read.parquet("s3://data/users/")  # 1GB
result = user_features.join(users, "user_id")

result.write.parquet("s3://output/features/")  # Unoptimized write
```

**Tasks**:

1. **Enable Spark UI** and analyze execution plan:
```python
spark.sparkContext.setLogLevel("WARN")
# Open Spark UI: http://localhost:4040
```

2. **Apply optimizations**:
   - Add partition pruning (partition by date)
   - Use broadcast join for small tables
   - Enable adaptive query execution
   - Cache intermediate results
   - Optimize shuffle partitions
   - Use columnar format (Parquet with compression)

3. **Measure improvements**:
   - Baseline runtime
   - Runtime after each optimization
   - Final runtime

4. **Handle data skew**:
   - Identify skewed keys (users with 10x more transactions)
   - Apply salting technique

**Validation**:
- Achieve <1 hour runtime (target: 50% reduction)
- Verify correctness (compare output with baseline)
- Document optimization impact (create table showing runtime for each step)

**Deliverables**:
- Optimized Spark code
- Performance comparison table
- Spark UI screenshots showing improved plan

---

### Exercise 3: Build a Cost-Optimized Hybrid Pipeline (Advanced, 12-15 hours)

**Objective**: Design and implement a cost-optimized hybrid architecture balancing batch and streaming.

**Scenario**: Build a user recommendation system with:
- **Batch**: Compute collaborative filtering features daily (expensive, run overnight)
- **Streaming**: Track real-time session behavior (cheap, run 24/7)
- **Serving**: Merge both for recommendations with <50ms latency

**Tasks**:

1. **Batch Pipeline** (Spark):
   - Compute user-item collaborative filtering matrix (daily)
   - Features: user similarity scores, popular items in user cohort
   - Write to Delta Lake

2. **Streaming Pipeline** (Flink):
   - Track session clicks in real-time (tumbling 5-min windows)
   - Features: items viewed in session, add-to-cart count
   - Write to Redis (TTL = 1 hour)

3. **Serving Layer** (FastAPI):
   - Implement multi-tier caching (in-memory ‚Üí Redis ‚Üí Delta Lake)
   - Merge batch + streaming features
   - Return recommendations with latency <50ms (p99)

4. **Cost Analysis**:
   - Calculate monthly cost for batch cluster (4 hours/day)
   - Calculate monthly cost for streaming cluster (24/7)
   - Compare to pure streaming alternative
   - Document cost savings

5. **Load Testing**:
   - Use Locust or JMeter to test 1000 req/sec
   - Measure p50, p95, p99 latency
   - Verify cache hit rate >90%

**Validation**:
- Recommendations reflect both historical preferences (batch) and current session (streaming)
- Latency p99 <50ms
- Cost <$2000/month
- Cache hit rate >90%

**Deliverables**:
- Complete pipeline code (batch + streaming + serving)
- Cost breakdown spreadsheet
- Load testing report with latency distribution

---

### Exercise 4: Handle Late Events in Production (Advanced, 10-12 hours)

**Objective**: Design a production-grade streaming pipeline that gracefully handles late events.

**Scenario**: IoT sensor data arriving from offline devices with variable latency (5% arrive >1 minute late).

**Tasks**:

1. **Implement main pipeline** (Flink):
   - Aggregate sensor readings (average temperature per 5-min window)
   - Set watermark to 1 minute
   - Sink aggregates to Kafka topic `sensor-aggregates`

2. **Implement late event side output**:
   - Capture events arriving after watermark
   - Sink to separate Kafka topic `late-sensor-events`
   - Calculate lateness distribution

3. **Build reconciliation process**:
   - Batch job to recompute aggregates including late events
   - Compare with original streaming aggregates
   - Identify windows with >5% difference

4. **Monitoring dashboard** (Grafana + Prometheus):
   - Metric: Late event percentage
   - Metric: Average lateness
   - Metric: Windows requiring reconciliation
   - Alert if late events >10%

5. **Adaptive watermark**:
   - Implement logic to adjust watermark based on observed lateness
   - If late events <1%: decrease watermark (lower latency)
   - If late events >5%: increase watermark (more completeness)

**Validation**:
- <1% data loss (late events properly handled)
- Reconciliation identifies all windows with incomplete data
- Dashboard shows real-time lateness metrics

**Deliverables**:
- Flink pipeline with late event handling
- Batch reconciliation job
- Grafana dashboard JSON
- Report analyzing late event patterns

---

## üîó Related Concepts

### Within This Course:
- [[02. Lambda vs Kappa Architectures for ML]] - Architectural patterns combining batch and streaming
- [[01. Architectural Patterns for ML Systems]] - Overall system architecture considerations
- [[04. Model Training and Serving Data Flows]] - Batch training vs streaming inference patterns
- [[05. Monitoring and Feedback Loops]] - Detecting data drift in batch vs streaming contexts

### External Connections:
- **Data Storage**: [[02. Data Storage and Versioning Strategies]] - How storage choices impact batch vs streaming performance
- **Feature Engineering**: [[03. Feature Engineering Workflows]] - Computing features in batch vs streaming modes
- **Orchestration**: [[10. Workflow Orchestration]] - Scheduling batch jobs vs managing streaming pipelines
- **Infrastructure**: [[Cloud Data Architecture]] - Cost optimization for batch and streaming workloads

---

## üìö Further Reading

### Essential Books:

1. **"Streaming Systems" by Tyler Akidau et al. (O'Reilly, 2018)**
   - Comprehensive guide to streaming concepts
   - Covers watermarks, windowing, triggers in depth
   - From Google Dataflow team (Apache Beam creators)

2. **"Designing Data-Intensive Applications" by Martin Kleppmann (O'Reilly, 2017)**
   - Chapter 11: Stream Processing
   - Compares batch and streaming paradigms
   - Discusses event time, processing time, state management

3. **"Stream Processing with Apache Flink" by Fabian Hueske & Vasiliki Kalavri (O'Reilly, 2019)**
   - Hands-on Flink implementation
   - Covers stateful stream processing, windowing, exactly-once semantics

4. **"Learning Spark, 3rd Edition" by Jules S. Damji et al. (O'Reilly, 2023)**
   - Spark Structured Streaming for micro-batching
   - Batch optimization techniques

### Key Papers:

1. **"The Dataflow Model" (Google, 2015)**
   - Foundational paper on unified batch + streaming
   - Introduced watermarks, triggers, accumulation modes
   - https://research.google/pubs/pub43864/

2. **"Lambda Architecture" by Nathan Marz**
   - Original description of batch + speed layer architecture
   - http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html

3. **"Questioning the Lambda Architecture" by Jay Kreps (2014)**
   - Critique of Lambda, proposes Kappa architecture
   - https://www.oreilly.com/radar/questioning-the-lambda-architecture/

4. **"Millwheel: Fault-Tolerant Stream Processing at Internet Scale" (Google, 2013)**
   - Low-latency streaming at scale
   - Exactly-once delivery guarantees

### Blogs and Articles:

1. **Confluent Blog - "Kafka Streams vs. Flink"**
   - https://www.confluent.io/blog/kafka-streams-vs-flink/
   - Practical comparison of streaming frameworks

2. **Databricks Blog - "Structured Streaming in Apache Spark"**
   - https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html
   - Micro-batching approach

3. **Netflix Tech Blog - "Keystone Real-time Stream Processing Platform"**
   - https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a
   - Production streaming architecture at Netflix scale

4. **Uber Engineering - "AthenaX: Uber's Streaming Platform"**
   - https://eng.uber.com/athenax/
   - Lessons from production streaming deployments

### Documentation:

1. **Apache Flink Documentation**
   - https://flink.apache.org/
   - Comprehensive streaming concepts guide

2. **Apache Spark Structured Streaming Guide**
   - https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
   - Micro-batching and streaming APIs

3. **Apache Kafka Streams Documentation**
   - https://kafka.apache.org/documentation/streams/
   - Stream processing library guide

4. **Google Cloud Dataflow (Apache Beam) Documentation**
   - https://cloud.google.com/dataflow/docs
   - Unified batch and streaming programming model

### Video Courses:

1. **"Apache Flink Training" by Ververica (Free)**
   - https://training.ververica.com/
   - Hands-on Flink streaming development

2. **"Stream Processing with Apache Kafka" by Confluent**
   - https://developer.confluent.io/learn-kafka/
   - Kafka Streams and ksqlDB tutorials

3. **"Databricks Academy - Structured Streaming"**
   - https://www.databricks.com/learn/training/
   - Spark Structured Streaming deep dive

---

## üìù Key Takeaways

1. **Latency drives architecture choice**: Batch for >1 hour, micro-batching for 5-60 min, streaming for <5 min latency requirements.

2. **Batch is simpler and cheaper**: Start with batch processing and add streaming only when latency requirements justify the added complexity and cost.

3. **Streaming requires sophisticated state management**: Unbounded state growth is a common failure mode; use windowing, TTL, and state compaction.

4. **Watermarks balance completeness vs latency**: Monitor late event percentage and adjust watermarks based on observed event-time latency distribution.

5. **Hybrid architectures optimize cost**: Combine expensive batch jobs (historical features) with cheap streaming jobs (real-time signals) for cost-effective ML systems.

6. **Event time ‚â† processing time**: Always use event time for business logic to ensure deterministic, correct results; use processing time only for monitoring.

7. **Idempotency is critical**: Design both batch and streaming jobs to be rerunnable without producing duplicates or incorrect results.

8. **Operational complexity of streaming is significant**: 24/7 monitoring, consumer lag management, state recovery, and backpressure handling require dedicated operational expertise.

9. **Cost visibility prevents surprises**: Tag resources, monitor spending, right-size clusters, and consider managed services to control cloud costs.

10. **Test streaming pipelines thoroughly**: Unit test windowing logic, integration test with embedded Kafka/Flink, and load test for backpressure scenarios before production deployment.

---

## ‚úèÔ∏è Notes Section

**Personal Insights**:

**Questions to Explore**:

**Related Projects**:

---

*Last updated: 2025-10-19*
*Part of: [[DEforAI - Data Engineering for AI/ML]]
*Chapter: [[03. Data Architecture for ML Systems]]*
