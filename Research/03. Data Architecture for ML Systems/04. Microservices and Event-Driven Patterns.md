# 04. Microservices and Event-Driven Patterns

**Date:** 2025-10-19
**Status:** #research
**Tags:** #data-engineering #ml-systems #microservices #event-driven #distributed-systems #architecture #api-design

---

## ðŸ“‹ Overview

Modern ML systems increasingly adopt **microservices architecture** and **event-driven patterns** to achieve scalability, fault isolation, and independent deployment of components. These architectural patterns enable large ML platforms to be developed by multiple teams working in parallel while maintaining system reliability and agility.

**Microservices architecture** decomposes ML systems into small, independently deployable services that communicate via well-defined APIs. Each service owns its data and can be developed, tested, and deployed independently. This contrasts with monolithic architectures where all components are tightly coupled.

**Event-driven architecture** enables services to communicate asynchronously through events, reducing coupling and enabling real-time responsiveness. Services publish events when their state changes, and other services subscribe to relevant events without knowing about the publishers.

For ML systems, these patterns enable:
- **Independent scaling**: Scale feature computation, model serving, and monitoring independently
- **Fault isolation**: Failure in model serving doesn't crash feature pipeline
- **Technology diversity**: Use Python for ML, Go for high-throughput APIs, Spark for batch processing
- **Team autonomy**: Feature engineering team deploys without coordinating with model serving team
- **Continuous deployment**: Deploy new model versions without downtime

**Key Patterns**:
- **Service Decomposition**: Breaking ML systems into bounded contexts (feature service, model service, etc.)
- **Synchronous Communication**: REST, gRPC for request-response patterns
- **Asynchronous Communication**: Kafka, RabbitMQ for event streaming
- **Saga Pattern**: Managing distributed transactions across services
- **Event Sourcing**: Storing state changes as immutable event log
- **CQRS**: Separating read and write models for optimization

---

## ðŸŽ¯ Learning Objectives

By the end of this subchapter, you will:

1. **Decompose ML systems** into microservices using Domain-Driven Design (DDD) principles
2. **Design APIs** for ML services with proper versioning and backward compatibility
3. **Implement synchronous communication** patterns using REST and gRPC
4. **Build event-driven pipelines** using Kafka and event streaming patterns
5. **Handle distributed transactions** with saga pattern and compensating transactions
6. **Implement event sourcing** to maintain complete audit trail of ML system state changes
7. **Apply CQRS** to optimize read and write paths in ML feature stores
8. **Deploy service mesh** (Istio, Linkerd) for observability and traffic management
9. **Test microservices** independently and with contract testing
10. **Monitor distributed ML systems** with distributed tracing and centralized logging

---

## ðŸ“š Core Concepts

### 1. Microservices Decomposition for ML Systems

**Definition**: Breaking down ML systems into small, focused services that each handle a specific business capability.

**Service Decomposition Strategies**:

1. **By ML Pipeline Stage**:
   - Data Ingestion Service
   - Feature Engineering Service
   - Model Training Service
   - Model Serving Service
   - Monitoring Service

2. **By Business Domain** (Domain-Driven Design):
   - User Profile Service (user features)
   - Product Catalog Service (product embeddings)
   - Recommendation Service (inference)
   - Personalization Service (A/B testing)

3. **By Data Entity**:
   - Customer Service (customer features)
   - Transaction Service (transaction features)
   - Fraud Detection Service (fraud scoring)

**Example ML System Decomposition**:
```
ML Platform (Microservices)
â”œâ”€â”€ Feature Store API Service
â”‚   â”œâ”€â”€ Online Feature Serving (low-latency reads)
â”‚   â”œâ”€â”€ Offline Feature Access (batch training)
â”‚   â””â”€â”€ Feature Registry (metadata management)
â”œâ”€â”€ Model Training Service
â”‚   â”œâ”€â”€ Training Job Orchestration
â”‚   â”œâ”€â”€ Hyperparameter Tuning
â”‚   â””â”€â”€ Model Versioning
â”œâ”€â”€ Model Serving Service
â”‚   â”œâ”€â”€ Online Inference API
â”‚   â”œâ”€â”€ Batch Prediction Service
â”‚   â””â”€â”€ Model Loading and Caching
â”œâ”€â”€ Monitoring Service
â”‚   â”œâ”€â”€ Data Drift Detection
â”‚   â”œâ”€â”€ Model Performance Tracking
â”‚   â””â”€â”€ Alerting
â””â”€â”€ Orchestration Service
    â”œâ”€â”€ Pipeline Scheduling
    â””â”€â”€ Dependency Management
```

**Benefits**:
- Each service can scale independently (model serving needs 20 instances, feature store needs 5)
- Teams own services end-to-end (feature team, model team)
- Deploy model serving updates without touching feature pipeline

**Challenges**:
- Increased operational complexity (managing 10+ services instead of 1 monolith)
- Network latency between services
- Distributed debugging and tracing

---

### 2. Synchronous Communication Patterns

**Definition**: Request-response communication where caller waits for response before proceeding.

**REST APIs** (Representational State Transfer):
```http
GET /api/v1/features/user/user_123
POST /api/v1/models/fraud-detector/predict
PUT /api/v1/models/churn-predictor/version/v2
```

**Pros**:
- Simple, widely understood
- HTTP-based (works with load balancers, CDNs)
- Easy debugging with curl, Postman

**Cons**:
- JSON serialization overhead
- HTTP headers add latency
- No schema enforcement (unless using OpenAPI)

**gRPC** (Google Remote Procedure Call):
```protobuf
service FeatureService {
  rpc GetUserFeatures(UserRequest) returns (UserFeatures);
  rpc BatchGetFeatures(BatchRequest) returns (stream FeatureResponse);
}
```

**Pros**:
- 5-10x faster than REST (protobuf binary encoding)
- Built-in schema enforcement (protobuf definitions)
- Streaming support (bidirectional)
- HTTP/2 multiplexing (multiple requests on one connection)

**Cons**:
- More complex setup
- Less human-readable (binary format)
- Fewer tools compared to REST

**When to Use**:
- **REST**: Public APIs, browser clients, simple CRUD operations
- **gRPC**: Internal service communication, high-throughput APIs, streaming data

---

### 3. Asynchronous Communication Patterns

**Definition**: Services communicate by publishing events to a message broker; consumers process events independently.

**Event-Driven Architecture**:
```
Producer Service â†’ Kafka Topic â†’ Consumer Service 1
                              â†’ Consumer Service 2
                              â†’ Consumer Service 3
```

**Benefits**:
- **Loose coupling**: Producers don't know about consumers
- **Scalability**: Add new consumers without modifying producers
- **Fault tolerance**: Consumers can be down temporarily; messages are buffered
- **Event history**: Kafka retains events for replay

**Event Types**:

1. **Domain Events**: "User registered", "Purchase completed", "Model trained"
2. **Integration Events**: Cross-service communication ("Feature computed", "Prediction scored")
3. **Command Events**: Trigger actions ("Train model", "Update features")

**Example ML Events**:
```json
// Domain Event: Model training completed
{
  "event_type": "model_training_completed",
  "timestamp": "2025-10-19T10:30:00Z",
  "model_id": "fraud-detector-v2",
  "metrics": {"accuracy": 0.94, "auc": 0.96},
  "version": "v2.1.0"
}

// Integration Event: Feature values updated
{
  "event_type": "feature_values_updated",
  "timestamp": "2025-10-19T10:31:00Z",
  "user_id": "user_123",
  "features": {"purchase_count_30d": 15, "avg_amount": 120.50}
}
```

**Kafka vs RabbitMQ**:
| **Feature** | **Kafka** | **RabbitMQ** |
|------------|-----------|--------------|
| Use case | High-throughput event streaming | Message queuing |
| Ordering | Per-partition ordering | Per-queue ordering |
| Retention | Configurable (days/weeks) | Until consumed |
| Replay | Yes (consumers control offset) | No |
| Throughput | 1M+ msgs/sec | 100K msgs/sec |
| Best for | Event sourcing, log aggregation | Task queues, RPC patterns |

---

### 4. API Design and Versioning

**Principle**: APIs are contracts between services. Breaking changes require versioning to maintain backward compatibility.

**Versioning Strategies**:

1. **URL Versioning** (most common):
```http
GET /api/v1/features/user/123
GET /api/v2/features/user/123  # New version with additional fields
```

2. **Header Versioning**:
```http
GET /api/features/user/123
Headers: Accept: application/vnd.company.v2+json
```

3. **Query Parameter Versioning**:
```http
GET /api/features/user/123?version=2
```

**API Contract Guidelines**:
- **Additive changes are safe**: Adding new fields doesn't break existing clients
- **Removing fields is breaking**: Requires new API version
- **Changing field types is breaking**: `int` â†’ `float` requires new version
- **Renaming fields is breaking**: Use new name in new version, deprecate old

**Example Evolution**:
```python
# v1 API (initial)
{
  "user_id": "123",
  "features": {
    "purchase_count": 10
  }
}

# v2 API (added feature, backward compatible)
{
  "user_id": "123",
  "features": {
    "purchase_count": 10,
    "avg_amount": 120.50  # New field added
  }
}

# v3 API (breaking change: renamed field)
{
  "user_id": "123",
  "features": {
    "transaction_count": 10,  # Renamed from purchase_count
    "avg_transaction_amount": 120.50  # Renamed from avg_amount
  }
}
```

**Deprecation Policy**:
- Announce deprecation 3-6 months before removal
- Support at least 2 versions simultaneously
- Monitor usage metrics to ensure safe migration

---

### 5. Saga Pattern for Distributed Transactions

**Problem**: In microservices, a single business transaction may span multiple services. Traditional ACID transactions don't work across service boundaries.

**Example Scenario**: "Train and deploy new model"
1. Model Training Service: Train model
2. Model Registry Service: Register model metadata
3. Model Serving Service: Deploy model to production
4. Monitoring Service: Set up performance tracking

If step 3 fails, we need to rollback steps 1-2 to maintain consistency.

**Saga Pattern**: Break transaction into local transactions with compensating actions.

**Two Saga Types**:

1. **Choreography-Based Saga** (event-driven):
   - Each service publishes events after completing its transaction
   - Other services listen and react
   - No central coordinator

2. **Orchestration-Based Saga** (coordinated):
   - Central orchestrator controls the flow
   - Orchestrator invokes services and handles failures
   - Easier to reason about, but single point of failure

**Example: Orchestration-Based Saga**:
```python
class ModelDeploymentSaga:
    def __init__(self):
        self.training_service = ModelTrainingService()
        self.registry_service = ModelRegistryService()
        self.serving_service = ModelServingService()

    async def execute(self, model_config):
        model_id = None
        deployment_id = None

        try:
            # Step 1: Train model
            model_id = await self.training_service.train(model_config)

            # Step 2: Register model
            await self.registry_service.register(model_id, model_config)

            # Step 3: Deploy model
            deployment_id = await self.serving_service.deploy(model_id)

            return {"status": "success", "deployment_id": deployment_id}

        except Exception as e:
            # Compensating transactions (rollback)
            if deployment_id:
                await self.serving_service.undeploy(deployment_id)
            if model_id:
                await self.registry_service.deregister(model_id)
                await self.training_service.delete_model(model_id)

            return {"status": "failed", "error": str(e)}
```

**Considerations**:
- Saga requires idempotent operations (safe to retry)
- Eventual consistency (brief period where state is inconsistent)
- Compensating actions must always succeed (design carefully)

---

### 6. Event Sourcing

**Definition**: Store all state changes as a sequence of immutable events instead of storing current state.

**Traditional State Storage** (current state only):
```python
# Database stores only current state
user_features = {
    "user_id": "123",
    "purchase_count": 15,
    "last_purchase_date": "2025-10-15"
}
```

**Event Sourcing** (store all events):
```python
# Event log stores all state changes
events = [
    {"event": "user_registered", "timestamp": "2025-01-01", "user_id": "123"},
    {"event": "purchase_made", "timestamp": "2025-01-05", "user_id": "123", "amount": 50.0},
    {"event": "purchase_made", "timestamp": "2025-01-10", "user_id": "123", "amount": 75.0},
    ...
    {"event": "purchase_made", "timestamp": "2025-10-15", "user_id": "123", "amount": 100.0}
]

# Reconstruct current state by replaying events
def reconstruct_state(user_id):
    state = {"user_id": user_id, "purchase_count": 0}
    for event in get_events(user_id):
        if event["event"] == "purchase_made":
            state["purchase_count"] += 1
            state["last_purchase_date"] = event["timestamp"]
    return state
```

**Benefits for ML Systems**:
- **Complete audit trail**: See how features evolved over time
- **Time travel**: Reconstruct feature values at any point in history
- **Debugging**: Replay events to reproduce issues
- **Feature recomputation**: Recompute features with new logic by replaying events

**Challenges**:
- Event log grows unbounded (requires compaction or snapshotting)
- Replaying events can be slow (use snapshots for performance)
- Schema evolution of events is complex

**Snapshotting** (optimization):
```python
# Periodically create snapshots to avoid replaying all events
snapshot = {
    "user_id": "123",
    "snapshot_date": "2025-10-01",
    "state": {"purchase_count": 10, "last_purchase_date": "2025-09-30"}
}

# Reconstruct state = load snapshot + replay events since snapshot
def reconstruct_state_optimized(user_id, target_date):
    snapshot = get_latest_snapshot(user_id, before=target_date)
    state = snapshot["state"]

    events = get_events(user_id, since=snapshot["snapshot_date"], until=target_date)
    for event in events:
        apply_event(state, event)

    return state
```

---

### 7. CQRS (Command Query Responsibility Segregation)

**Principle**: Separate read (query) and write (command) models to optimize each independently.

**Traditional Model** (same model for reads and writes):
```python
class FeatureStore:
    def write_features(self, user_id, features):
        # Write to database
        pass

    def read_features(self, user_id):
        # Read from same database
        pass
```

**CQRS Model** (separate read and write):
```python
# Write Model (optimized for writes)
class FeatureWriteModel:
    def write_features(self, user_id, features):
        # Write to write-optimized store (e.g., PostgreSQL for ACID)
        self.write_db.insert(user_id, features)

        # Publish event
        self.event_bus.publish({
            "event": "features_updated",
            "user_id": user_id,
            "features": features
        })

# Read Model (optimized for reads)
class FeatureReadModel:
    def __init__(self):
        # Listen to events and update read-optimized store
        self.event_bus.subscribe("features_updated", self.update_read_model)

    def update_read_model(self, event):
        # Update read-optimized store (e.g., Redis for low latency)
        self.read_db.set(event["user_id"], event["features"])

    def read_features(self, user_id):
        # Read from read-optimized store
        return self.read_db.get(user_id)
```

**Benefits**:
- **Optimized reads**: Use Redis/DynamoDB for low-latency feature serving
- **Optimized writes**: Use PostgreSQL/Delta Lake for transactional writes
- **Independent scaling**: Scale read replicas separately from write nodes

**Use Case in ML**: Feature Store
- **Write path**: Batch feature computation writes to Delta Lake (columnar, append-optimized)
- **Read path**: Online feature serving reads from Redis (key-value, sub-ms latency)
- **Synchronization**: Streaming job replicates Delta Lake â†’ Redis

---

### 8. Service Mesh

**Definition**: Infrastructure layer that manages service-to-service communication, providing observability, security, and traffic management.

**Service Mesh Components** (using Istio):

1. **Data Plane**: Sidecar proxies (Envoy) deployed alongside each service
2. **Control Plane**: Manages and configures proxies (Istio Pilot, Citadel)

**Features**:
- **Traffic Management**: Load balancing, canary deployments, circuit breaking
- **Security**: mTLS (mutual TLS) for encrypted service communication
- **Observability**: Distributed tracing, metrics, logs
- **Resilience**: Retries, timeouts, circuit breakers

**Example: Canary Deployment with Istio**:
```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: fraud-detector
spec:
  hosts:
  - fraud-detector
  http:
  - match:
    - headers:
        canary:
          exact: "true"
    route:
    - destination:
        host: fraud-detector
        subset: v2  # New model version
      weight: 100
  - route:
    - destination:
        host: fraud-detector
        subset: v1  # Current model version
      weight: 90
    - destination:
        host: fraud-detector
        subset: v2  # New model version
      weight: 10  # 10% of production traffic to new version
```

**Distributed Tracing** (Jaeger with Istio):
- Tracks requests across multiple services
- Visualizes latency breakdown
- Identifies bottlenecks

**Example Trace**:
```
Request: GET /predict
â”œâ”€ Feature Service (20ms)
â”‚  â””â”€ Database Query (15ms)
â”œâ”€ Model Service (100ms)
â”‚  â”œâ”€ Model Loading (60ms)
â”‚  â””â”€ Inference (40ms)
â””â”€ Total Latency: 120ms
```

---

### 9. Domain-Driven Design (DDD) for ML Systems

**Principle**: Organize services around business domains (bounded contexts), not technical layers.

**Bounded Contexts in ML Platform**:

1. **User Context**: User profiles, user features, user segmentation
2. **Product Context**: Product catalog, product embeddings, product recommendations
3. **Transaction Context**: Purchase history, transaction features
4. **Model Context**: Model training, versioning, deployment
5. **Monitoring Context**: Drift detection, performance tracking, alerting

**Ubiquitous Language**: Use domain-specific terminology consistently across code, docs, and communication.
- "Purchase" (not "transaction record")
- "User churn" (not "user inactive flag")
- "Model deployment" (not "model upload")

**Aggregates**: Cluster of entities treated as a single unit.
- **User Aggregate**: User profile + user preferences + user history
- **Model Aggregate**: Model artifact + model metadata + training config

**Example DDD Service Boundaries**:
```python
# User Context Service
class UserService:
    def get_user_profile(self, user_id):
        pass

    def update_user_preferences(self, user_id, preferences):
        pass

# Recommendation Context Service
class RecommendationService:
    def get_recommendations(self, user_id):
        # Calls UserService to get user context
        user_profile = self.user_service.get_user_profile(user_id)
        # Calls ProductService to get product catalog
        products = self.product_service.get_products()
        # Calls ModelService to get predictions
        predictions = self.model_service.predict(user_profile, products)
        return predictions
```

---

### 10. Testing Microservices

**Testing Pyramid for Microservices**:

1. **Unit Tests** (70%): Test individual components
2. **Integration Tests** (20%): Test service with dependencies (database, message queue)
3. **Contract Tests** (5%): Test API contracts between services
4. **End-to-End Tests** (5%): Test full user workflow across services

**Contract Testing** (using Pact):
```python
# Consumer (Model Service) defines contract
@pact_test
def test_feature_service_contract():
    expected = {
        "user_id": "123",
        "features": {
            "purchase_count": 10,
            "avg_amount": 120.50
        }
    }

    # Define expected interaction
    pact.given("user 123 exists") \
        .upon_receiving("request for user features") \
        .with_request("GET", "/api/v1/features/user/123") \
        .will_respond_with(200, body=expected)

    # Test consumer uses contract correctly
    features = feature_client.get_user_features("123")
    assert features["purchase_count"] == 10

# Provider (Feature Service) verifies it meets contract
@pact_verify
def test_provider_honors_contract():
    # Pact framework replays consumer expectations against provider
    # Fails if provider doesn't match contract
    pass
```

**Benefits**:
- Catch breaking changes before deployment
- Services can evolve independently as long as contracts are met
- No need for end-to-end tests for every integration

---

## ðŸ’¡ Practical Examples

### Example 1: ML Platform with Microservices

**Scenario**: Build an e-commerce recommendation platform using microservices.

**Services**:
1. **Feature Service**: Serve user and product features
2. **Model Service**: Load models and serve predictions
3. **A/B Testing Service**: Route users to model variants
4. **Feedback Service**: Collect user interactions (clicks, purchases)
5. **Monitoring Service**: Track model performance

**Architecture**:
```
Frontend App
    â†“
API Gateway (Kong/AWS API Gateway)
    â†“
â”œâ”€ Feature Service (FastAPI, Redis)
â”œâ”€ Model Service (TorchServe, Kubernetes)
â”œâ”€ A/B Testing Service (Go, DynamoDB)
â”œâ”€ Feedback Service (Python, Kafka)
â””â”€ Monitoring Service (Python, Prometheus)
```

**Implementation**:

**1. Feature Service** (FastAPI):
```python
from fastapi import FastAPI, HTTPException
import redis
from pydantic import BaseModel

app = FastAPI(title="Feature Service")
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

class UserFeatures(BaseModel):
    user_id: str
    purchase_count_30d: int
    avg_purchase_amount: float
    category_preferences: list[str]

@app.get("/api/v1/features/user/{user_id}", response_model=UserFeatures)
async def get_user_features(user_id: str):
    """
    Get user features from Redis (populated by batch/streaming pipelines)
    """
    key = f"features:user:{user_id}"
    features = redis_client.hgetall(key)

    if not features:
        raise HTTPException(status_code=404, detail="User not found")

    return UserFeatures(
        user_id=user_id,
        purchase_count_30d=int(features.get("purchase_count_30d", 0)),
        avg_purchase_amount=float(features.get("avg_purchase_amount", 0.0)),
        category_preferences=features.get("category_preferences", "").split(",")
    )

@app.post("/api/v1/features/user/{user_id}")
async def update_user_features(user_id: str, features: UserFeatures):
    """
    Update user features (called by feature computation pipeline)
    """
    key = f"features:user:{user_id}"
    redis_client.hset(key, mapping={
        "purchase_count_30d": features.purchase_count_30d,
        "avg_purchase_amount": features.avg_purchase_amount,
        "category_preferences": ",".join(features.category_preferences)
    })
    redis_client.expire(key, 86400 * 30)  # 30-day TTL

    return {"status": "success"}

# Health check for Kubernetes
@app.get("/health")
async def health():
    return {"status": "healthy"}
```

**2. Model Service** (TorchServe with custom handler):
```python
import torch
import json
import httpx
from ts.torch_handler.base_handler import BaseHandler

class RecommendationHandler(BaseHandler):
    def __init__(self):
        super().__init__()
        self.feature_service_url = "http://feature-service/api/v1/features"

    def initialize(self, context):
        """Load model on startup"""
        self.manifest = context.manifest
        properties = context.system_properties
        model_dir = properties.get("model_dir")

        # Load PyTorch model
        self.model = torch.jit.load(f"{model_dir}/model.pt")
        self.model.eval()

        self.initialized = True

    async def preprocess(self, requests):
        """
        Fetch features from Feature Service
        """
        user_ids = [req.get("user_id") for req in requests]

        # Batch fetch features
        async with httpx.AsyncClient() as client:
            feature_requests = [
                client.get(f"{self.feature_service_url}/user/{uid}")
                for uid in user_ids
            ]
            responses = await asyncio.gather(*feature_requests)

        features = [resp.json() for resp in responses]
        return features

    def inference(self, features):
        """
        Run model inference
        """
        # Convert features to tensor
        feature_tensors = [
            torch.tensor([
                f["purchase_count_30d"],
                f["avg_purchase_amount"]
            ], dtype=torch.float32)
            for f in features
        ]

        batch_tensor = torch.stack(feature_tensors)

        # Predict
        with torch.no_grad():
            predictions = self.model(batch_tensor)

        return predictions.tolist()

    def postprocess(self, predictions):
        """
        Format predictions for response
        """
        return [{"score": pred} for pred in predictions]
```

**3. A/B Testing Service** (Go for high performance):
```go
package main

import (
    "encoding/json"
    "math/rand"
    "net/http"
    "github.com/gorilla/mux"
)

type ABTestRequest struct {
    UserID string `json:"user_id"`
    TestID string `json:"test_id"`
}

type ABTestResponse struct {
    Variant string `json:"variant"`
    ModelVersion string `json:"model_version"`
}

// Simple hash-based assignment (consistent for same user)
func assignVariant(userID string, testID string) string {
    hash := hashUserID(userID, testID)
    if hash % 2 == 0 {
        return "control"
    }
    return "treatment"
}

func handleABTest(w http.ResponseWriter, r *http.Request) {
    var req ABTestRequest
    json.NewDecoder(r.Body).Decode(&req)

    variant := assignVariant(req.UserID, req.TestID)

    // Map variant to model version
    modelVersion := "v1"
    if variant == "treatment" {
        modelVersion = "v2"
    }

    response := ABTestResponse{
        Variant: variant,
        ModelVersion: modelVersion,
    }

    w.Header().Set("Content-Type", "application/json")
    json.NewEncoder(w).Encode(response)
}

func main() {
    r := mux.NewRouter()
    r.HandleFunc("/api/v1/abtest/assign", handleABTest).Methods("POST")
    http.ListenAndServe(":8082", r)
}
```

**4. Orchestrating Services** (Python client):
```python
import httpx
import asyncio

class RecommendationOrchestrator:
    def __init__(self):
        self.feature_service = "http://feature-service"
        self.model_service = "http://model-service"
        self.abtest_service = "http://abtest-service"

    async def get_recommendations(self, user_id: str):
        async with httpx.AsyncClient() as client:
            # 1. Get A/B test variant (which model version to use)
            abtest_resp = await client.post(
                f"{self.abtest_service}/api/v1/abtest/assign",
                json={"user_id": user_id, "test_id": "rec_model_test"}
            )
            variant_data = abtest_resp.json()
            model_version = variant_data["model_version"]

            # 2. Get user features
            features_resp = await client.get(
                f"{self.feature_service}/api/v1/features/user/{user_id}"
            )
            features = features_resp.json()

            # 3. Get model predictions (using assigned model version)
            predictions_resp = await client.post(
                f"{self.model_service}/predictions/{model_version}",
                json={"user_id": user_id, "features": features}
            )
            predictions = predictions_resp.json()

            return {
                "user_id": user_id,
                "recommendations": predictions["recommendations"],
                "model_version": model_version,
                "variant": variant_data["variant"]
            }

# Usage
orchestrator = RecommendationOrchestrator()
recommendations = await orchestrator.get_recommendations("user_123")
```

---

### Example 2: Event-Driven Feature Pipeline

**Scenario**: Real-time feature updates using event-driven architecture.

**Flow**:
1. User makes purchase â†’ Purchase Service publishes `purchase_completed` event to Kafka
2. Feature Computation Service consumes event, updates features
3. Feature Computation Service publishes `features_updated` event
4. Feature Store Service consumes event, updates Redis
5. Monitoring Service consumes event, tracks feature freshness

**Implementation**:

**1. Purchase Service** (event producer):
```python
from kafka import KafkaProducer
import json
from datetime import datetime

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

class PurchaseService:
    def complete_purchase(self, user_id: str, product_id: str, amount: float):
        # 1. Save purchase to database
        purchase_id = self.db.save_purchase(user_id, product_id, amount)

        # 2. Publish domain event
        event = {
            "event_type": "purchase_completed",
            "timestamp": datetime.utcnow().isoformat(),
            "data": {
                "purchase_id": purchase_id,
                "user_id": user_id,
                "product_id": product_id,
                "amount": amount
            }
        }

        producer.send('purchase_events', value=event)
        producer.flush()

        return purchase_id
```

**2. Feature Computation Service** (event consumer & producer):
```python
from kafka import KafkaConsumer, KafkaProducer
import json

consumer = KafkaConsumer(
    'purchase_events',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    group_id='feature_computation_service'
)

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

class FeatureComputationService:
    def __init__(self):
        self.feature_store = {}  # In-memory state (or use database)

    def process_purchase_event(self, event):
        """
        Incrementally update user features
        """
        user_id = event["data"]["user_id"]
        amount = event["data"]["amount"]

        # Update features (simplified; in practice, query database for historical data)
        if user_id not in self.feature_store:
            self.feature_store[user_id] = {
                "purchase_count_30d": 0,
                "total_amount_30d": 0.0
            }

        self.feature_store[user_id]["purchase_count_30d"] += 1
        self.feature_store[user_id]["total_amount_30d"] += amount

        # Publish feature update event
        feature_event = {
            "event_type": "features_updated",
            "timestamp": datetime.utcnow().isoformat(),
            "data": {
                "user_id": user_id,
                "features": self.feature_store[user_id]
            }
        }

        producer.send('feature_events', value=feature_event)

    def run(self):
        for message in consumer:
            event = message.value
            if event["event_type"] == "purchase_completed":
                self.process_purchase_event(event)

# Start service
service = FeatureComputationService()
service.run()
```

**3. Feature Store Service** (event consumer, updates Redis):
```python
from kafka import KafkaConsumer
import redis
import json

consumer = KafkaConsumer(
    'feature_events',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    group_id='feature_store_service'
)

redis_client = redis.Redis(host='localhost', port=6379)

class FeatureStoreService:
    def process_feature_update_event(self, event):
        user_id = event["data"]["user_id"]
        features = event["data"]["features"]

        # Update Redis
        key = f"features:user:{user_id}"
        redis_client.hset(key, mapping=features)
        redis_client.expire(key, 86400 * 30)  # 30-day TTL

        print(f"Updated features for {user_id}: {features}")

    def run(self):
        for message in consumer:
            event = message.value
            if event["event_type"] == "features_updated":
                self.process_feature_update_event(event)

# Start service
service = FeatureStoreService()
service.run()
```

**Benefits**:
- Decoupled services (Purchase Service doesn't know about Feature Computation)
- Scalable (add more consumers to handle higher throughput)
- Fault-tolerant (events are persisted in Kafka; consumers can be down temporarily)
- Extensible (add new consumers without modifying producers)

---

### Example 3: Saga Pattern for Model Deployment

**Scenario**: Deploy a new model version across multiple services with rollback capability.

**Steps**:
1. Model Registry Service: Register new model metadata
2. Model Artifact Service: Upload model binary to S3
3. Model Serving Service: Deploy model to production
4. Monitoring Service: Create dashboards and alerts

If any step fails, rollback all previous steps.

**Implementation**:

```python
import asyncio
from dataclasses import dataclass
from enum import Enum

class SagaStatus(Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    ROLLED_BACK = "rolled_back"

@dataclass
class SagaStep:
    name: str
    execute: callable
    compensate: callable  # Rollback function
    status: SagaStatus = SagaStatus.PENDING

class ModelDeploymentSaga:
    def __init__(self):
        self.registry_service = ModelRegistryService()
        self.artifact_service = ModelArtifactService()
        self.serving_service = ModelServingService()
        self.monitoring_service = MonitoringService()

        self.steps = [
            SagaStep(
                name="Register Model Metadata",
                execute=self.registry_service.register_model,
                compensate=self.registry_service.deregister_model
            ),
            SagaStep(
                name="Upload Model Artifact",
                execute=self.artifact_service.upload_model,
                compensate=self.artifact_service.delete_model
            ),
            SagaStep(
                name="Deploy to Production",
                execute=self.serving_service.deploy,
                compensate=self.serving_service.rollback_deployment
            ),
            SagaStep(
                name="Setup Monitoring",
                execute=self.monitoring_service.create_dashboard,
                compensate=self.monitoring_service.delete_dashboard
            )
        ]

    async def execute(self, model_config: dict):
        """
        Execute saga with automatic rollback on failure
        """
        completed_steps = []
        saga_context = {"model_config": model_config}

        try:
            for step in self.steps:
                print(f"Executing: {step.name}")
                step.status = SagaStatus.IN_PROGRESS

                # Execute step
                result = await step.execute(saga_context)
                saga_context.update(result)  # Pass results to next step

                step.status = SagaStatus.COMPLETED
                completed_steps.append(step)

            return {"status": "success", "context": saga_context}

        except Exception as e:
            print(f"Saga failed at step: {step.name}")
            print(f"Error: {str(e)}")

            # Rollback completed steps in reverse order
            for completed_step in reversed(completed_steps):
                print(f"Rolling back: {completed_step.name}")
                try:
                    await completed_step.compensate(saga_context)
                    completed_step.status = SagaStatus.ROLLED_BACK
                except Exception as rollback_error:
                    print(f"Rollback failed for {completed_step.name}: {rollback_error}")
                    # Log to alerting system (rollback failures are critical!)

            return {"status": "failed", "error": str(e)}

# Service implementations
class ModelRegistryService:
    async def register_model(self, context):
        model_config = context["model_config"]
        model_id = f"model_{model_config['name']}_v{model_config['version']}"

        # Register in database
        await db.execute(
            "INSERT INTO models (id, name, version, created_at) VALUES (?, ?, ?, ?)",
            (model_id, model_config['name'], model_config['version'], datetime.utcnow())
        )

        return {"model_id": model_id}

    async def deregister_model(self, context):
        model_id = context.get("model_id")
        if model_id:
            await db.execute("DELETE FROM models WHERE id = ?", (model_id,))

class ModelArtifactService:
    async def upload_model(self, context):
        model_id = context["model_id"]
        model_path = context["model_config"]["local_path"]

        # Upload to S3
        s3_path = f"s3://models/{model_id}/model.pkl"
        await s3_client.upload_file(model_path, s3_path)

        return {"s3_path": s3_path}

    async def delete_model(self, context):
        s3_path = context.get("s3_path")
        if s3_path:
            await s3_client.delete_object(s3_path)

class ModelServingService:
    async def deploy(self, context):
        model_id = context["model_id"]
        s3_path = context["s3_path"]

        # Deploy to Kubernetes
        deployment_id = await kubernetes_client.create_deployment(
            name=model_id,
            image="model-server:latest",
            env={"MODEL_PATH": s3_path}
        )

        # Wait for deployment to be ready
        await kubernetes_client.wait_for_ready(deployment_id, timeout=300)

        return {"deployment_id": deployment_id}

    async def rollback_deployment(self, context):
        deployment_id = context.get("deployment_id")
        if deployment_id:
            await kubernetes_client.delete_deployment(deployment_id)

class MonitoringService:
    async def create_dashboard(self, context):
        model_id = context["model_id"]

        # Create Grafana dashboard
        dashboard_id = await grafana_client.create_dashboard(
            title=f"Model Monitoring - {model_id}",
            panels=[
                {"title": "Prediction Latency", "metric": f"model_{model_id}_latency"},
                {"title": "Prediction Count", "metric": f"model_{model_id}_count"}
            ]
        )

        return {"dashboard_id": dashboard_id}

    async def delete_dashboard(self, context):
        dashboard_id = context.get("dashboard_id")
        if dashboard_id:
            await grafana_client.delete_dashboard(dashboard_id)

# Usage
saga = ModelDeploymentSaga()
result = await saga.execute({
    "name": "fraud_detector",
    "version": "v2",
    "local_path": "/tmp/fraud_detector_v2.pkl"
})

if result["status"] == "success":
    print(f"Model deployed successfully: {result['context']['deployment_id']}")
else:
    print(f"Deployment failed: {result['error']}")
```

---

## ðŸ”§ Code Examples

### Code Example 1: gRPC Service for Feature Serving

**Define gRPC service** (feature_service.proto):
```protobuf
syntax = "proto3";

package features;

service FeatureService {
  rpc GetUserFeatures(UserFeaturesRequest) returns (UserFeaturesResponse);
  rpc BatchGetUserFeatures(BatchUserFeaturesRequest) returns (stream UserFeaturesResponse);
  rpc UpdateUserFeatures(UpdateUserFeaturesRequest) returns (UpdateResponse);
}

message UserFeaturesRequest {
  string user_id = 1;
}

message UserFeaturesResponse {
  string user_id = 1;
  map<string, float> features = 2;
}

message BatchUserFeaturesRequest {
  repeated string user_ids = 1;
}

message UpdateUserFeaturesRequest {
  string user_id = 1;
  map<string, float> features = 2;
}

message UpdateResponse {
  bool success = 1;
  string message = 2;
}
```

**Compile protobuf**:
```bash
python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. feature_service.proto
```

**Implement gRPC server**:
```python
import grpc
from concurrent import futures
import redis
import feature_service_pb2
import feature_service_pb2_grpc

class FeatureServiceServicer(feature_service_pb2_grpc.FeatureServiceServicer):
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

    def GetUserFeatures(self, request, context):
        """
        Single user feature request
        """
        user_id = request.user_id
        key = f"features:user:{user_id}"

        features_dict = self.redis_client.hgetall(key)

        if not features_dict:
            context.set_code(grpc.StatusCode.NOT_FOUND)
            context.set_details(f"User {user_id} not found")
            return feature_service_pb2.UserFeaturesResponse()

        # Convert string values to float
        features = {k: float(v) for k, v in features_dict.items()}

        return feature_service_pb2.UserFeaturesResponse(
            user_id=user_id,
            features=features
        )

    def BatchGetUserFeatures(self, request, context):
        """
        Streaming response for batch request
        """
        for user_id in request.user_ids:
            try:
                response = self.GetUserFeatures(
                    feature_service_pb2.UserFeaturesRequest(user_id=user_id),
                    context
                )
                yield response
            except Exception as e:
                print(f"Error fetching features for {user_id}: {e}")

    def UpdateUserFeatures(self, request, context):
        user_id = request.user_id
        features = dict(request.features)

        key = f"features:user:{user_id}"
        self.redis_client.hset(key, mapping=features)
        self.redis_client.expire(key, 86400 * 30)

        return feature_service_pb2.UpdateResponse(
            success=True,
            message=f"Features updated for {user_id}"
        )

def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    feature_service_pb2_grpc.add_FeatureServiceServicer_to_server(
        FeatureServiceServicer(), server
    )
    server.add_insecure_port('[::]:50051')
    print("gRPC server started on port 50051")
    server.start()
    server.wait_for_termination()

if __name__ == '__main__':
    serve()
```

**gRPC client**:
```python
import grpc
import feature_service_pb2
import feature_service_pb2_grpc

def get_user_features(user_id: str):
    with grpc.insecure_channel('localhost:50051') as channel:
        stub = feature_service_pb2_grpc.FeatureServiceStub(channel)

        request = feature_service_pb2.UserFeaturesRequest(user_id=user_id)
        response = stub.GetUserFeatures(request)

        print(f"Features for {response.user_id}: {dict(response.features)}")
        return dict(response.features)

def batch_get_user_features(user_ids: list[str]):
    with grpc.insecure_channel('localhost:50051') as channel:
        stub = feature_service_pb2_grpc.FeatureServiceStub(channel)

        request = feature_service_pb2.BatchUserFeaturesRequest(user_ids=user_ids)

        # Streaming response
        for response in stub.BatchGetUserFeatures(request):
            print(f"Features for {response.user_id}: {dict(response.features)}")

# Usage
features = get_user_features("user_123")
batch_get_user_features(["user_123", "user_456", "user_789"])
```

**Performance Comparison** (REST vs gRPC):
```python
import time
import requests

# REST benchmark
start = time.time()
for i in range(1000):
    requests.get(f"http://localhost:8000/api/v1/features/user/user_{i}")
rest_time = time.time() - start
print(f"REST: {rest_time:.2f}s for 1000 requests")

# gRPC benchmark
start = time.time()
with grpc.insecure_channel('localhost:50051') as channel:
    stub = feature_service_pb2_grpc.FeatureServiceStub(channel)
    for i in range(1000):
        stub.GetUserFeatures(feature_service_pb2.UserFeaturesRequest(user_id=f"user_{i}"))
grpc_time = time.time() - start
print(f"gRPC: {grpc_time:.2f}s for 1000 requests")

# Typical result: gRPC is 5-10x faster
# REST: 12.5s, gRPC: 2.1s
```

---

### Code Example 2: Circuit Breaker Pattern

**Purpose**: Prevent cascading failures when downstream services are unavailable.

```python
from enum import Enum
import time
from functools import wraps

class CircuitState(Enum):
    CLOSED = "closed"  # Normal operation
    OPEN = "open"      # Service is failing, reject requests
    HALF_OPEN = "half_open"  # Testing if service recovered

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60, success_threshold=2):
        self.failure_threshold = failure_threshold
        self.timeout = timeout  # Seconds before trying again
        self.success_threshold = success_threshold

        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    def call(self, func, *args, **kwargs):
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time < self.timeout:
                raise Exception("Circuit breaker is OPEN")
            else:
                # Timeout elapsed, try again (HALF_OPEN)
                self.state = CircuitState.HALF_OPEN

        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e

    def on_success(self):
        self.failure_count = 0

        if self.state == CircuitState.HALF_OPEN:
            self.success_count += 1
            if self.success_count >= self.success_threshold:
                # Service recovered, close circuit
                self.state = CircuitState.CLOSED
                self.success_count = 0

    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            # Too many failures, open circuit
            self.state = CircuitState.OPEN

        if self.state == CircuitState.HALF_OPEN:
            # Failed during recovery, open circuit again
            self.state = CircuitState.OPEN
            self.success_count = 0

# Decorator for easy usage
def circuit_breaker(failure_threshold=5, timeout=60):
    breaker = CircuitBreaker(failure_threshold=failure_threshold, timeout=timeout)

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            return breaker.call(func, *args, **kwargs)
        return wrapper
    return decorator

# Usage: Protect model service calls
class ModelService:
    @circuit_breaker(failure_threshold=3, timeout=30)
    def predict(self, features):
        # Call to external model service
        response = requests.post("http://model-service/predict", json=features)
        response.raise_for_status()
        return response.json()

# If model service is down:
# - First 3 requests fail and are attempted
# - After 3 failures, circuit opens
# - Next requests fail immediately without calling service
# - After 30 seconds, try again (HALF_OPEN)
# - If 2 consecutive successes, close circuit
```

---

### Code Example 3: Implementing CQRS with Event Sourcing

```python
from dataclasses import dataclass
from datetime import datetime
from typing import List
import json

@dataclass
class Event:
    event_id: str
    event_type: str
    timestamp: datetime
    data: dict

# Write Model: Event Store
class EventStore:
    def __init__(self):
        self.events = []  # In production: use Kafka or event database

    def append_event(self, event: Event):
        self.events.append(event)
        print(f"Event stored: {event.event_type}")

    def get_events(self, entity_id: str, since: datetime = None) -> List[Event]:
        filtered = [e for e in self.events if e.data.get("user_id") == entity_id]
        if since:
            filtered = [e for e in filtered if e.timestamp >= since]
        return filtered

# Command Model (Write)
class FeatureCommandModel:
    def __init__(self, event_store: EventStore):
        self.event_store = event_store

    def update_user_features(self, user_id: str, features: dict):
        """
        Write command: Store as event
        """
        event = Event(
            event_id=f"evt_{int(datetime.utcnow().timestamp())}",
            event_type="features_updated",
            timestamp=datetime.utcnow(),
            data={"user_id": user_id, "features": features}
        )

        self.event_store.append_event(event)

        # Publish to event bus for read model to consume
        event_bus.publish(event)

# Read Model (Query)
class FeatureQueryModel:
    def __init__(self):
        self.materialized_view = {}  # In production: use Redis/DynamoDB

    def handle_feature_updated_event(self, event: Event):
        """
        Event handler: Update materialized view
        """
        user_id = event.data["user_id"]
        features = event.data["features"]

        # Update materialized view (optimized for reads)
        self.materialized_view[user_id] = features
        print(f"Materialized view updated for {user_id}")

    def get_user_features(self, user_id: str) -> dict:
        """
        Query: Read from materialized view (fast)
        """
        return self.materialized_view.get(user_id, {})

# Event Bus (connects write and read models)
class EventBus:
    def __init__(self):
        self.subscribers = []

    def subscribe(self, handler):
        self.subscribers.append(handler)

    def publish(self, event: Event):
        for handler in self.subscribers:
            handler(event)

# Usage
event_store = EventStore()
event_bus = EventBus()

# Write model
command_model = FeatureCommandModel(event_store)

# Read model
query_model = FeatureQueryModel()
event_bus.subscribe(query_model.handle_feature_updated_event)

# Write: Update features (command)
command_model.update_user_features("user_123", {"purchase_count": 10})
command_model.update_user_features("user_123", {"purchase_count": 11})

# Read: Query features (query)
features = query_model.get_user_features("user_123")
print(f"Current features: {features}")  # {"purchase_count": 11}

# Rebuild read model from events (time travel)
query_model_rebuilt = FeatureQueryModel()
for event in event_store.get_events("user_123"):
    query_model_rebuilt.handle_feature_updated_event(event)

features_rebuilt = query_model_rebuilt.get_user_features("user_123")
print(f"Rebuilt features: {features_rebuilt}")  # Same as current
```

**Benefits**:
- **Write model** optimized for transactional writes (event append is O(1))
- **Read model** optimized for fast queries (key-value lookup in Redis)
- **Event log** provides complete audit trail
- **Replay events** to rebuild state or fix bugs in projection logic

---

### Code Example 4: Distributed Tracing with OpenTelemetry

```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
import requests

# Setup tracing
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

# Export traces to Jaeger
jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

# Auto-instrument HTTP requests
RequestsInstrumentor().instrument()

# Example: Trace ML prediction workflow
class MLPredictionService:
    def __init__(self):
        self.feature_service_url = "http://feature-service"
        self.model_service_url = "http://model-service"

    def predict(self, user_id: str):
        with tracer.start_as_current_span("ml_prediction") as span:
            span.set_attribute("user_id", user_id)

            # Step 1: Fetch features
            with tracer.start_as_current_span("fetch_features"):
                features = self._fetch_features(user_id)
                span.set_attribute("feature_count", len(features))

            # Step 2: Run model inference
            with tracer.start_as_current_span("model_inference"):
                prediction = self._run_inference(features)
                span.set_attribute("prediction_score", prediction["score"])

            return prediction

    def _fetch_features(self, user_id: str):
        # HTTP request is auto-instrumented
        response = requests.get(f"{self.feature_service_url}/api/v1/features/user/{user_id}")
        return response.json()

    def _run_inference(self, features: dict):
        response = requests.post(f"{self.model_service_url}/predict", json=features)
        return response.json()

# Usage
service = MLPredictionService()
prediction = service.predict("user_123")

# View trace in Jaeger UI (http://localhost:16686)
# Trace visualization:
# ml_prediction (120ms)
#   â”œâ”€ fetch_features (20ms)
#   â”‚  â””â”€ HTTP GET /api/v1/features/user/user_123 (18ms)
#   â””â”€ model_inference (95ms)
#      â””â”€ HTTP POST /predict (93ms)
```

**Benefits**:
- Visualize latency breakdown across services
- Identify bottlenecks (model inference taking 95ms)
- Debug failures by tracing request path
- Monitor service dependencies

---

## âœ… Best Practices

### 1. Design for Failure

**Principle**: In distributed systems, failures are inevitable. Design services to handle failures gracefully.

**Strategies**:
- **Circuit Breakers**: Prevent cascading failures
- **Retries with Exponential Backoff**: Retry transient failures
- **Timeouts**: Fail fast instead of hanging indefinitely
- **Bulkheads**: Isolate resources to prevent one failing component from bringing down the entire system

**Example**:
```python
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10)
)
def call_model_service(features):
    response = requests.post("http://model-service/predict", json=features, timeout=5)
    response.raise_for_status()
    return response.json()

# Retries: 1s, 2s, 4s delays before giving up
```

---

### 2. Use Idempotent APIs

**Principle**: API calls should be safe to retry without causing duplicate side effects.

**Implementation**:
- **Idempotency Keys**: Client provides unique key for each request
- **Check-Then-Act**: Server checks if request was already processed

**Example**:
```python
@app.post("/api/v1/models/deploy")
async def deploy_model(model_id: str, idempotency_key: str):
    # Check if deployment already exists with this key
    existing = db.query("SELECT * FROM deployments WHERE idempotency_key = ?", (idempotency_key,))

    if existing:
        # Request was already processed, return existing result
        return existing

    # Process deployment
    deployment_id = deploy(model_id)

    # Store with idempotency key
    db.execute(
        "INSERT INTO deployments (deployment_id, model_id, idempotency_key) VALUES (?, ?, ?)",
        (deployment_id, model_id, idempotency_key)
    )

    return {"deployment_id": deployment_id}
```

---

### 3. Version APIs from Day One

**Principle**: Breaking API changes require new versions to avoid breaking existing clients.

**Best Practices**:
- Start with `/api/v1/`
- Maintain at least 2 versions simultaneously
- Announce deprecation 3-6 months in advance
- Monitor version usage to plan end-of-life

---

### 4. Implement Health Checks

**Principle**: Services should expose health endpoints for orchestration systems (Kubernetes, load balancers).

**Health Check Types**:
1. **Liveness**: Is the service running? (restart if fails)
2. **Readiness**: Is the service ready to accept traffic? (remove from load balancer if fails)

**Example**:
```python
@app.get("/health/liveness")
async def liveness():
    # Simple check: service is running
    return {"status": "alive"}

@app.get("/health/readiness")
async def readiness():
    # Check dependencies: database, Redis, model loaded
    checks = {
        "database": check_database(),
        "redis": check_redis(),
        "model_loaded": check_model_loaded()
    }

    all_healthy = all(checks.values())
    status_code = 200 if all_healthy else 503

    return Response(
        content=json.dumps({"status": "ready" if all_healthy else "not_ready", "checks": checks}),
        status_code=status_code
    )
```

---

### 5. Use Contract Testing

**Principle**: Test service integrations using consumer-driven contracts (Pact).

**Benefits**:
- Catch breaking changes before deployment
- No need for full end-to-end tests
- Services evolve independently

---

### 6. Implement Distributed Tracing

**Principle**: Trace requests across services to debug latency and failures.

**Tools**: Jaeger, Zipkin, AWS X-Ray
**Standard**: OpenTelemetry

---

### 7. Centralize Logging

**Principle**: Aggregate logs from all services for debugging and monitoring.

**Stack**: Fluentd/Fluent Bit â†’ Elasticsearch â†’ Kibana
**Structured Logging**:
```python
import structlog

logger = structlog.get_logger()

logger.info("prediction_request", user_id="123", model_version="v2", latency_ms=45.2)
# Output: {"event": "prediction_request", "user_id": "123", "model_version": "v2", "latency_ms": 45.2}
```

---

### 8. Use Asynchronous Communication for Non-Critical Paths

**Principle**: Use events for operations that don't need immediate response.

**Example**:
- **Synchronous**: Feature serving (need features immediately for prediction)
- **Asynchronous**: Feature computation (can happen in background), monitoring

---

### 9. Limit Service Dependencies

**Principle**: Minimize direct service-to-service calls to reduce coupling.

**Pattern**: Use event-driven architecture instead of direct calls where possible.

---

### 10. Monitor Service Metrics

**Key Metrics** (RED method):
- **Rate**: Requests per second
- **Errors**: Error rate
- **Duration**: Latency (p50, p95, p99)

**Tools**: Prometheus + Grafana

```python
from prometheus_client import Counter, Histogram

request_count = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint'])
request_latency = Histogram('http_request_duration_seconds', 'HTTP request latency')

@app.get("/api/v1/predict")
@request_latency.time()
async def predict(user_id: str):
    request_count.labels(method='GET', endpoint='/predict').inc()
    # ... prediction logic
```

---

## âš ï¸ Common Pitfalls

### 1. Overly Fine-Grained Microservices

**Pitfall**: Creating too many tiny services ("nanoservices") increases operational complexity without benefits.

**Example**:
- **Bad**: Separate services for UserFeatureReader, UserFeatureWriter, UserFeatureValidator (3 services for one domain)
- **Good**: Single UserFeatureService with read/write/validate operations

**Guideline**: Start with larger services, split only when needed (different scaling needs, different teams).

---

### 2. Distributed Monolith

**Pitfall**: Microservices that are tightly coupled (cannot deploy independently).

**Symptoms**:
- Service A can't deploy without updating Service B
- Shared database across services
- Synchronous calls creating dependency chains

**Solution**: Use asynchronous communication, avoid shared databases, enforce bounded contexts.

---

### 3. Ignoring Network Latency

**Pitfall**: Excessive service-to-service calls add latency.

**Example**:
```python
# Bad: 3 sequential service calls (300ms total)
features = feature_service.get(user_id)  # 100ms
product = product_service.get(product_id)  # 100ms
prediction = model_service.predict(features, product)  # 100ms
```

**Solution**: Batch requests, use caching, consider denormalization.

---

### 4. Not Handling Eventual Consistency

**Pitfall**: Assuming strong consistency in event-driven systems.

**Example**: User updates profile â†’ Profile service publishes event â†’ Recommendation service hasn't received event yet â†’ recommendation uses stale profile.

**Solution**: Design UI/UX to handle eventual consistency, use read-your-writes consistency where needed.

---

### 5. Poor Error Handling in Async Communication

**Pitfall**: Events fail silently without retries or dead-letter queues.

**Solution**: Configure dead-letter queues (DLQ) for failed messages, implement retry logic.

---

### 6. Not Versioning Events

**Pitfall**: Changing event schema breaks consumers.

**Solution**: Version events, support multiple versions simultaneously.

```json
{
  "event_version": "v2",
  "event_type": "purchase_completed",
  "data": {...}
}
```

---

### 7. Lack of Observability

**Pitfall**: Can't debug failures across services without distributed tracing.

**Solution**: Implement distributed tracing, centralized logging, and metrics from day one.

---

### 8. Synchronous Communication for Everything

**Pitfall**: Using only REST/gRPC creates tight coupling and cascading failures.

**Solution**: Use asynchronous events for non-critical workflows.

---

### 9. Not Testing Compensating Transactions

**Pitfall**: Saga rollback logic fails in production because it wasn't tested.

**Solution**: Test failure scenarios and rollback paths explicitly.

---

### 10. Ignoring Security in Service Communication

**Pitfall**: Internal services communicate without authentication/encryption.

**Solution**: Use mTLS (mutual TLS) via service mesh, implement service-to-service authentication.

---

## ðŸ‹ï¸ Hands-On Exercises

### Exercise 1: Build a Microservices ML Platform (Intermediate, 12-15 hours)

**Objective**: Build a complete ML platform with microservices architecture.

**Services to Implement**:
1. Feature Service (FastAPI + Redis)
2. Model Service (FastAPI + PyTorch/TensorFlow)
3. API Gateway (Kong or custom)

**Tasks**:

1. **Feature Service**:
   - REST API: GET /features/user/{user_id}
   - gRPC API: GetUserFeatures
   - Store features in Redis
   - Health checks

2. **Model Service**:
   - REST API: POST /predict
   - Load model on startup
   - Return predictions with latency metrics

3. **API Gateway**:
   - Route requests to services
   - Rate limiting (100 req/min per user)
   - Authentication (JWT tokens)

4. **Deploy with Docker Compose**:
```yaml
version: '3.8'
services:
  redis:
    image: redis:latest
  feature-service:
    build: ./feature-service
    ports:
      - "8001:8000"
  model-service:
    build: ./model-service
    ports:
      - "8002:8000"
  api-gateway:
    build: ./api-gateway
    ports:
      - "8080:8000"
```

**Validation**:
- Services run independently
- Can deploy feature service without restarting model service
- API gateway routes requests correctly
- Health checks return 200

**Deliverables**:
- Code for all 3 services
- Docker Compose configuration
- API documentation (OpenAPI)
- Load test results (1000 req/sec)

---

### Exercise 2: Event-Driven Pipeline with Kafka (Intermediate, 10-12 hours)

**Objective**: Build an event-driven feature pipeline.

**Architecture**:
1. Data Ingestion Service â†’ publishes events to Kafka
2. Feature Computation Service â†’ consumes events, computes features, publishes feature events
3. Feature Store Service â†’ consumes feature events, updates Redis

**Tasks**:

1. **Set up Kafka**:
```bash
docker run -d -p 9092:9092 --name kafka wurstmeister/kafka
```

2. **Implement Producer** (Data Ingestion):
   - Read CSV file of user transactions
   - Publish each transaction as Kafka event

3. **Implement Consumer 1** (Feature Computation):
   - Consume transaction events
   - Compute aggregated features (purchase_count, avg_amount)
   - Publish feature_updated events

4. **Implement Consumer 2** (Feature Store):
   - Consume feature_updated events
   - Update Redis

5. **Test Event Flow**:
   - Produce 1000 transaction events
   - Verify features are computed and stored in Redis
   - Measure end-to-end latency

**Validation**:
- Events flow through pipeline
- Features updated in Redis
- Can add new consumer without modifying producers

**Deliverables**:
- Producer and consumer code
- Docker Compose with Kafka + Redis
- End-to-end latency measurement

---

### Exercise 3: Implement Saga Pattern (Advanced, 10-12 hours)

**Objective**: Implement orchestration-based saga for model deployment.

**Saga Steps**:
1. Upload model to S3
2. Register model in database
3. Deploy to Kubernetes
4. Create monitoring dashboard

**Tasks**:

1. **Implement SagaOrchestrator**:
   - Execute steps sequentially
   - On failure, rollback completed steps in reverse order
   - Log saga execution state

2. **Implement Compensating Actions**:
   - Delete S3 object if deployment fails
   - Delete database entry if S3 upload fails

3. **Test Failure Scenarios**:
   - Simulate S3 upload failure â†’ verify no database entry
   - Simulate Kubernetes deployment failure â†’ verify S3 deleted, database entry removed

4. **Add Saga State Persistence**:
   - Store saga state in database
   - Enable saga recovery if orchestrator crashes

**Validation**:
- All steps complete successfully in happy path
- Rollback works correctly for each failure point
- Saga state persisted and recoverable

**Deliverables**:
- Saga orchestrator code
- Compensating transaction implementations
- Test results for failure scenarios

---

### Exercise 4: Distributed Tracing (Intermediate, 8-10 hours)

**Objective**: Implement distributed tracing across microservices.

**Setup**:
1. Deploy Jaeger: `docker run -d -p 16686:16686 -p 6831:6831/udp jaegertracing/all-in-one`
2. Instrument services with OpenTelemetry

**Tasks**:

1. **Instrument Feature Service**:
   - Add tracing to all endpoints
   - Include custom attributes (user_id, feature_count)

2. **Instrument Model Service**:
   - Trace model loading, inference

3. **Create Trace for Full Prediction Workflow**:
   - API Gateway â†’ Feature Service â†’ Model Service
   - Propagate trace context across services

4. **Analyze Traces in Jaeger**:
   - Identify bottlenecks (which service is slowest?)
   - Measure latency breakdown

**Validation**:
- Traces visible in Jaeger UI
- Can trace request across all services
- Latency breakdown shows service contributions

**Deliverables**:
- Instrumented service code
- Jaeger screenshots showing traces
- Latency analysis report

---

## ðŸ”— Related Concepts

### Within This Course:
- [[01. Architectural Patterns for ML Systems]] - Overall system architecture considerations
- [[02. Lambda vs Kappa Architectures for ML]] - Batch vs streaming architectures
- [[03. Batch vs Streaming Considerations]] - When to use batch vs streaming
- [[05. Scalability and Fault Tolerance Principles]] - Scaling microservices and handling failures

### External Connections:
- **API Design**: [[REST API Best Practices]], [[gRPC Design Patterns]]
- **Event Streaming**: [[Apache Kafka Architecture]], [[Event-Driven Systems]]
- **Testing**: [[Contract Testing with Pact]], [[Integration Testing Strategies]]
- **Observability**: [[Distributed Tracing]], [[Centralized Logging]], [[Metrics and Monitoring]]

---

## ðŸ“š Further Reading

### Essential Books:

1. **"Building Microservices, 2nd Edition" by Sam Newman (O'Reilly, 2021)**
   - Comprehensive guide to microservices architecture
   - Service decomposition, communication patterns, deployment
   - Practical advice from real-world experience

2. **"Designing Data-Intensive Applications" by Martin Kleppmann (O'Reilly, 2017)**
   - Chapter 12: The Future of Data Systems (event sourcing, CQRS)
   - Distributed systems fundamentals

3. **"Domain-Driven Design" by Eric Evans (Addison-Wesley, 2003)**
   - Foundational book on DDD
   - Bounded contexts, ubiquitous language, aggregates

4. **"Release It!, 2nd Edition" by Michael Nygard (Pragmatic Bookshelf, 2018)**
   - Stability patterns (circuit breakers, bulkheads, timeouts)
   - Production-ready systems

### Key Papers:

1. **"Microservices: A Definition of This New Architectural Term" by Martin Fowler (2014)**
   - https://martinfowler.com/articles/microservices.html
   - Foundational article on microservices

2. **"CQRS" by Martin Fowler**
   - https://martinfowler.com/bliki/CQRS.html
   - Command Query Responsibility Segregation pattern

3. **"Sagas" by Hector Garcia-Molina & Kenneth Salem (1987)**
   - Original saga pattern paper
   - Managing long-running transactions

### Blogs and Articles:

1. **Netflix Tech Blog - "Zuul 2: The Netflix Journey to Asynchronous, Non-Blocking Systems"**
   - https://netflixtechblog.com/zuul-2-the-netflix-journey-to-asynchronous-non-blocking-systems-45947377fb5c
   - API gateway architecture at scale

2. **Uber Engineering - "Domain-Oriented Microservice Architecture at Uber"**
   - https://eng.uber.com/microservice-architecture/
   - Practical DDD and microservices

3. **AWS Architecture Blog - "Implementing Saga Pattern with AWS Step Functions"**
   - https://aws.amazon.com/blogs/architecture/
   - Orchestrating distributed transactions

4. **Confluent Blog - "Event-Driven Microservices with Kafka"**
   - https://www.confluent.io/blog/
   - Event streaming for microservices

### Documentation:

1. **gRPC Documentation**
   - https://grpc.io/docs/
   - Protocol buffers, service definition, best practices

2. **Apache Kafka Documentation**
   - https://kafka.apache.org/documentation/
   - Event streaming platform

3. **Istio Service Mesh**
   - https://istio.io/latest/docs/
   - Service mesh for microservices

4. **OpenTelemetry**
   - https://opentelemetry.io/docs/
   - Distributed tracing standard

### Video Courses:

1. **"Microservices Architecture" by Chris Richardson (O'Reilly)**
   - Comprehensive course on microservices patterns

2. **"Building Event-Driven Microservices" by Adam Bellemare (O'Reilly)**
   - Event sourcing, CQRS, Kafka

3. **"gRPC [Java] Master Class" by Stephane Maarek (Udemy)**
   - Hands-on gRPC development

---

## ðŸ“ Key Takeaways

1. **Microservices enable independent scaling and deployment**: Each service can be scaled and deployed independently based on its specific needs, improving agility and resource utilization.

2. **Domain-Driven Design guides service boundaries**: Use bounded contexts to decompose ML systems into cohesive services that align with business domains.

3. **gRPC outperforms REST for internal service communication**: 5-10x faster due to binary serialization and HTTP/2, making it ideal for high-throughput inter-service communication.

4. **Event-driven architecture reduces coupling**: Services communicate asynchronously via events, enabling loose coupling and independent evolution.

5. **Saga pattern enables distributed transactions**: Orchestrate multi-service workflows with compensating transactions for rollback capability.

6. **CQRS optimizes read and write paths independently**: Separate read and write models to use optimal storage for each (e.g., PostgreSQL for writes, Redis for reads).

7. **Circuit breakers prevent cascading failures**: Fail fast when downstream services are unavailable to maintain system stability.

8. **Contract testing enables independent service evolution**: Consumer-driven contracts catch breaking changes before deployment without requiring full end-to-end tests.

9. **Distributed tracing is essential for debugging**: OpenTelemetry and Jaeger enable tracing requests across services to identify bottlenecks and failures.

10. **Microservices add operational complexity**: Manage complexity with service mesh (Istio), centralized logging (ELK), and comprehensive monitoring (Prometheus/Grafana).

---

## âœï¸ Notes Section

**Personal Insights**:

**Questions to Explore**:

**Related Projects**:

---

*Last updated: 2025-10-19*
*Part of: [[DEforAI - Data Engineering for AI/ML]]*
*Chapter: [[03. Data Architecture for ML Systems]]*
